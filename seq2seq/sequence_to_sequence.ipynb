{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import helper\n",
    "\n",
    "source_path = 'data/letters_source.txt'\n",
    "target_path = 'data/letters_target.txt'\n",
    "\n",
    "source_sentences = helper.load_data(source_path)\n",
    "target_sentences = helper.load_data(target_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bsaqq', 'npy', 'lbwuj', 'bqv', 'kial', 'tddam', 'edxpjpg', 'nspv', 'huloz', '']\n"
     ]
    }
   ],
   "source": [
    "print(source_sentences[:50].split('\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abqqs', 'npy', 'bjluw', 'bqv', 'aikl', 'addmt', 'degjppx', 'npsv', 'hlouz', '']\n"
     ]
    }
   ],
   "source": [
    "print(target_sentences[:50].split('\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Process\n",
    "To do anything useful with it, we'll need to turn the each string into a list of characters<br>\n",
    "\n",
    "Convert the words to ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_character_words(data):\n",
    "    special_words = ['<PAD>', '<UNK>', '<GO>', '<EOS>']\n",
    "    \n",
    "    set_words = set([character for line in data.split('\\n') for character in line])\n",
    "    word_to_int = {word: ind for ind, word in enumerate(special_words + list(set_words))}\n",
    "    int_to_word = {ind: word for word, ind in word_to_int.items()}\n",
    "\n",
    "    return int_to_word, word_to_int\n",
    "\n",
    "\n",
    "# Building letter2int and int2letter\n",
    "source_int_to_letter, source_letter_to_int = extract_character_words(source_sentences)\n",
    "target_int_to_letter, target_letter_to_int = extract_character_words(target_sentences)\n",
    "\n",
    "# Convert Character to ids\n",
    "source_letter_ids = [[source_letter_to_int.get(word, source_letter_to_int['<UNK>']) for word in line] for line in source_sentences.split('\\n')]\n",
    "target_letter_ids = [[target_letter_to_int.get(word, target_letter_to_int['<UNK>']) for word in line] for line in target_sentences.split('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12, 10, 16, 23, 23], [26, 19, 13], [24, 12, 6, 27, 25]]\n",
      "[[16, 12, 23, 23, 10], [26, 19, 13], [12, 25, 24, 27, 6]]\n"
     ]
    }
   ],
   "source": [
    "print(source_letter_ids[:3])\n",
    "print(target_letter_ids[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "### Check the version of TensorFlow\n",
    "This will check to make sure that you have correct version of TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.1.0\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.1'), 'Please use TensorFlow version 1.1 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "epochs = 60\n",
    "# Batch Size\n",
    "batch_size = 128\n",
    "# RNN Size\n",
    "rnn_size = 50\n",
    "# Number of Layers\n",
    "num_layers = 2\n",
    "# Embedding Size\n",
    "encoding_embedding_size = 15\n",
    "decoding_embedding_size = 15\n",
    "# Learning Rate\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model_inputs():\n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name=\"input\")\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    \n",
    "    target_sequence_length = tf.placeholder(tf.int32, (None,), name=\"target_seq_length\")\n",
    "    max_target_sequence_length = tf.reduce_max(tf.int32, (None,), name=\"max_target_len\")\n",
    "    source_sequence_length = tf.placeholder(tf.int32, (None,), name=\"source_seq_length\")\n",
    "    \n",
    "    return input_data, targets, learning_rate, target_sequence_length, max_target_sequence_length, source_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
