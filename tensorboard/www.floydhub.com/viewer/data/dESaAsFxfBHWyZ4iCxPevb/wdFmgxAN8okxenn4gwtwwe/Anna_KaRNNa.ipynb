{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anna KaRNNa\n",
    "\n",
    "In this notebook, I'll build a character-wise RNN trained on Anna Karenina, one of my all-time favorite books. It'll be able to generate new text based on the text from the book.\n",
    "\n",
    "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn). Also, some information [here at r2rt](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html) and from [Sherjil Ozair](https://github.com/sherjilozair/char-rnn-tensorflow) on GitHub. Below is the general architecture of the character-wise RNN.\n",
    "\n",
    "<img src=\"assets/charseq.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll load the text file and convert it into integers for our network to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = set(text)\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "chars = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([44, 79, 68, 42, 30, 67, 48, 59, 33, 72, 72, 72, 26, 68, 42, 42,  3,\n",
       "       59, 77, 68,  8, 58, 24, 58, 67,  0, 59, 68, 48, 67, 59, 68, 24, 24,\n",
       "       59, 68, 24, 58, 60, 67, 56, 59, 67, 34, 67, 48,  3, 59,  2, 69, 79,\n",
       "       68, 42, 42,  3, 59, 77, 68,  8, 58, 24,  3, 59, 58,  0, 59,  2, 69,\n",
       "       79, 68, 42, 42,  3, 59, 58, 69, 59, 58, 30,  0, 59, 10,  4, 69, 72,\n",
       "        4, 68,  3, 53, 72, 72,  9, 34, 67, 48,  3, 30, 79, 58, 69], dtype=int32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I need to split up the data into batches, and into training and validation sets. I should be making a test set here, but I'm not going to worry about that. My test will be if the network can generate new text.\n",
    "\n",
    "Here I'll make both input and target arrays. The targets are the same as the inputs, except shifted one character over. I'll also drop the last bit of data so that I'll only have completely full batches.\n",
    "\n",
    "The idea here is to make a 2D matrix where the number of rows is equal to the number of batches. Each row will be one long concatenated string from the character data. We'll split this data into a training set and validation set using the `split_frac` keyword. This will keep 90% of the batches in the training set, the other 10% in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(chars, batch_size, num_steps, split_frac=0.9):\n",
    "    \"\"\" \n",
    "    Split character data into training and validation sets, inputs and targets for each set.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    chars: character array\n",
    "    batch_size: Size of examples in each of batch\n",
    "    num_steps: Number of sequence steps to keep in the input and pass to the network\n",
    "    split_frac: Fraction of batches to keep in the training set\n",
    "    \n",
    "    \n",
    "    Returns train_x, train_y, val_x, val_y\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    slice_size = batch_size * num_steps\n",
    "    n_batches = int(len(chars) / slice_size)\n",
    "    \n",
    "    # Drop the last few characters to make only full batches\n",
    "    x = chars[: n_batches*slice_size]\n",
    "    y = chars[1: n_batches*slice_size + 1]\n",
    "    \n",
    "    # Split the data into batch_size slices, then stack them into a 2D matrix \n",
    "    x = np.stack(np.split(x, batch_size))\n",
    "    y = np.stack(np.split(y, batch_size))\n",
    "    \n",
    "    # Now x and y are arrays with dimensions batch_size x n_batches*num_steps\n",
    "    \n",
    "    # Split into training and validation sets, keep the virst split_frac batches for training\n",
    "    split_idx = int(n_batches*split_frac)\n",
    "    train_x, train_y= x[:, :split_idx*num_steps], y[:, :split_idx*num_steps]\n",
    "    val_x, val_y = x[:, split_idx*num_steps:], y[:, split_idx*num_steps:]\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x, train_y, val_x, val_y = split_data(chars, 10, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 178400)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[44, 79, 68, 42, 30, 67, 48, 59, 33, 72],\n",
       "       [16, 69, 63, 59, 79, 67, 59,  8, 10, 34],\n",
       "       [59, 61, 68, 30, 61, 79, 58, 69, 50, 59],\n",
       "       [10, 30, 79, 67, 48, 59,  4, 10,  2, 24],\n",
       "       [59, 30, 79, 67, 59, 24, 68, 69, 63, 62],\n",
       "       [59, 32, 79, 48, 10,  2, 50, 79, 59, 24],\n",
       "       [30, 59, 30, 10, 72, 63, 10, 53, 72, 72],\n",
       "       [10, 59, 79, 67, 48,  0, 67, 24, 77, 37],\n",
       "       [79, 68, 30, 59, 58,  0, 59, 30, 79, 67],\n",
       "       [67, 48,  0, 67, 24, 77, 59, 68, 69, 63]], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[:,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll write another function to grab batches out of the arrays made by split data. Here each batch will be a sliding window on these arrays with size `batch_size X num_steps`. For example, if we want our network to train on a sequence of 100 characters, `num_steps = 100`. For the next batch, we'll shift this window the next sequence of `num_steps` characters. In this way we can feed batches to the network and the cell states will continue through on each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(arrs, num_steps):\n",
    "    batch_size, slice_size = arrs[0].shape\n",
    "    \n",
    "    n_batches = int(slice_size/num_steps)\n",
    "    for b in range(n_batches):\n",
    "        yield [x[:, b*num_steps: (b+1)*num_steps] for x in arrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_rnn(num_classes, batch_size=50, num_steps=50, lstm_size=128, num_layers=2,\n",
    "              learning_rate=0.001, grad_clip=5, sampling=False):\n",
    "        \n",
    "    if sampling == True:\n",
    "        batch_size, num_steps = 1, 1\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    with tf.name_scope('inputs'):\n",
    "        inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "        x_one_hot = tf.one_hot(inputs, num_classes, name='x_one_hot')\n",
    "\n",
    "    with tf.name_scope('targets'):\n",
    "        targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "        y_one_hot = tf.one_hot(targets, num_classes, name='y_one_hot')\n",
    "        y_reshaped = tf.reshape(y_one_hot, [-1, num_classes])\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    with tf.name_scope('RNN_layers'):\n",
    "        # Build the RNN layers\n",
    "        def build_BasicLSTMCell(lstm_size, keep_prob):\n",
    "            lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "            drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "            return drop\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([build_BasicLSTMCell(lstm_size, keep_prob) for val in range(num_layers)])\n",
    "    \n",
    "    with tf.name_scope(\"RNN_init_state\"):\n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    with tf.name_scope(\"RNN_forward\"):\n",
    "        # Run the data through the RNN layers\n",
    "        rnn_inputs = [tf.squeeze(i, squeeze_dims=[1]) for i in tf.split(x_one_hot, num_steps, 1)]\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=initial_state)\n",
    "    final_state = state\n",
    "    \n",
    "    # Reshape output so it's a bunch of rows, one row for each cell output    \n",
    "    with tf.name_scope('sequence_shape'):\n",
    "        seq_output = tf.concat(outputs, axis=1,name='seq_output')\n",
    "        output = tf.reshape(seq_output, [-1, lstm_size], name='graph_output')\n",
    "    \n",
    "    # Now connect the RNN putputs to a softmax layer and calculate the cost\n",
    "    with tf.name_scope('logits'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1),\n",
    "                               name='softmax_w')\n",
    "        softmax_b = tf.Variable(tf.zeros(num_classes), name='softmax_b')\n",
    "        logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        tf.summary.histogram('softmax_w', softmax_w)\n",
    "        tf.summary.histogram('softmax_b', softmax_b)\n",
    "\n",
    "    with tf.name_scope('predictions'):\n",
    "        preds = tf.nn.softmax(logits, name='predictions')\n",
    "        tf.summary.histogram('predictions', preds)\n",
    "\n",
    "    with tf.name_scope('cost'):\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped, name='loss')\n",
    "        cost = tf.reduce_mean(loss, name='cost')\n",
    "        tf.summary.scalar('cost', cost)\n",
    "\n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    with tf.name_scope('train'):\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "        optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    # Export the nodes\n",
    "    export_nodes = ['inputs', 'targets', 'initial_state', 'final_state',\n",
    "                    'keep_prob', 'cost', 'preds', 'optimizer', 'merged']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Here I'm defining the hyperparameters for the network. The two you probably haven't seen before are `lstm_size` and `num_layers`. These set the number of hidden units in the LSTM layers and the number of LSTM layers, respectively. Of course, making these bigger will improve the network's performance but you'll have to watch out for overfitting. If your validation loss is much larger than the training loss, you're probably overfitting. Decrease the size of the network or decrease the dropout keep probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_steps = 100\n",
    "lstm_size = 512\n",
    "num_layers = 2\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write out the graph for TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = build_rnn(len(vocab),\n",
    "                  batch_size=batch_size,\n",
    "                  num_steps=num_steps,\n",
    "                  learning_rate=learning_rate,\n",
    "                  lstm_size=lstm_size,\n",
    "                  num_layers=num_layers)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    file_writer = tf.summary.FileWriter('./logs/3', sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Time for training which is is pretty straightforward. Here I pass in some data, and get an LSTM state back. Then I pass that state back in to the network so the next batch can continue the state from the previous batch. And every so often (set by `save_every_n`) I calculate the validation loss and save a checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p checkpoints/anna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10  Iteration 1/1780 Training loss: 4.4200 3.5427 sec/batch\n",
      "Epoch 1/10  Iteration 2/1780 Training loss: 4.3738 3.6093 sec/batch\n",
      "Epoch 1/10  Iteration 3/1780 Training loss: 4.1854 3.4074 sec/batch\n",
      "Epoch 1/10  Iteration 4/1780 Training loss: 4.3983 3.6861 sec/batch\n",
      "Epoch 1/10  Iteration 5/1780 Training loss: 4.3457 4.1835 sec/batch\n",
      "Epoch 1/10  Iteration 6/1780 Training loss: 4.2776 3.4802 sec/batch\n",
      "Epoch 1/10  Iteration 7/1780 Training loss: 4.2079 4.3144 sec/batch\n",
      "Epoch 1/10  Iteration 8/1780 Training loss: 4.1360 4.8322 sec/batch\n",
      "Epoch 1/10  Iteration 9/1780 Training loss: 4.0631 4.7170 sec/batch\n",
      "Epoch 1/10  Iteration 10/1780 Training loss: 4.0001 5.0246 sec/batch\n",
      "Epoch 1/10  Iteration 11/1780 Training loss: 3.9438 4.7248 sec/batch\n",
      "Epoch 1/10  Iteration 12/1780 Training loss: 3.8987 4.6540 sec/batch\n",
      "Epoch 1/10  Iteration 13/1780 Training loss: 3.8578 4.3570 sec/batch\n",
      "Epoch 1/10  Iteration 14/1780 Training loss: 3.8220 4.3667 sec/batch\n",
      "Epoch 1/10  Iteration 15/1780 Training loss: 3.7905 4.3987 sec/batch\n",
      "Epoch 1/10  Iteration 16/1780 Training loss: 3.7616 4.3450 sec/batch\n",
      "Epoch 1/10  Iteration 17/1780 Training loss: 3.7346 4.3290 sec/batch\n",
      "Epoch 1/10  Iteration 18/1780 Training loss: 3.7122 4.3400 sec/batch\n",
      "Epoch 1/10  Iteration 19/1780 Training loss: 3.6913 4.3419 sec/batch\n",
      "Epoch 1/10  Iteration 20/1780 Training loss: 3.6697 4.3209 sec/batch\n",
      "Epoch 1/10  Iteration 21/1780 Training loss: 3.6509 4.3411 sec/batch\n",
      "Epoch 1/10  Iteration 22/1780 Training loss: 3.6333 4.3791 sec/batch\n",
      "Epoch 1/10  Iteration 23/1780 Training loss: 3.6166 4.3108 sec/batch\n",
      "Epoch 1/10  Iteration 24/1780 Training loss: 3.6016 4.3155 sec/batch\n",
      "Epoch 1/10  Iteration 25/1780 Training loss: 3.5872 4.3360 sec/batch\n",
      "Epoch 1/10  Iteration 26/1780 Training loss: 3.5740 4.6233 sec/batch\n",
      "Epoch 1/10  Iteration 27/1780 Training loss: 3.5617 4.7664 sec/batch\n",
      "Epoch 1/10  Iteration 28/1780 Training loss: 3.5493 4.9893 sec/batch\n",
      "Epoch 1/10  Iteration 29/1780 Training loss: 3.5379 4.9840 sec/batch\n",
      "Epoch 1/10  Iteration 30/1780 Training loss: 3.5275 5.1630 sec/batch\n",
      "Epoch 1/10  Iteration 31/1780 Training loss: 3.5185 5.5108 sec/batch\n",
      "Epoch 1/10  Iteration 32/1780 Training loss: 3.5086 4.7291 sec/batch\n",
      "Epoch 1/10  Iteration 33/1780 Training loss: 3.4991 4.7318 sec/batch\n",
      "Epoch 1/10  Iteration 34/1780 Training loss: 3.4907 5.3184 sec/batch\n",
      "Epoch 1/10  Iteration 35/1780 Training loss: 3.4823 4.9188 sec/batch\n",
      "Epoch 1/10  Iteration 36/1780 Training loss: 3.4747 4.8235 sec/batch\n",
      "Epoch 1/10  Iteration 37/1780 Training loss: 3.4666 4.7758 sec/batch\n",
      "Epoch 1/10  Iteration 38/1780 Training loss: 3.4591 4.7630 sec/batch\n",
      "Epoch 1/10  Iteration 39/1780 Training loss: 3.4520 4.8003 sec/batch\n",
      "Epoch 1/10  Iteration 40/1780 Training loss: 3.4451 4.8128 sec/batch\n",
      "Epoch 1/10  Iteration 41/1780 Training loss: 3.4385 4.7876 sec/batch\n",
      "Epoch 1/10  Iteration 42/1780 Training loss: 3.4322 4.7780 sec/batch\n",
      "Epoch 1/10  Iteration 43/1780 Training loss: 3.4262 5.4885 sec/batch\n",
      "Epoch 1/10  Iteration 44/1780 Training loss: 3.4202 5.3400 sec/batch\n",
      "Epoch 1/10  Iteration 45/1780 Training loss: 3.4143 5.0976 sec/batch\n",
      "Epoch 1/10  Iteration 46/1780 Training loss: 3.4090 5.0030 sec/batch\n",
      "Epoch 1/10  Iteration 47/1780 Training loss: 3.4043 4.5670 sec/batch\n",
      "Epoch 1/10  Iteration 48/1780 Training loss: 3.3995 4.6857 sec/batch\n",
      "Epoch 1/10  Iteration 49/1780 Training loss: 3.3951 4.4440 sec/batch\n",
      "Epoch 1/10  Iteration 50/1780 Training loss: 3.3907 4.7411 sec/batch\n",
      "Epoch 1/10  Iteration 51/1780 Training loss: 3.3863 4.9261 sec/batch\n",
      "Epoch 1/10  Iteration 52/1780 Training loss: 3.3817 5.6076 sec/batch\n",
      "Epoch 1/10  Iteration 53/1780 Training loss: 3.3776 5.6638 sec/batch\n",
      "Epoch 1/10  Iteration 54/1780 Training loss: 3.3733 5.8248 sec/batch\n",
      "Epoch 1/10  Iteration 55/1780 Training loss: 3.3693 4.9613 sec/batch\n",
      "Epoch 1/10  Iteration 56/1780 Training loss: 3.3652 5.2828 sec/batch\n",
      "Epoch 1/10  Iteration 57/1780 Training loss: 3.3615 4.9159 sec/batch\n",
      "Epoch 1/10  Iteration 58/1780 Training loss: 3.3578 5.0082 sec/batch\n",
      "Epoch 1/10  Iteration 59/1780 Training loss: 3.3541 4.9288 sec/batch\n",
      "Epoch 1/10  Iteration 60/1780 Training loss: 3.3507 4.8060 sec/batch\n",
      "Epoch 1/10  Iteration 61/1780 Training loss: 3.3473 4.7938 sec/batch\n",
      "Epoch 1/10  Iteration 62/1780 Training loss: 3.3445 4.3340 sec/batch\n",
      "Epoch 1/10  Iteration 63/1780 Training loss: 3.3417 4.0785 sec/batch\n",
      "Epoch 1/10  Iteration 64/1780 Training loss: 3.3383 5.3941 sec/batch\n",
      "Epoch 1/10  Iteration 65/1780 Training loss: 3.3352 4.5763 sec/batch\n",
      "Epoch 1/10  Iteration 66/1780 Training loss: 3.3324 3.9476 sec/batch\n",
      "Epoch 1/10  Iteration 67/1780 Training loss: 3.3297 4.0286 sec/batch\n",
      "Epoch 1/10  Iteration 68/1780 Training loss: 3.3263 3.8875 sec/batch\n",
      "Epoch 1/10  Iteration 69/1780 Training loss: 3.3234 3.8682 sec/batch\n",
      "Epoch 1/10  Iteration 70/1780 Training loss: 3.3209 3.7996 sec/batch\n",
      "Epoch 1/10  Iteration 71/1780 Training loss: 3.3182 3.8171 sec/batch\n",
      "Epoch 1/10  Iteration 72/1780 Training loss: 3.3160 3.8996 sec/batch\n",
      "Epoch 1/10  Iteration 73/1780 Training loss: 3.3134 3.7936 sec/batch\n",
      "Epoch 1/10  Iteration 74/1780 Training loss: 3.3110 3.7837 sec/batch\n",
      "Epoch 1/10  Iteration 75/1780 Training loss: 3.3087 3.7992 sec/batch\n",
      "Epoch 1/10  Iteration 76/1780 Training loss: 3.3066 3.7965 sec/batch\n",
      "Epoch 1/10  Iteration 77/1780 Training loss: 3.3044 3.7936 sec/batch\n",
      "Epoch 1/10  Iteration 78/1780 Training loss: 3.3022 3.8722 sec/batch\n",
      "Epoch 1/10  Iteration 79/1780 Training loss: 3.3000 3.7836 sec/batch\n",
      "Epoch 1/10  Iteration 80/1780 Training loss: 3.2976 4.0359 sec/batch\n",
      "Epoch 1/10  Iteration 81/1780 Training loss: 3.2954 3.9243 sec/batch\n",
      "Epoch 1/10  Iteration 82/1780 Training loss: 3.2934 3.8252 sec/batch\n",
      "Epoch 1/10  Iteration 83/1780 Training loss: 3.2914 3.7862 sec/batch\n",
      "Epoch 1/10  Iteration 84/1780 Training loss: 3.2893 3.7949 sec/batch\n",
      "Epoch 1/10  Iteration 85/1780 Training loss: 3.2871 3.7888 sec/batch\n",
      "Epoch 1/10  Iteration 86/1780 Training loss: 3.2850 3.7819 sec/batch\n",
      "Epoch 1/10  Iteration 87/1780 Training loss: 3.2830 3.8904 sec/batch\n",
      "Epoch 1/10  Iteration 88/1780 Training loss: 3.2810 4.0062 sec/batch\n",
      "Epoch 1/10  Iteration 89/1780 Training loss: 3.2791 3.8013 sec/batch\n",
      "Epoch 1/10  Iteration 90/1780 Training loss: 3.2772 3.7972 sec/batch\n",
      "Epoch 1/10  Iteration 91/1780 Training loss: 3.2754 3.7951 sec/batch\n",
      "Epoch 1/10  Iteration 92/1780 Training loss: 3.2735 3.7895 sec/batch\n",
      "Epoch 1/10  Iteration 93/1780 Training loss: 3.2717 3.7980 sec/batch\n",
      "Epoch 1/10  Iteration 94/1780 Training loss: 3.2699 3.7954 sec/batch\n",
      "Epoch 1/10  Iteration 95/1780 Training loss: 3.2679 3.7934 sec/batch\n",
      "Epoch 1/10  Iteration 96/1780 Training loss: 3.2660 3.7823 sec/batch\n",
      "Epoch 1/10  Iteration 97/1780 Training loss: 3.2643 3.7990 sec/batch\n",
      "Epoch 1/10  Iteration 98/1780 Training loss: 3.2624 3.8007 sec/batch\n",
      "Epoch 1/10  Iteration 99/1780 Training loss: 3.2606 3.8077 sec/batch\n",
      "Epoch 1/10  Iteration 100/1780 Training loss: 3.2587 3.8147 sec/batch\n",
      "Validation loss: 3.03312 Saving checkpoint!\n",
      "Epoch 1/10  Iteration 101/1780 Training loss: 3.2569 3.7296 sec/batch\n",
      "Epoch 1/10  Iteration 102/1780 Training loss: 3.2551 3.8014 sec/batch\n",
      "Epoch 1/10  Iteration 103/1780 Training loss: 3.2533 3.7978 sec/batch\n",
      "Epoch 1/10  Iteration 104/1780 Training loss: 3.2514 3.7893 sec/batch\n",
      "Epoch 1/10  Iteration 105/1780 Training loss: 3.2496 3.8068 sec/batch\n",
      "Epoch 1/10  Iteration 106/1780 Training loss: 3.2477 3.7870 sec/batch\n",
      "Epoch 1/10  Iteration 107/1780 Training loss: 3.2459 3.7956 sec/batch\n",
      "Epoch 1/10  Iteration 108/1780 Training loss: 3.2442 3.7955 sec/batch\n",
      "Epoch 1/10  Iteration 109/1780 Training loss: 3.2427 4.0382 sec/batch\n",
      "Epoch 1/10  Iteration 110/1780 Training loss: 3.2409 4.4717 sec/batch\n",
      "Epoch 1/10  Iteration 111/1780 Training loss: 3.2394 3.8705 sec/batch\n",
      "Epoch 1/10  Iteration 112/1780 Training loss: 3.2378 3.9119 sec/batch\n",
      "Epoch 1/10  Iteration 113/1780 Training loss: 3.2360 3.8175 sec/batch\n",
      "Epoch 1/10  Iteration 114/1780 Training loss: 3.2342 3.7938 sec/batch\n",
      "Epoch 1/10  Iteration 115/1780 Training loss: 3.2324 3.8075 sec/batch\n",
      "Epoch 1/10  Iteration 116/1780 Training loss: 3.2306 3.7818 sec/batch\n",
      "Epoch 1/10  Iteration 117/1780 Training loss: 3.2288 3.8007 sec/batch\n",
      "Epoch 1/10  Iteration 118/1780 Training loss: 3.2271 3.8145 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10  Iteration 119/1780 Training loss: 3.2254 3.8227 sec/batch\n",
      "Epoch 1/10  Iteration 120/1780 Training loss: 3.2235 4.9693 sec/batch\n",
      "Epoch 1/10  Iteration 121/1780 Training loss: 3.2219 5.0501 sec/batch\n",
      "Epoch 1/10  Iteration 122/1780 Training loss: 3.2202 3.9961 sec/batch\n",
      "Epoch 1/10  Iteration 123/1780 Training loss: 3.2183 3.8178 sec/batch\n",
      "Epoch 1/10  Iteration 124/1780 Training loss: 3.2165 3.8345 sec/batch\n",
      "Epoch 1/10  Iteration 125/1780 Training loss: 3.2146 3.8703 sec/batch\n",
      "Epoch 1/10  Iteration 126/1780 Training loss: 3.2125 3.9015 sec/batch\n",
      "Epoch 1/10  Iteration 127/1780 Training loss: 3.2105 3.8733 sec/batch\n",
      "Epoch 1/10  Iteration 128/1780 Training loss: 3.2086 3.8445 sec/batch\n",
      "Epoch 1/10  Iteration 129/1780 Training loss: 3.2065 3.9670 sec/batch\n",
      "Epoch 1/10  Iteration 130/1780 Training loss: 3.2044 4.0543 sec/batch\n",
      "Epoch 1/10  Iteration 131/1780 Training loss: 3.2022 3.9050 sec/batch\n",
      "Epoch 1/10  Iteration 132/1780 Training loss: 3.1998 3.9409 sec/batch\n",
      "Epoch 1/10  Iteration 133/1780 Training loss: 3.1976 3.8757 sec/batch\n",
      "Epoch 1/10  Iteration 134/1780 Training loss: 3.1953 3.7958 sec/batch\n",
      "Epoch 1/10  Iteration 135/1780 Training loss: 3.1926 3.7810 sec/batch\n",
      "Epoch 1/10  Iteration 136/1780 Training loss: 3.1902 3.7883 sec/batch\n",
      "Epoch 1/10  Iteration 137/1780 Training loss: 3.1876 3.7975 sec/batch\n",
      "Epoch 1/10  Iteration 138/1780 Training loss: 3.1850 3.8004 sec/batch\n",
      "Epoch 1/10  Iteration 139/1780 Training loss: 3.1826 3.8578 sec/batch\n",
      "Epoch 1/10  Iteration 140/1780 Training loss: 3.1803 3.9224 sec/batch\n",
      "Epoch 1/10  Iteration 141/1780 Training loss: 3.1781 3.8158 sec/batch\n",
      "Epoch 1/10  Iteration 142/1780 Training loss: 3.1755 4.0288 sec/batch\n",
      "Epoch 1/10  Iteration 143/1780 Training loss: 3.1729 3.8276 sec/batch\n",
      "Epoch 1/10  Iteration 144/1780 Training loss: 3.1704 3.8035 sec/batch\n",
      "Epoch 1/10  Iteration 145/1780 Training loss: 3.1679 3.9140 sec/batch\n",
      "Epoch 1/10  Iteration 146/1780 Training loss: 3.1654 3.8931 sec/batch\n",
      "Epoch 1/10  Iteration 147/1780 Training loss: 3.1629 4.0205 sec/batch\n",
      "Epoch 1/10  Iteration 148/1780 Training loss: 3.1605 3.8603 sec/batch\n",
      "Epoch 1/10  Iteration 149/1780 Training loss: 3.1578 3.9309 sec/batch\n",
      "Epoch 1/10  Iteration 150/1780 Training loss: 3.1551 3.8909 sec/batch\n",
      "Epoch 1/10  Iteration 151/1780 Training loss: 3.1526 3.8216 sec/batch\n",
      "Epoch 1/10  Iteration 152/1780 Training loss: 3.1501 4.3899 sec/batch\n",
      "Epoch 1/10  Iteration 153/1780 Training loss: 3.1475 4.2983 sec/batch\n",
      "Epoch 1/10  Iteration 154/1780 Training loss: 3.1449 3.9494 sec/batch\n",
      "Epoch 1/10  Iteration 155/1780 Training loss: 3.1420 3.9163 sec/batch\n",
      "Epoch 1/10  Iteration 156/1780 Training loss: 3.1393 4.2077 sec/batch\n",
      "Epoch 1/10  Iteration 157/1780 Training loss: 3.1364 4.2295 sec/batch\n",
      "Epoch 1/10  Iteration 158/1780 Training loss: 3.1337 3.8152 sec/batch\n",
      "Epoch 1/10  Iteration 159/1780 Training loss: 3.1307 4.1785 sec/batch\n",
      "Epoch 1/10  Iteration 160/1780 Training loss: 3.1280 3.8338 sec/batch\n",
      "Epoch 1/10  Iteration 161/1780 Training loss: 3.1251 4.4191 sec/batch\n",
      "Epoch 1/10  Iteration 162/1780 Training loss: 3.1221 5.0884 sec/batch\n",
      "Epoch 1/10  Iteration 163/1780 Training loss: 3.1190 5.1654 sec/batch\n",
      "Epoch 1/10  Iteration 164/1780 Training loss: 3.1160 4.3936 sec/batch\n",
      "Epoch 1/10  Iteration 165/1780 Training loss: 3.1130 4.1919 sec/batch\n",
      "Epoch 1/10  Iteration 166/1780 Training loss: 3.1101 4.0132 sec/batch\n",
      "Epoch 1/10  Iteration 167/1780 Training loss: 3.1073 3.9464 sec/batch\n",
      "Epoch 1/10  Iteration 168/1780 Training loss: 3.1043 3.8075 sec/batch\n",
      "Epoch 1/10  Iteration 169/1780 Training loss: 3.1014 3.8238 sec/batch\n",
      "Epoch 1/10  Iteration 170/1780 Training loss: 3.0983 3.8047 sec/batch\n",
      "Epoch 1/10  Iteration 171/1780 Training loss: 3.0955 3.8872 sec/batch\n",
      "Epoch 1/10  Iteration 172/1780 Training loss: 3.0927 3.8433 sec/batch\n",
      "Epoch 1/10  Iteration 173/1780 Training loss: 3.0900 3.8746 sec/batch\n",
      "Epoch 1/10  Iteration 174/1780 Training loss: 3.0872 3.8346 sec/batch\n",
      "Epoch 1/10  Iteration 175/1780 Training loss: 3.0844 3.9041 sec/batch\n",
      "Epoch 1/10  Iteration 176/1780 Training loss: 3.0815 3.8128 sec/batch\n",
      "Epoch 1/10  Iteration 177/1780 Training loss: 3.0785 3.7891 sec/batch\n",
      "Epoch 1/10  Iteration 178/1780 Training loss: 3.0755 3.8193 sec/batch\n",
      "Epoch 2/10  Iteration 179/1780 Training loss: 2.5974 3.8424 sec/batch\n",
      "Epoch 2/10  Iteration 180/1780 Training loss: 2.5507 4.2878 sec/batch\n",
      "Epoch 2/10  Iteration 181/1780 Training loss: 2.5418 3.8882 sec/batch\n",
      "Epoch 2/10  Iteration 182/1780 Training loss: 2.5371 3.9937 sec/batch\n",
      "Epoch 2/10  Iteration 183/1780 Training loss: 2.5334 4.1886 sec/batch\n",
      "Epoch 2/10  Iteration 184/1780 Training loss: 2.5289 4.0773 sec/batch\n",
      "Epoch 2/10  Iteration 185/1780 Training loss: 2.5263 3.8417 sec/batch\n",
      "Epoch 2/10  Iteration 186/1780 Training loss: 2.5251 3.8036 sec/batch\n",
      "Epoch 2/10  Iteration 187/1780 Training loss: 2.5262 3.8736 sec/batch\n",
      "Epoch 2/10  Iteration 188/1780 Training loss: 2.5236 3.7859 sec/batch\n",
      "Epoch 2/10  Iteration 189/1780 Training loss: 2.5205 3.8317 sec/batch\n",
      "Epoch 2/10  Iteration 190/1780 Training loss: 2.5196 3.9209 sec/batch\n",
      "Epoch 2/10  Iteration 191/1780 Training loss: 2.5180 4.0485 sec/batch\n",
      "Epoch 2/10  Iteration 192/1780 Training loss: 2.5183 3.9034 sec/batch\n",
      "Epoch 2/10  Iteration 193/1780 Training loss: 2.5163 4.1609 sec/batch\n",
      "Epoch 2/10  Iteration 194/1780 Training loss: 2.5148 3.9115 sec/batch\n",
      "Epoch 2/10  Iteration 195/1780 Training loss: 2.5132 4.1900 sec/batch\n",
      "Epoch 2/10  Iteration 196/1780 Training loss: 2.5130 3.9594 sec/batch\n",
      "Epoch 2/10  Iteration 197/1780 Training loss: 2.5116 4.7969 sec/batch\n",
      "Epoch 2/10  Iteration 198/1780 Training loss: 2.5086 5.4179 sec/batch\n",
      "Epoch 2/10  Iteration 199/1780 Training loss: 2.5061 5.0849 sec/batch\n",
      "Epoch 2/10  Iteration 200/1780 Training loss: 2.5060 5.7020 sec/batch\n",
      "Validation loss: 2.38122 Saving checkpoint!\n",
      "Epoch 2/10  Iteration 201/1780 Training loss: 2.5048 3.7423 sec/batch\n",
      "Epoch 2/10  Iteration 202/1780 Training loss: 2.5026 3.7975 sec/batch\n",
      "Epoch 2/10  Iteration 203/1780 Training loss: 2.5006 3.7937 sec/batch\n",
      "Epoch 2/10  Iteration 204/1780 Training loss: 2.4989 3.8759 sec/batch\n",
      "Epoch 2/10  Iteration 205/1780 Training loss: 2.4966 3.7965 sec/batch\n",
      "Epoch 2/10  Iteration 206/1780 Training loss: 2.4947 3.7953 sec/batch\n",
      "Epoch 2/10  Iteration 207/1780 Training loss: 2.4935 3.7970 sec/batch\n",
      "Epoch 2/10  Iteration 208/1780 Training loss: 2.4919 3.7962 sec/batch\n",
      "Epoch 2/10  Iteration 209/1780 Training loss: 2.4908 3.8906 sec/batch\n",
      "Epoch 2/10  Iteration 210/1780 Training loss: 2.4885 3.7977 sec/batch\n",
      "Epoch 2/10  Iteration 211/1780 Training loss: 2.4862 3.7916 sec/batch\n",
      "Epoch 2/10  Iteration 212/1780 Training loss: 2.4846 3.8068 sec/batch\n",
      "Epoch 2/10  Iteration 213/1780 Training loss: 2.4827 3.7934 sec/batch\n",
      "Epoch 2/10  Iteration 214/1780 Training loss: 2.4814 3.8141 sec/batch\n",
      "Epoch 2/10  Iteration 215/1780 Training loss: 2.4795 3.8108 sec/batch\n",
      "Epoch 2/10  Iteration 216/1780 Training loss: 2.4770 3.7922 sec/batch\n",
      "Epoch 2/10  Iteration 217/1780 Training loss: 2.4749 3.8169 sec/batch\n",
      "Epoch 2/10  Iteration 218/1780 Training loss: 2.4727 3.7938 sec/batch\n",
      "Epoch 2/10  Iteration 219/1780 Training loss: 2.4705 3.7948 sec/batch\n",
      "Epoch 2/10  Iteration 220/1780 Training loss: 2.4683 3.7986 sec/batch\n",
      "Epoch 2/10  Iteration 221/1780 Training loss: 2.4663 3.7782 sec/batch\n",
      "Epoch 2/10  Iteration 222/1780 Training loss: 2.4642 3.7985 sec/batch\n",
      "Epoch 2/10  Iteration 223/1780 Training loss: 2.4623 3.8012 sec/batch\n",
      "Epoch 2/10  Iteration 224/1780 Training loss: 2.4597 3.8732 sec/batch\n",
      "Epoch 2/10  Iteration 225/1780 Training loss: 2.4583 3.8624 sec/batch\n",
      "Epoch 2/10  Iteration 226/1780 Training loss: 2.4566 3.7997 sec/batch\n",
      "Epoch 2/10  Iteration 227/1780 Training loss: 2.4550 3.8127 sec/batch\n",
      "Epoch 2/10  Iteration 228/1780 Training loss: 2.4539 3.8069 sec/batch\n",
      "Epoch 2/10  Iteration 229/1780 Training loss: 2.4519 3.7931 sec/batch\n",
      "Epoch 2/10  Iteration 230/1780 Training loss: 2.4504 3.7955 sec/batch\n",
      "Epoch 2/10  Iteration 231/1780 Training loss: 2.4488 3.7996 sec/batch\n",
      "Epoch 2/10  Iteration 232/1780 Training loss: 2.4471 3.7946 sec/batch\n",
      "Epoch 2/10  Iteration 233/1780 Training loss: 2.4455 3.7995 sec/batch\n",
      "Epoch 2/10  Iteration 234/1780 Training loss: 2.4441 3.7999 sec/batch\n",
      "Epoch 2/10  Iteration 235/1780 Training loss: 2.4426 3.8100 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10  Iteration 236/1780 Training loss: 2.4408 3.7921 sec/batch\n",
      "Epoch 2/10  Iteration 237/1780 Training loss: 2.4394 3.7951 sec/batch\n",
      "Epoch 2/10  Iteration 238/1780 Training loss: 2.4382 3.7903 sec/batch\n",
      "Epoch 2/10  Iteration 239/1780 Training loss: 2.4375 3.7908 sec/batch\n",
      "Epoch 2/10  Iteration 240/1780 Training loss: 2.4362 3.8147 sec/batch\n",
      "Epoch 2/10  Iteration 241/1780 Training loss: 2.4352 3.7833 sec/batch\n",
      "Epoch 2/10  Iteration 242/1780 Training loss: 2.4337 3.7931 sec/batch\n",
      "Epoch 2/10  Iteration 243/1780 Training loss: 2.4321 3.7850 sec/batch\n",
      "Epoch 2/10  Iteration 244/1780 Training loss: 2.4310 3.8014 sec/batch\n",
      "Epoch 2/10  Iteration 245/1780 Training loss: 2.4296 3.8006 sec/batch\n",
      "Epoch 2/10  Iteration 246/1780 Training loss: 2.4279 3.7939 sec/batch\n",
      "Epoch 2/10  Iteration 247/1780 Training loss: 2.4261 3.8027 sec/batch\n",
      "Epoch 2/10  Iteration 248/1780 Training loss: 2.4248 3.7906 sec/batch\n",
      "Epoch 2/10  Iteration 249/1780 Training loss: 2.4236 3.7972 sec/batch\n",
      "Epoch 2/10  Iteration 250/1780 Training loss: 2.4224 3.7830 sec/batch\n",
      "Epoch 2/10  Iteration 251/1780 Training loss: 2.4210 3.7967 sec/batch\n",
      "Epoch 2/10  Iteration 252/1780 Training loss: 2.4193 3.8033 sec/batch\n",
      "Epoch 2/10  Iteration 253/1780 Training loss: 2.4179 3.7934 sec/batch\n",
      "Epoch 2/10  Iteration 254/1780 Training loss: 2.4168 3.7831 sec/batch\n",
      "Epoch 2/10  Iteration 255/1780 Training loss: 2.4154 3.8382 sec/batch\n",
      "Epoch 2/10  Iteration 256/1780 Training loss: 2.4142 3.8943 sec/batch\n",
      "Epoch 2/10  Iteration 257/1780 Training loss: 2.4126 3.7995 sec/batch\n",
      "Epoch 2/10  Iteration 258/1780 Training loss: 2.4111 3.7978 sec/batch\n",
      "Epoch 2/10  Iteration 259/1780 Training loss: 2.4096 3.7892 sec/batch\n",
      "Epoch 2/10  Iteration 260/1780 Training loss: 2.4084 3.7948 sec/batch\n",
      "Epoch 2/10  Iteration 261/1780 Training loss: 2.4067 3.7989 sec/batch\n",
      "Epoch 2/10  Iteration 262/1780 Training loss: 2.4051 3.7908 sec/batch\n",
      "Epoch 2/10  Iteration 263/1780 Training loss: 2.4032 3.7966 sec/batch\n",
      "Epoch 2/10  Iteration 264/1780 Training loss: 2.4017 3.7883 sec/batch\n",
      "Epoch 2/10  Iteration 265/1780 Training loss: 2.4004 3.7884 sec/batch\n",
      "Epoch 2/10  Iteration 266/1780 Training loss: 2.3990 3.7954 sec/batch\n",
      "Epoch 2/10  Iteration 267/1780 Training loss: 2.3974 3.7936 sec/batch\n",
      "Epoch 2/10  Iteration 268/1780 Training loss: 2.3961 3.7820 sec/batch\n",
      "Epoch 2/10  Iteration 269/1780 Training loss: 2.3947 3.7891 sec/batch\n",
      "Epoch 2/10  Iteration 270/1780 Training loss: 2.3936 3.8076 sec/batch\n",
      "Epoch 2/10  Iteration 271/1780 Training loss: 2.3920 3.7834 sec/batch\n",
      "Epoch 2/10  Iteration 272/1780 Training loss: 2.3905 3.8119 sec/batch\n",
      "Epoch 2/10  Iteration 273/1780 Training loss: 2.3889 3.7932 sec/batch\n",
      "Epoch 2/10  Iteration 274/1780 Training loss: 2.3874 3.8341 sec/batch\n",
      "Epoch 2/10  Iteration 275/1780 Training loss: 2.3861 3.9450 sec/batch\n",
      "Epoch 2/10  Iteration 276/1780 Training loss: 2.3846 3.7951 sec/batch\n",
      "Epoch 2/10  Iteration 277/1780 Training loss: 2.3831 4.3144 sec/batch\n",
      "Epoch 2/10  Iteration 278/1780 Training loss: 2.3816 4.9281 sec/batch\n",
      "Epoch 2/10  Iteration 279/1780 Training loss: 2.3804 4.5716 sec/batch\n",
      "Epoch 2/10  Iteration 280/1780 Training loss: 2.3791 3.9492 sec/batch\n",
      "Epoch 2/10  Iteration 281/1780 Training loss: 2.3776 4.1082 sec/batch\n",
      "Epoch 2/10  Iteration 282/1780 Training loss: 2.3762 3.7766 sec/batch\n",
      "Epoch 2/10  Iteration 283/1780 Training loss: 2.3747 3.7718 sec/batch\n",
      "Epoch 2/10  Iteration 284/1780 Training loss: 2.3734 3.8236 sec/batch\n",
      "Epoch 2/10  Iteration 285/1780 Training loss: 2.3721 3.8943 sec/batch\n",
      "Epoch 2/10  Iteration 286/1780 Training loss: 2.3711 4.5748 sec/batch\n",
      "Epoch 2/10  Iteration 287/1780 Training loss: 2.3700 4.5521 sec/batch\n",
      "Epoch 2/10  Iteration 288/1780 Training loss: 2.3685 4.7550 sec/batch\n",
      "Epoch 2/10  Iteration 289/1780 Training loss: 2.3674 5.7559 sec/batch\n",
      "Epoch 2/10  Iteration 290/1780 Training loss: 2.3662 4.1171 sec/batch\n",
      "Epoch 2/10  Iteration 291/1780 Training loss: 2.3649 4.1901 sec/batch\n",
      "Epoch 2/10  Iteration 292/1780 Training loss: 2.3636 4.4862 sec/batch\n",
      "Epoch 2/10  Iteration 293/1780 Training loss: 2.3622 4.7492 sec/batch\n",
      "Epoch 2/10  Iteration 294/1780 Training loss: 2.3606 4.7596 sec/batch\n",
      "Epoch 2/10  Iteration 295/1780 Training loss: 2.3594 5.1807 sec/batch\n",
      "Epoch 2/10  Iteration 296/1780 Training loss: 2.3581 5.1163 sec/batch\n",
      "Epoch 2/10  Iteration 297/1780 Training loss: 2.3571 5.0238 sec/batch\n",
      "Epoch 2/10  Iteration 298/1780 Training loss: 2.3559 5.3574 sec/batch\n",
      "Epoch 2/10  Iteration 299/1780 Training loss: 2.3549 5.4009 sec/batch\n",
      "Epoch 2/10  Iteration 300/1780 Training loss: 2.3536 5.3063 sec/batch\n",
      "Validation loss: 2.11658 Saving checkpoint!\n",
      "Epoch 2/10  Iteration 301/1780 Training loss: 2.3525 5.4014 sec/batch\n",
      "Epoch 2/10  Iteration 302/1780 Training loss: 2.3514 5.5874 sec/batch\n",
      "Epoch 2/10  Iteration 303/1780 Training loss: 2.3502 5.7162 sec/batch\n",
      "Epoch 2/10  Iteration 304/1780 Training loss: 2.3488 5.3204 sec/batch\n",
      "Epoch 2/10  Iteration 305/1780 Training loss: 2.3477 5.3899 sec/batch\n",
      "Epoch 2/10  Iteration 306/1780 Training loss: 2.3466 5.3741 sec/batch\n",
      "Epoch 2/10  Iteration 307/1780 Training loss: 2.3455 5.3107 sec/batch\n",
      "Epoch 2/10  Iteration 308/1780 Training loss: 2.3444 5.3431 sec/batch\n",
      "Epoch 2/10  Iteration 309/1780 Training loss: 2.3431 5.3429 sec/batch\n",
      "Epoch 2/10  Iteration 310/1780 Training loss: 2.3416 5.3182 sec/batch\n",
      "Epoch 2/10  Iteration 311/1780 Training loss: 2.3405 5.4192 sec/batch\n",
      "Epoch 2/10  Iteration 312/1780 Training loss: 2.3395 5.5512 sec/batch\n",
      "Epoch 2/10  Iteration 313/1780 Training loss: 2.3382 5.3033 sec/batch\n",
      "Epoch 2/10  Iteration 314/1780 Training loss: 2.3372 5.3579 sec/batch\n",
      "Epoch 2/10  Iteration 315/1780 Training loss: 2.3361 5.3890 sec/batch\n",
      "Epoch 2/10  Iteration 316/1780 Training loss: 2.3350 5.3522 sec/batch\n",
      "Epoch 2/10  Iteration 317/1780 Training loss: 2.3341 5.3363 sec/batch\n",
      "Epoch 2/10  Iteration 318/1780 Training loss: 2.3329 5.3477 sec/batch\n",
      "Epoch 2/10  Iteration 319/1780 Training loss: 2.3320 5.3605 sec/batch\n",
      "Epoch 2/10  Iteration 320/1780 Training loss: 2.3308 5.3200 sec/batch\n",
      "Epoch 2/10  Iteration 321/1780 Training loss: 2.3296 5.3384 sec/batch\n",
      "Epoch 2/10  Iteration 322/1780 Training loss: 2.3285 5.3512 sec/batch\n",
      "Epoch 2/10  Iteration 323/1780 Training loss: 2.3272 5.3364 sec/batch\n",
      "Epoch 2/10  Iteration 324/1780 Training loss: 2.3263 5.3350 sec/batch\n",
      "Epoch 2/10  Iteration 325/1780 Training loss: 2.3253 5.4137 sec/batch\n",
      "Epoch 2/10  Iteration 326/1780 Training loss: 2.3243 5.3785 sec/batch\n",
      "Epoch 2/10  Iteration 327/1780 Training loss: 2.3232 5.3783 sec/batch\n",
      "Epoch 2/10  Iteration 328/1780 Training loss: 2.3220 5.2792 sec/batch\n",
      "Epoch 2/10  Iteration 329/1780 Training loss: 2.3209 5.3249 sec/batch\n",
      "Epoch 2/10  Iteration 330/1780 Training loss: 2.3201 5.3521 sec/batch\n",
      "Epoch 2/10  Iteration 331/1780 Training loss: 2.3191 5.3349 sec/batch\n",
      "Epoch 2/10  Iteration 332/1780 Training loss: 2.3180 5.5759 sec/batch\n",
      "Epoch 2/10  Iteration 333/1780 Training loss: 2.3168 5.3395 sec/batch\n",
      "Epoch 2/10  Iteration 334/1780 Training loss: 2.3158 5.3150 sec/batch\n",
      "Epoch 2/10  Iteration 335/1780 Training loss: 2.3146 5.3307 sec/batch\n",
      "Epoch 2/10  Iteration 336/1780 Training loss: 2.3135 5.3988 sec/batch\n",
      "Epoch 2/10  Iteration 337/1780 Training loss: 2.3122 5.4607 sec/batch\n",
      "Epoch 2/10  Iteration 338/1780 Training loss: 2.3114 5.3175 sec/batch\n",
      "Epoch 2/10  Iteration 339/1780 Training loss: 2.3104 5.3356 sec/batch\n",
      "Epoch 2/10  Iteration 340/1780 Training loss: 2.3092 5.3710 sec/batch\n",
      "Epoch 2/10  Iteration 341/1780 Training loss: 2.3082 5.3584 sec/batch\n",
      "Epoch 2/10  Iteration 342/1780 Training loss: 2.3071 5.4062 sec/batch\n",
      "Epoch 2/10  Iteration 343/1780 Training loss: 2.3061 5.3778 sec/batch\n",
      "Epoch 2/10  Iteration 344/1780 Training loss: 2.3050 5.4551 sec/batch\n",
      "Epoch 2/10  Iteration 345/1780 Training loss: 2.3041 5.4130 sec/batch\n",
      "Epoch 2/10  Iteration 346/1780 Training loss: 2.3032 5.3577 sec/batch\n",
      "Epoch 2/10  Iteration 347/1780 Training loss: 2.3021 5.3919 sec/batch\n",
      "Epoch 2/10  Iteration 348/1780 Training loss: 2.3010 5.4315 sec/batch\n",
      "Epoch 2/10  Iteration 349/1780 Training loss: 2.2999 5.3994 sec/batch\n",
      "Epoch 2/10  Iteration 350/1780 Training loss: 2.2989 5.3826 sec/batch\n",
      "Epoch 2/10  Iteration 351/1780 Training loss: 2.2980 5.4287 sec/batch\n",
      "Epoch 2/10  Iteration 352/1780 Training loss: 2.2971 5.3840 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10  Iteration 353/1780 Training loss: 2.2962 5.3518 sec/batch\n",
      "Epoch 2/10  Iteration 354/1780 Training loss: 2.2952 5.4143 sec/batch\n",
      "Epoch 2/10  Iteration 355/1780 Training loss: 2.2941 5.3777 sec/batch\n",
      "Epoch 2/10  Iteration 356/1780 Training loss: 2.2932 5.4279 sec/batch\n",
      "Epoch 3/10  Iteration 357/1780 Training loss: 2.1736 5.4091 sec/batch\n",
      "Epoch 3/10  Iteration 358/1780 Training loss: 2.1270 5.4482 sec/batch\n",
      "Epoch 3/10  Iteration 359/1780 Training loss: 2.1155 5.3929 sec/batch\n",
      "Epoch 3/10  Iteration 360/1780 Training loss: 2.1103 5.3984 sec/batch\n",
      "Epoch 3/10  Iteration 361/1780 Training loss: 2.1081 5.4265 sec/batch\n",
      "Epoch 3/10  Iteration 362/1780 Training loss: 2.1027 5.3785 sec/batch\n",
      "Epoch 3/10  Iteration 363/1780 Training loss: 2.1031 5.4295 sec/batch\n",
      "Epoch 3/10  Iteration 364/1780 Training loss: 2.1029 5.3972 sec/batch\n",
      "Epoch 3/10  Iteration 365/1780 Training loss: 2.1052 5.3781 sec/batch\n",
      "Epoch 3/10  Iteration 366/1780 Training loss: 2.1043 5.4089 sec/batch\n",
      "Epoch 3/10  Iteration 367/1780 Training loss: 2.1007 5.3851 sec/batch\n",
      "Epoch 3/10  Iteration 368/1780 Training loss: 2.0986 5.3681 sec/batch\n",
      "Epoch 3/10  Iteration 369/1780 Training loss: 2.0976 5.3485 sec/batch\n",
      "Epoch 3/10  Iteration 370/1780 Training loss: 2.0993 5.5153 sec/batch\n",
      "Epoch 3/10  Iteration 371/1780 Training loss: 2.0978 5.2999 sec/batch\n",
      "Epoch 3/10  Iteration 372/1780 Training loss: 2.0958 5.3132 sec/batch\n",
      "Epoch 3/10  Iteration 373/1780 Training loss: 2.0942 5.3367 sec/batch\n",
      "Epoch 3/10  Iteration 374/1780 Training loss: 2.0954 5.3993 sec/batch\n",
      "Epoch 3/10  Iteration 375/1780 Training loss: 2.0949 5.3743 sec/batch\n",
      "Epoch 3/10  Iteration 376/1780 Training loss: 2.0937 5.3623 sec/batch\n",
      "Epoch 3/10  Iteration 377/1780 Training loss: 2.0926 5.3673 sec/batch\n",
      "Epoch 3/10  Iteration 378/1780 Training loss: 2.0938 5.3309 sec/batch\n",
      "Epoch 3/10  Iteration 379/1780 Training loss: 2.0923 5.2880 sec/batch\n",
      "Epoch 3/10  Iteration 380/1780 Training loss: 2.0904 5.2558 sec/batch\n",
      "Epoch 3/10  Iteration 381/1780 Training loss: 2.0896 5.2889 sec/batch\n",
      "Epoch 3/10  Iteration 382/1780 Training loss: 2.0879 5.2926 sec/batch\n",
      "Epoch 3/10  Iteration 383/1780 Training loss: 2.0867 5.2752 sec/batch\n",
      "Epoch 3/10  Iteration 384/1780 Training loss: 2.0861 5.2728 sec/batch\n",
      "Epoch 3/10  Iteration 385/1780 Training loss: 2.0863 5.5260 sec/batch\n",
      "Epoch 3/10  Iteration 386/1780 Training loss: 2.0858 5.2850 sec/batch\n",
      "Epoch 3/10  Iteration 387/1780 Training loss: 2.0850 5.3177 sec/batch\n",
      "Epoch 3/10  Iteration 388/1780 Training loss: 2.0835 5.2324 sec/batch\n",
      "Epoch 3/10  Iteration 389/1780 Training loss: 2.0825 5.2892 sec/batch\n",
      "Epoch 3/10  Iteration 390/1780 Training loss: 2.0825 5.2915 sec/batch\n",
      "Epoch 3/10  Iteration 391/1780 Training loss: 2.0816 5.3102 sec/batch\n",
      "Epoch 3/10  Iteration 392/1780 Training loss: 2.0803 5.3921 sec/batch\n",
      "Epoch 3/10  Iteration 393/1780 Training loss: 2.0793 5.4462 sec/batch\n",
      "Epoch 3/10  Iteration 394/1780 Training loss: 2.0774 5.3228 sec/batch\n",
      "Epoch 3/10  Iteration 395/1780 Training loss: 2.0755 5.5164 sec/batch\n",
      "Epoch 3/10  Iteration 396/1780 Training loss: 2.0740 5.2592 sec/batch\n",
      "Epoch 3/10  Iteration 397/1780 Training loss: 2.0730 5.3128 sec/batch\n",
      "Epoch 3/10  Iteration 398/1780 Training loss: 2.0723 5.2810 sec/batch\n",
      "Epoch 3/10  Iteration 399/1780 Training loss: 2.0711 5.2590 sec/batch\n",
      "Epoch 3/10  Iteration 400/1780 Training loss: 2.0700 5.3116 sec/batch\n",
      "Validation loss: 1.93262 Saving checkpoint!\n",
      "Epoch 3/10  Iteration 401/1780 Training loss: 2.0699 5.2436 sec/batch\n",
      "Epoch 3/10  Iteration 402/1780 Training loss: 2.0678 5.2618 sec/batch\n",
      "Epoch 3/10  Iteration 403/1780 Training loss: 2.0673 5.3040 sec/batch\n",
      "Epoch 3/10  Iteration 404/1780 Training loss: 2.0661 5.2896 sec/batch\n",
      "Epoch 3/10  Iteration 405/1780 Training loss: 2.0652 5.3697 sec/batch\n",
      "Epoch 3/10  Iteration 406/1780 Training loss: 2.0651 5.3059 sec/batch\n",
      "Epoch 3/10  Iteration 407/1780 Training loss: 2.0638 5.2769 sec/batch\n",
      "Epoch 3/10  Iteration 408/1780 Training loss: 2.0637 5.5902 sec/batch\n",
      "Epoch 3/10  Iteration 409/1780 Training loss: 2.0627 5.3023 sec/batch\n",
      "Epoch 3/10  Iteration 410/1780 Training loss: 2.0619 5.2761 sec/batch\n",
      "Epoch 3/10  Iteration 411/1780 Training loss: 2.0609 5.2949 sec/batch\n",
      "Epoch 3/10  Iteration 412/1780 Training loss: 2.0603 5.4082 sec/batch\n",
      "Epoch 3/10  Iteration 413/1780 Training loss: 2.0596 6.3174 sec/batch\n",
      "Epoch 3/10  Iteration 414/1780 Training loss: 2.0587 5.4162 sec/batch\n",
      "Epoch 3/10  Iteration 415/1780 Training loss: 2.0576 5.4317 sec/batch\n",
      "Epoch 3/10  Iteration 416/1780 Training loss: 2.0576 5.2687 sec/batch\n",
      "Epoch 3/10  Iteration 417/1780 Training loss: 2.0566 5.2501 sec/batch\n",
      "Epoch 3/10  Iteration 418/1780 Training loss: 2.0565 5.2708 sec/batch\n",
      "Epoch 3/10  Iteration 419/1780 Training loss: 2.0563 5.2924 sec/batch\n",
      "Epoch 3/10  Iteration 420/1780 Training loss: 2.0557 5.2317 sec/batch\n",
      "Epoch 3/10  Iteration 421/1780 Training loss: 2.0548 5.2510 sec/batch\n",
      "Epoch 3/10  Iteration 422/1780 Training loss: 2.0545 5.2834 sec/batch\n",
      "Epoch 3/10  Iteration 423/1780 Training loss: 2.0540 5.2629 sec/batch\n",
      "Epoch 3/10  Iteration 424/1780 Training loss: 2.0528 5.2463 sec/batch\n",
      "Epoch 3/10  Iteration 425/1780 Training loss: 2.0519 5.2673 sec/batch\n",
      "Epoch 3/10  Iteration 426/1780 Training loss: 2.0512 5.2484 sec/batch\n",
      "Epoch 3/10  Iteration 427/1780 Training loss: 2.0510 5.2624 sec/batch\n",
      "Epoch 3/10  Iteration 428/1780 Training loss: 2.0504 5.5251 sec/batch\n",
      "Epoch 3/10  Iteration 429/1780 Training loss: 2.0500 5.2320 sec/batch\n",
      "Epoch 3/10  Iteration 430/1780 Training loss: 2.0491 5.3254 sec/batch\n",
      "Epoch 3/10  Iteration 431/1780 Training loss: 2.0483 5.5750 sec/batch\n",
      "Epoch 3/10  Iteration 432/1780 Training loss: 2.0480 5.7084 sec/batch\n",
      "Epoch 3/10  Iteration 433/1780 Training loss: 2.0471 5.4075 sec/batch\n",
      "Epoch 3/10  Iteration 434/1780 Training loss: 2.0466 5.2809 sec/batch\n",
      "Epoch 3/10  Iteration 435/1780 Training loss: 2.0455 5.2617 sec/batch\n",
      "Epoch 3/10  Iteration 436/1780 Training loss: 2.0447 5.3313 sec/batch\n",
      "Epoch 3/10  Iteration 437/1780 Training loss: 2.0436 5.2877 sec/batch\n",
      "Epoch 3/10  Iteration 438/1780 Training loss: 2.0431 5.2632 sec/batch\n",
      "Epoch 3/10  Iteration 439/1780 Training loss: 2.0419 5.2832 sec/batch\n",
      "Epoch 3/10  Iteration 440/1780 Training loss: 2.0412 5.2620 sec/batch\n",
      "Epoch 3/10  Iteration 441/1780 Training loss: 2.0401 5.2247 sec/batch\n",
      "Epoch 3/10  Iteration 442/1780 Training loss: 2.0393 5.4106 sec/batch\n",
      "Epoch 3/10  Iteration 443/1780 Training loss: 2.0386 5.2746 sec/batch\n",
      "Epoch 3/10  Iteration 444/1780 Training loss: 2.0376 5.3150 sec/batch\n",
      "Epoch 3/10  Iteration 445/1780 Training loss: 2.0365 5.2611 sec/batch\n",
      "Epoch 3/10  Iteration 446/1780 Training loss: 2.0360 5.2326 sec/batch\n",
      "Epoch 3/10  Iteration 447/1780 Training loss: 2.0351 5.2986 sec/batch\n",
      "Epoch 3/10  Iteration 448/1780 Training loss: 2.0344 5.4993 sec/batch\n",
      "Epoch 3/10  Iteration 449/1780 Training loss: 2.0332 5.2689 sec/batch\n",
      "Epoch 3/10  Iteration 450/1780 Training loss: 2.0323 5.3028 sec/batch\n",
      "Epoch 3/10  Iteration 451/1780 Training loss: 2.0313 5.2935 sec/batch\n",
      "Epoch 3/10  Iteration 452/1780 Training loss: 2.0305 5.2951 sec/batch\n",
      "Epoch 3/10  Iteration 453/1780 Training loss: 2.0298 5.3665 sec/batch\n",
      "Epoch 3/10  Iteration 454/1780 Training loss: 2.0289 5.2850 sec/batch\n",
      "Epoch 3/10  Iteration 455/1780 Training loss: 2.0278 5.2885 sec/batch\n",
      "Epoch 3/10  Iteration 456/1780 Training loss: 2.0266 5.2765 sec/batch\n",
      "Epoch 3/10  Iteration 457/1780 Training loss: 2.0260 5.3109 sec/batch\n",
      "Epoch 3/10  Iteration 458/1780 Training loss: 2.0254 5.3223 sec/batch\n",
      "Epoch 3/10  Iteration 459/1780 Training loss: 2.0244 5.2618 sec/batch\n",
      "Epoch 3/10  Iteration 460/1780 Training loss: 2.0236 5.2833 sec/batch\n",
      "Epoch 3/10  Iteration 461/1780 Training loss: 2.0227 5.2827 sec/batch\n",
      "Epoch 3/10  Iteration 462/1780 Training loss: 2.0219 5.2402 sec/batch\n",
      "Epoch 3/10  Iteration 463/1780 Training loss: 2.0212 5.2681 sec/batch\n",
      "Epoch 3/10  Iteration 464/1780 Training loss: 2.0208 5.2811 sec/batch\n",
      "Epoch 3/10  Iteration 465/1780 Training loss: 2.0202 5.2656 sec/batch\n",
      "Epoch 3/10  Iteration 466/1780 Training loss: 2.0196 5.2809 sec/batch\n",
      "Epoch 3/10  Iteration 467/1780 Training loss: 2.0189 5.2927 sec/batch\n",
      "Epoch 3/10  Iteration 468/1780 Training loss: 2.0182 5.2584 sec/batch\n",
      "Epoch 3/10  Iteration 469/1780 Training loss: 2.0174 5.2014 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10  Iteration 470/1780 Training loss: 2.0167 5.2321 sec/batch\n",
      "Epoch 3/10  Iteration 471/1780 Training loss: 2.0159 5.1999 sec/batch\n",
      "Epoch 3/10  Iteration 472/1780 Training loss: 2.0150 5.2752 sec/batch\n",
      "Epoch 3/10  Iteration 473/1780 Training loss: 2.0142 5.2292 sec/batch\n",
      "Epoch 3/10  Iteration 474/1780 Training loss: 2.0135 5.1965 sec/batch\n",
      "Epoch 3/10  Iteration 475/1780 Training loss: 2.0129 5.2663 sec/batch\n",
      "Epoch 3/10  Iteration 476/1780 Training loss: 2.0122 5.3606 sec/batch\n",
      "Epoch 3/10  Iteration 477/1780 Training loss: 2.0117 5.5163 sec/batch\n",
      "Epoch 3/10  Iteration 478/1780 Training loss: 2.0109 5.9272 sec/batch\n",
      "Epoch 3/10  Iteration 479/1780 Training loss: 2.0100 5.4973 sec/batch\n",
      "Epoch 3/10  Iteration 480/1780 Training loss: 2.0096 5.2069 sec/batch\n",
      "Epoch 3/10  Iteration 481/1780 Training loss: 2.0089 5.2316 sec/batch\n",
      "Epoch 3/10  Iteration 482/1780 Training loss: 2.0079 5.2441 sec/batch\n",
      "Epoch 3/10  Iteration 483/1780 Training loss: 2.0074 5.1920 sec/batch\n",
      "Epoch 3/10  Iteration 484/1780 Training loss: 2.0068 5.2158 sec/batch\n",
      "Epoch 3/10  Iteration 485/1780 Training loss: 2.0061 5.2440 sec/batch\n",
      "Epoch 3/10  Iteration 486/1780 Training loss: 2.0055 5.2194 sec/batch\n",
      "Epoch 3/10  Iteration 487/1780 Training loss: 2.0046 5.1941 sec/batch\n",
      "Epoch 3/10  Iteration 488/1780 Training loss: 2.0037 5.2430 sec/batch\n",
      "Epoch 3/10  Iteration 489/1780 Training loss: 2.0032 5.2056 sec/batch\n",
      "Epoch 3/10  Iteration 490/1780 Training loss: 2.0027 5.2177 sec/batch\n",
      "Epoch 3/10  Iteration 491/1780 Training loss: 2.0020 5.2579 sec/batch\n",
      "Epoch 3/10  Iteration 492/1780 Training loss: 2.0015 5.2142 sec/batch\n",
      "Epoch 3/10  Iteration 493/1780 Training loss: 2.0010 5.1986 sec/batch\n",
      "Epoch 3/10  Iteration 494/1780 Training loss: 2.0005 5.2013 sec/batch\n",
      "Epoch 3/10  Iteration 495/1780 Training loss: 2.0002 5.2339 sec/batch\n",
      "Epoch 3/10  Iteration 496/1780 Training loss: 1.9995 5.1976 sec/batch\n",
      "Epoch 3/10  Iteration 497/1780 Training loss: 1.9991 5.2111 sec/batch\n",
      "Epoch 3/10  Iteration 498/1780 Training loss: 1.9984 5.3825 sec/batch\n",
      "Epoch 3/10  Iteration 499/1780 Training loss: 1.9978 5.4894 sec/batch\n",
      "Epoch 3/10  Iteration 500/1780 Training loss: 1.9972 5.1881 sec/batch\n",
      "Validation loss: 1.81061 Saving checkpoint!\n",
      "Epoch 3/10  Iteration 501/1780 Training loss: 1.9968 5.2003 sec/batch\n",
      "Epoch 3/10  Iteration 502/1780 Training loss: 1.9963 5.3913 sec/batch\n",
      "Epoch 3/10  Iteration 503/1780 Training loss: 1.9958 5.3427 sec/batch\n",
      "Epoch 3/10  Iteration 504/1780 Training loss: 1.9954 5.4171 sec/batch\n",
      "Epoch 3/10  Iteration 505/1780 Training loss: 1.9949 5.3756 sec/batch\n",
      "Epoch 3/10  Iteration 506/1780 Training loss: 1.9942 4.7886 sec/batch\n",
      "Epoch 3/10  Iteration 507/1780 Training loss: 1.9936 4.0615 sec/batch\n",
      "Epoch 3/10  Iteration 508/1780 Training loss: 1.9932 4.6355 sec/batch\n",
      "Epoch 3/10  Iteration 509/1780 Training loss: 1.9927 4.3888 sec/batch\n",
      "Epoch 3/10  Iteration 510/1780 Training loss: 1.9922 4.0082 sec/batch\n",
      "Epoch 3/10  Iteration 511/1780 Training loss: 1.9916 4.3410 sec/batch\n",
      "Epoch 3/10  Iteration 512/1780 Training loss: 1.9911 4.3275 sec/batch\n",
      "Epoch 3/10  Iteration 513/1780 Training loss: 1.9905 3.9809 sec/batch\n",
      "Epoch 3/10  Iteration 514/1780 Training loss: 1.9899 4.0130 sec/batch\n",
      "Epoch 3/10  Iteration 515/1780 Training loss: 1.9892 4.0381 sec/batch\n",
      "Epoch 3/10  Iteration 516/1780 Training loss: 1.9888 4.6345 sec/batch\n",
      "Epoch 3/10  Iteration 517/1780 Training loss: 1.9884 3.8098 sec/batch\n",
      "Epoch 3/10  Iteration 518/1780 Training loss: 1.9878 3.7842 sec/batch\n",
      "Epoch 3/10  Iteration 519/1780 Training loss: 1.9874 3.7689 sec/batch\n",
      "Epoch 3/10  Iteration 520/1780 Training loss: 1.9868 3.7677 sec/batch\n",
      "Epoch 3/10  Iteration 521/1780 Training loss: 1.9862 3.7742 sec/batch\n",
      "Epoch 3/10  Iteration 522/1780 Training loss: 1.9855 3.7767 sec/batch\n",
      "Epoch 3/10  Iteration 523/1780 Training loss: 1.9850 3.7611 sec/batch\n",
      "Epoch 3/10  Iteration 524/1780 Training loss: 1.9848 3.7817 sec/batch\n",
      "Epoch 3/10  Iteration 525/1780 Training loss: 1.9842 3.7629 sec/batch\n",
      "Epoch 3/10  Iteration 526/1780 Training loss: 1.9836 3.7595 sec/batch\n",
      "Epoch 3/10  Iteration 527/1780 Training loss: 1.9829 3.7819 sec/batch\n",
      "Epoch 3/10  Iteration 528/1780 Training loss: 1.9823 3.7594 sec/batch\n",
      "Epoch 3/10  Iteration 529/1780 Training loss: 1.9818 3.7706 sec/batch\n",
      "Epoch 3/10  Iteration 530/1780 Training loss: 1.9813 3.7800 sec/batch\n",
      "Epoch 3/10  Iteration 531/1780 Training loss: 1.9808 3.7572 sec/batch\n",
      "Epoch 3/10  Iteration 532/1780 Training loss: 1.9802 3.7924 sec/batch\n",
      "Epoch 3/10  Iteration 533/1780 Training loss: 1.9795 3.7675 sec/batch\n",
      "Epoch 3/10  Iteration 534/1780 Training loss: 1.9791 3.7673 sec/batch\n",
      "Epoch 4/10  Iteration 535/1780 Training loss: 1.9619 3.7634 sec/batch\n",
      "Epoch 4/10  Iteration 536/1780 Training loss: 1.9140 3.9858 sec/batch\n",
      "Epoch 4/10  Iteration 537/1780 Training loss: 1.8971 3.7802 sec/batch\n",
      "Epoch 4/10  Iteration 538/1780 Training loss: 1.8892 3.7734 sec/batch\n",
      "Epoch 4/10  Iteration 539/1780 Training loss: 1.8835 3.7696 sec/batch\n",
      "Epoch 4/10  Iteration 540/1780 Training loss: 1.8728 3.7591 sec/batch\n",
      "Epoch 4/10  Iteration 541/1780 Training loss: 1.8749 3.7774 sec/batch\n",
      "Epoch 4/10  Iteration 542/1780 Training loss: 1.8728 3.7581 sec/batch\n",
      "Epoch 4/10  Iteration 543/1780 Training loss: 1.8760 3.7723 sec/batch\n",
      "Epoch 4/10  Iteration 544/1780 Training loss: 1.8768 3.7726 sec/batch\n",
      "Epoch 4/10  Iteration 545/1780 Training loss: 1.8730 3.7648 sec/batch\n",
      "Epoch 4/10  Iteration 546/1780 Training loss: 1.8708 3.7550 sec/batch\n",
      "Epoch 4/10  Iteration 547/1780 Training loss: 1.8715 3.8197 sec/batch\n",
      "Epoch 4/10  Iteration 548/1780 Training loss: 1.8739 3.8706 sec/batch\n",
      "Epoch 4/10  Iteration 549/1780 Training loss: 1.8730 3.7843 sec/batch\n",
      "Epoch 4/10  Iteration 550/1780 Training loss: 1.8707 3.7662 sec/batch\n",
      "Epoch 4/10  Iteration 551/1780 Training loss: 1.8703 3.7750 sec/batch\n",
      "Epoch 4/10  Iteration 552/1780 Training loss: 1.8720 3.7654 sec/batch\n",
      "Epoch 4/10  Iteration 553/1780 Training loss: 1.8714 3.7639 sec/batch\n",
      "Epoch 4/10  Iteration 554/1780 Training loss: 1.8710 3.7671 sec/batch\n",
      "Epoch 4/10  Iteration 555/1780 Training loss: 1.8704 3.7784 sec/batch\n",
      "Epoch 4/10  Iteration 556/1780 Training loss: 1.8718 3.7684 sec/batch\n",
      "Epoch 4/10  Iteration 557/1780 Training loss: 1.8707 3.7750 sec/batch\n",
      "Epoch 4/10  Iteration 558/1780 Training loss: 1.8699 3.7680 sec/batch\n",
      "Epoch 4/10  Iteration 559/1780 Training loss: 1.8697 3.7720 sec/batch\n",
      "Epoch 4/10  Iteration 560/1780 Training loss: 1.8681 3.7554 sec/batch\n",
      "Epoch 4/10  Iteration 561/1780 Training loss: 1.8666 3.7812 sec/batch\n",
      "Epoch 4/10  Iteration 562/1780 Training loss: 1.8666 3.7541 sec/batch\n",
      "Epoch 4/10  Iteration 563/1780 Training loss: 1.8670 3.7778 sec/batch\n",
      "Epoch 4/10  Iteration 564/1780 Training loss: 1.8669 3.9926 sec/batch\n",
      "Epoch 4/10  Iteration 565/1780 Training loss: 1.8664 3.8005 sec/batch\n",
      "Epoch 4/10  Iteration 566/1780 Training loss: 1.8648 3.7949 sec/batch\n",
      "Epoch 4/10  Iteration 567/1780 Training loss: 1.8645 3.7811 sec/batch\n",
      "Epoch 4/10  Iteration 568/1780 Training loss: 1.8644 3.7647 sec/batch\n",
      "Epoch 4/10  Iteration 569/1780 Training loss: 1.8635 3.7700 sec/batch\n",
      "Epoch 4/10  Iteration 570/1780 Training loss: 1.8626 3.7803 sec/batch\n",
      "Epoch 4/10  Iteration 571/1780 Training loss: 1.8618 3.7651 sec/batch\n",
      "Epoch 4/10  Iteration 572/1780 Training loss: 1.8605 3.7636 sec/batch\n",
      "Epoch 4/10  Iteration 573/1780 Training loss: 1.8588 3.8580 sec/batch\n",
      "Epoch 4/10  Iteration 574/1780 Training loss: 1.8576 3.8101 sec/batch\n",
      "Epoch 4/10  Iteration 575/1780 Training loss: 1.8566 3.7606 sec/batch\n",
      "Epoch 4/10  Iteration 576/1780 Training loss: 1.8562 3.7771 sec/batch\n",
      "Epoch 4/10  Iteration 577/1780 Training loss: 1.8553 3.7953 sec/batch\n",
      "Epoch 4/10  Iteration 578/1780 Training loss: 1.8541 4.2497 sec/batch\n",
      "Epoch 4/10  Iteration 579/1780 Training loss: 1.8540 4.3818 sec/batch\n",
      "Epoch 4/10  Iteration 580/1780 Training loss: 1.8523 3.8159 sec/batch\n",
      "Epoch 4/10  Iteration 581/1780 Training loss: 1.8517 3.8738 sec/batch\n",
      "Epoch 4/10  Iteration 582/1780 Training loss: 1.8506 3.7742 sec/batch\n",
      "Epoch 4/10  Iteration 583/1780 Training loss: 1.8500 3.7696 sec/batch\n",
      "Epoch 4/10  Iteration 584/1780 Training loss: 1.8504 3.7694 sec/batch\n",
      "Epoch 4/10  Iteration 585/1780 Training loss: 1.8493 3.7930 sec/batch\n",
      "Epoch 4/10  Iteration 586/1780 Training loss: 1.8497 3.7613 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10  Iteration 587/1780 Training loss: 1.8490 3.7644 sec/batch\n",
      "Epoch 4/10  Iteration 588/1780 Training loss: 1.8486 3.7611 sec/batch\n",
      "Epoch 4/10  Iteration 589/1780 Training loss: 1.8479 3.7606 sec/batch\n",
      "Epoch 4/10  Iteration 590/1780 Training loss: 1.8476 3.7587 sec/batch\n",
      "Epoch 4/10  Iteration 591/1780 Training loss: 1.8474 3.7511 sec/batch\n",
      "Epoch 4/10  Iteration 592/1780 Training loss: 1.8467 3.8474 sec/batch\n",
      "Epoch 4/10  Iteration 593/1780 Training loss: 1.8459 3.8562 sec/batch\n",
      "Epoch 4/10  Iteration 594/1780 Training loss: 1.8463 3.7574 sec/batch\n",
      "Epoch 4/10  Iteration 595/1780 Training loss: 1.8460 3.7981 sec/batch\n",
      "Epoch 4/10  Iteration 596/1780 Training loss: 1.8463 3.7679 sec/batch\n",
      "Epoch 4/10  Iteration 597/1780 Training loss: 1.8466 3.7564 sec/batch\n",
      "Epoch 4/10  Iteration 598/1780 Training loss: 1.8467 3.7819 sec/batch\n",
      "Epoch 4/10  Iteration 599/1780 Training loss: 1.8463 3.7734 sec/batch\n",
      "Epoch 4/10  Iteration 600/1780 Training loss: 1.8465 3.7637 sec/batch\n",
      "Validation loss: 1.71491 Saving checkpoint!\n",
      "Epoch 4/10  Iteration 601/1780 Training loss: 1.8468 3.6944 sec/batch\n",
      "Epoch 4/10  Iteration 602/1780 Training loss: 1.8460 3.7613 sec/batch\n",
      "Epoch 4/10  Iteration 603/1780 Training loss: 1.8455 3.7955 sec/batch\n",
      "Epoch 4/10  Iteration 604/1780 Training loss: 1.8451 3.8741 sec/batch\n",
      "Epoch 4/10  Iteration 605/1780 Training loss: 1.8452 3.7607 sec/batch\n",
      "Epoch 4/10  Iteration 606/1780 Training loss: 1.8449 3.7695 sec/batch\n",
      "Epoch 4/10  Iteration 607/1780 Training loss: 1.8448 3.7650 sec/batch\n",
      "Epoch 4/10  Iteration 608/1780 Training loss: 1.8441 3.7730 sec/batch\n",
      "Epoch 4/10  Iteration 609/1780 Training loss: 1.8436 3.7583 sec/batch\n",
      "Epoch 4/10  Iteration 610/1780 Training loss: 1.8436 3.7841 sec/batch\n",
      "Epoch 4/10  Iteration 611/1780 Training loss: 1.8429 3.7661 sec/batch\n",
      "Epoch 4/10  Iteration 612/1780 Training loss: 1.8426 3.7658 sec/batch\n",
      "Epoch 4/10  Iteration 613/1780 Training loss: 1.8416 3.7684 sec/batch\n",
      "Epoch 4/10  Iteration 614/1780 Training loss: 1.8410 3.7787 sec/batch\n",
      "Epoch 4/10  Iteration 615/1780 Training loss: 1.8401 3.7623 sec/batch\n",
      "Epoch 4/10  Iteration 616/1780 Training loss: 1.8398 3.7556 sec/batch\n",
      "Epoch 4/10  Iteration 617/1780 Training loss: 1.8389 3.7543 sec/batch\n",
      "Epoch 4/10  Iteration 618/1780 Training loss: 1.8385 3.7638 sec/batch\n",
      "Epoch 4/10  Iteration 619/1780 Training loss: 1.8376 3.7661 sec/batch\n",
      "Epoch 4/10  Iteration 620/1780 Training loss: 1.8369 3.8193 sec/batch\n",
      "Epoch 4/10  Iteration 621/1780 Training loss: 1.8363 3.7490 sec/batch\n",
      "Epoch 4/10  Iteration 622/1780 Training loss: 1.8355 3.7689 sec/batch\n",
      "Epoch 4/10  Iteration 623/1780 Training loss: 1.8346 3.7621 sec/batch\n",
      "Epoch 4/10  Iteration 624/1780 Training loss: 1.8343 3.7657 sec/batch\n",
      "Epoch 4/10  Iteration 625/1780 Training loss: 1.8336 3.7740 sec/batch\n",
      "Epoch 4/10  Iteration 626/1780 Training loss: 1.8331 3.7764 sec/batch\n",
      "Epoch 4/10  Iteration 627/1780 Training loss: 1.8322 3.7555 sec/batch\n",
      "Epoch 4/10  Iteration 628/1780 Training loss: 1.8315 3.7526 sec/batch\n",
      "Epoch 4/10  Iteration 629/1780 Training loss: 1.8307 3.7608 sec/batch\n",
      "Epoch 4/10  Iteration 630/1780 Training loss: 1.8303 3.7607 sec/batch\n",
      "Epoch 4/10  Iteration 631/1780 Training loss: 1.8297 3.7612 sec/batch\n",
      "Epoch 4/10  Iteration 632/1780 Training loss: 1.8289 3.7548 sec/batch\n",
      "Epoch 4/10  Iteration 633/1780 Training loss: 1.8282 3.7535 sec/batch\n",
      "Epoch 4/10  Iteration 634/1780 Training loss: 1.8273 3.7781 sec/batch\n",
      "Epoch 4/10  Iteration 635/1780 Training loss: 1.8270 3.8067 sec/batch\n",
      "Epoch 4/10  Iteration 636/1780 Training loss: 1.8265 4.0035 sec/batch\n",
      "Epoch 4/10  Iteration 637/1780 Training loss: 1.8258 3.7848 sec/batch\n",
      "Epoch 4/10  Iteration 638/1780 Training loss: 1.8252 3.7613 sec/batch\n",
      "Epoch 4/10  Iteration 639/1780 Training loss: 1.8246 3.7933 sec/batch\n",
      "Epoch 4/10  Iteration 640/1780 Training loss: 1.8241 3.7673 sec/batch\n",
      "Epoch 4/10  Iteration 641/1780 Training loss: 1.8236 3.7721 sec/batch\n",
      "Epoch 4/10  Iteration 642/1780 Training loss: 1.8233 3.7495 sec/batch\n",
      "Epoch 4/10  Iteration 643/1780 Training loss: 1.8229 3.7593 sec/batch\n",
      "Epoch 4/10  Iteration 644/1780 Training loss: 1.8225 3.7690 sec/batch\n",
      "Epoch 4/10  Iteration 645/1780 Training loss: 1.8221 3.7607 sec/batch\n",
      "Epoch 4/10  Iteration 646/1780 Training loss: 1.8215 3.7639 sec/batch\n",
      "Epoch 4/10  Iteration 647/1780 Training loss: 1.8209 3.7737 sec/batch\n",
      "Epoch 4/10  Iteration 648/1780 Training loss: 1.8204 3.7644 sec/batch\n",
      "Epoch 4/10  Iteration 649/1780 Training loss: 1.8198 3.7722 sec/batch\n",
      "Epoch 4/10  Iteration 650/1780 Training loss: 1.8189 3.7958 sec/batch\n",
      "Epoch 4/10  Iteration 651/1780 Training loss: 1.8184 4.3903 sec/batch\n",
      "Epoch 4/10  Iteration 652/1780 Training loss: 1.8179 3.8130 sec/batch\n",
      "Epoch 4/10  Iteration 653/1780 Training loss: 1.8174 3.8052 sec/batch\n",
      "Epoch 4/10  Iteration 654/1780 Training loss: 1.8170 3.7452 sec/batch\n",
      "Epoch 4/10  Iteration 655/1780 Training loss: 1.8166 3.7691 sec/batch\n",
      "Epoch 4/10  Iteration 656/1780 Training loss: 1.8159 3.7740 sec/batch\n",
      "Epoch 4/10  Iteration 657/1780 Training loss: 1.8153 3.8811 sec/batch\n",
      "Epoch 4/10  Iteration 658/1780 Training loss: 1.8149 3.7840 sec/batch\n",
      "Epoch 4/10  Iteration 659/1780 Training loss: 1.8144 3.7870 sec/batch\n",
      "Epoch 4/10  Iteration 660/1780 Training loss: 1.8136 3.8259 sec/batch\n",
      "Epoch 4/10  Iteration 661/1780 Training loss: 1.8133 3.7779 sec/batch\n",
      "Epoch 4/10  Iteration 662/1780 Training loss: 1.8130 3.7499 sec/batch\n",
      "Epoch 4/10  Iteration 663/1780 Training loss: 1.8124 3.7704 sec/batch\n",
      "Epoch 4/10  Iteration 664/1780 Training loss: 1.8120 3.7689 sec/batch\n",
      "Epoch 4/10  Iteration 665/1780 Training loss: 1.8113 3.7547 sec/batch\n",
      "Epoch 4/10  Iteration 666/1780 Training loss: 1.8107 3.7525 sec/batch\n",
      "Epoch 4/10  Iteration 667/1780 Training loss: 1.8103 3.8424 sec/batch\n",
      "Epoch 4/10  Iteration 668/1780 Training loss: 1.8098 3.8371 sec/batch\n",
      "Epoch 4/10  Iteration 669/1780 Training loss: 1.8094 3.8821 sec/batch\n",
      "Epoch 4/10  Iteration 670/1780 Training loss: 1.8091 3.8440 sec/batch\n",
      "Epoch 4/10  Iteration 671/1780 Training loss: 1.8088 3.8047 sec/batch\n",
      "Epoch 4/10  Iteration 672/1780 Training loss: 1.8085 3.7642 sec/batch\n",
      "Epoch 4/10  Iteration 673/1780 Training loss: 1.8082 3.7599 sec/batch\n",
      "Epoch 4/10  Iteration 674/1780 Training loss: 1.8077 3.7454 sec/batch\n",
      "Epoch 4/10  Iteration 675/1780 Training loss: 1.8076 3.7744 sec/batch\n",
      "Epoch 4/10  Iteration 676/1780 Training loss: 1.8072 3.7538 sec/batch\n",
      "Epoch 4/10  Iteration 677/1780 Training loss: 1.8067 3.7547 sec/batch\n",
      "Epoch 4/10  Iteration 678/1780 Training loss: 1.8065 3.7699 sec/batch\n",
      "Epoch 4/10  Iteration 679/1780 Training loss: 1.8059 3.7965 sec/batch\n",
      "Epoch 4/10  Iteration 680/1780 Training loss: 1.8057 4.2856 sec/batch\n",
      "Epoch 4/10  Iteration 681/1780 Training loss: 1.8053 4.5698 sec/batch\n",
      "Epoch 4/10  Iteration 682/1780 Training loss: 1.8051 4.3130 sec/batch\n",
      "Epoch 4/10  Iteration 683/1780 Training loss: 1.8048 3.8277 sec/batch\n",
      "Epoch 4/10  Iteration 684/1780 Training loss: 1.8043 3.8006 sec/batch\n",
      "Epoch 4/10  Iteration 685/1780 Training loss: 1.8038 3.7650 sec/batch\n",
      "Epoch 4/10  Iteration 686/1780 Training loss: 1.8035 3.7707 sec/batch\n",
      "Epoch 4/10  Iteration 687/1780 Training loss: 1.8033 3.7695 sec/batch\n",
      "Epoch 4/10  Iteration 688/1780 Training loss: 1.8030 3.7661 sec/batch\n",
      "Epoch 4/10  Iteration 689/1780 Training loss: 1.8026 3.7609 sec/batch\n",
      "Epoch 4/10  Iteration 690/1780 Training loss: 1.8022 3.7624 sec/batch\n",
      "Epoch 4/10  Iteration 691/1780 Training loss: 1.8020 3.7680 sec/batch\n",
      "Epoch 4/10  Iteration 692/1780 Training loss: 1.8016 3.7636 sec/batch\n",
      "Epoch 4/10  Iteration 693/1780 Training loss: 1.8011 3.7678 sec/batch\n",
      "Epoch 4/10  Iteration 694/1780 Training loss: 1.8009 3.7528 sec/batch\n",
      "Epoch 4/10  Iteration 695/1780 Training loss: 1.8007 3.7567 sec/batch\n",
      "Epoch 4/10  Iteration 696/1780 Training loss: 1.8004 3.7790 sec/batch\n",
      "Epoch 4/10  Iteration 697/1780 Training loss: 1.8000 3.9251 sec/batch\n",
      "Epoch 4/10  Iteration 698/1780 Training loss: 1.7997 3.8474 sec/batch\n",
      "Epoch 4/10  Iteration 699/1780 Training loss: 1.7993 3.8347 sec/batch\n",
      "Epoch 4/10  Iteration 700/1780 Training loss: 1.7989 3.7863 sec/batch\n",
      "Validation loss: 1.62002 Saving checkpoint!\n",
      "Epoch 4/10  Iteration 701/1780 Training loss: 1.7990 3.6912 sec/batch\n",
      "Epoch 4/10  Iteration 702/1780 Training loss: 1.7990 3.7752 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10  Iteration 703/1780 Training loss: 1.7986 3.7656 sec/batch\n",
      "Epoch 4/10  Iteration 704/1780 Training loss: 1.7982 3.7535 sec/batch\n",
      "Epoch 4/10  Iteration 705/1780 Training loss: 1.7977 3.7576 sec/batch\n",
      "Epoch 4/10  Iteration 706/1780 Training loss: 1.7972 3.7686 sec/batch\n",
      "Epoch 4/10  Iteration 707/1780 Training loss: 1.7970 3.8079 sec/batch\n",
      "Epoch 4/10  Iteration 708/1780 Training loss: 1.7966 3.8159 sec/batch\n",
      "Epoch 4/10  Iteration 709/1780 Training loss: 1.7964 3.7758 sec/batch\n",
      "Epoch 4/10  Iteration 710/1780 Training loss: 1.7959 3.7647 sec/batch\n",
      "Epoch 4/10  Iteration 711/1780 Training loss: 1.7955 3.7615 sec/batch\n",
      "Epoch 4/10  Iteration 712/1780 Training loss: 1.7951 3.7599 sec/batch\n",
      "Epoch 5/10  Iteration 713/1780 Training loss: 1.8132 3.7588 sec/batch\n",
      "Epoch 5/10  Iteration 714/1780 Training loss: 1.7702 3.7953 sec/batch\n",
      "Epoch 5/10  Iteration 715/1780 Training loss: 1.7532 3.7728 sec/batch\n",
      "Epoch 5/10  Iteration 716/1780 Training loss: 1.7464 3.7498 sec/batch\n",
      "Epoch 5/10  Iteration 717/1780 Training loss: 1.7389 3.7651 sec/batch\n",
      "Epoch 5/10  Iteration 718/1780 Training loss: 1.7269 3.8171 sec/batch\n",
      "Epoch 5/10  Iteration 719/1780 Training loss: 1.7278 3.8663 sec/batch\n",
      "Epoch 5/10  Iteration 720/1780 Training loss: 1.7265 3.7636 sec/batch\n",
      "Epoch 5/10  Iteration 721/1780 Training loss: 1.7292 3.7700 sec/batch\n",
      "Epoch 5/10  Iteration 722/1780 Training loss: 1.7279 3.7557 sec/batch\n",
      "Epoch 5/10  Iteration 723/1780 Training loss: 1.7236 3.8576 sec/batch\n",
      "Epoch 5/10  Iteration 724/1780 Training loss: 1.7221 3.8871 sec/batch\n",
      "Epoch 5/10  Iteration 725/1780 Training loss: 1.7220 3.9290 sec/batch\n",
      "Epoch 5/10  Iteration 726/1780 Training loss: 1.7243 3.7926 sec/batch\n",
      "Epoch 5/10  Iteration 727/1780 Training loss: 1.7232 3.7896 sec/batch\n",
      "Epoch 5/10  Iteration 728/1780 Training loss: 1.7216 3.7708 sec/batch\n",
      "Epoch 5/10  Iteration 729/1780 Training loss: 1.7212 3.7694 sec/batch\n",
      "Epoch 5/10  Iteration 730/1780 Training loss: 1.7227 3.7753 sec/batch\n",
      "Epoch 5/10  Iteration 731/1780 Training loss: 1.7229 3.7608 sec/batch\n",
      "Epoch 5/10  Iteration 732/1780 Training loss: 1.7234 3.7604 sec/batch\n",
      "Epoch 5/10  Iteration 733/1780 Training loss: 1.7224 3.7790 sec/batch\n",
      "Epoch 5/10  Iteration 734/1780 Training loss: 1.7235 3.7583 sec/batch\n",
      "Epoch 5/10  Iteration 735/1780 Training loss: 1.7224 3.7588 sec/batch\n",
      "Epoch 5/10  Iteration 736/1780 Training loss: 1.7215 3.7699 sec/batch\n",
      "Epoch 5/10  Iteration 737/1780 Training loss: 1.7213 3.7525 sec/batch\n",
      "Epoch 5/10  Iteration 738/1780 Training loss: 1.7195 3.7638 sec/batch\n",
      "Epoch 5/10  Iteration 739/1780 Training loss: 1.7179 3.8041 sec/batch\n",
      "Epoch 5/10  Iteration 740/1780 Training loss: 1.7179 3.7561 sec/batch\n",
      "Epoch 5/10  Iteration 741/1780 Training loss: 1.7180 3.7528 sec/batch\n",
      "Epoch 5/10  Iteration 742/1780 Training loss: 1.7176 3.7737 sec/batch\n",
      "Epoch 5/10  Iteration 743/1780 Training loss: 1.7170 3.7720 sec/batch\n",
      "Epoch 5/10  Iteration 744/1780 Training loss: 1.7158 3.7647 sec/batch\n",
      "Epoch 5/10  Iteration 745/1780 Training loss: 1.7159 3.7581 sec/batch\n",
      "Epoch 5/10  Iteration 746/1780 Training loss: 1.7162 3.7671 sec/batch\n",
      "Epoch 5/10  Iteration 747/1780 Training loss: 1.7156 3.9201 sec/batch\n",
      "Epoch 5/10  Iteration 748/1780 Training loss: 1.7150 3.7673 sec/batch\n",
      "Epoch 5/10  Iteration 749/1780 Training loss: 1.7141 3.7689 sec/batch\n",
      "Epoch 5/10  Iteration 750/1780 Training loss: 1.7126 3.7592 sec/batch\n",
      "Epoch 5/10  Iteration 751/1780 Training loss: 1.7111 3.7742 sec/batch\n",
      "Epoch 5/10  Iteration 752/1780 Training loss: 1.7102 3.7777 sec/batch\n",
      "Epoch 5/10  Iteration 753/1780 Training loss: 1.7094 3.7681 sec/batch\n",
      "Epoch 5/10  Iteration 754/1780 Training loss: 1.7094 3.8083 sec/batch\n",
      "Epoch 5/10  Iteration 755/1780 Training loss: 1.7085 3.8814 sec/batch\n",
      "Epoch 5/10  Iteration 756/1780 Training loss: 1.7074 3.7676 sec/batch\n",
      "Epoch 5/10  Iteration 757/1780 Training loss: 1.7073 3.7695 sec/batch\n",
      "Epoch 5/10  Iteration 758/1780 Training loss: 1.7060 3.7541 sec/batch\n",
      "Epoch 5/10  Iteration 759/1780 Training loss: 1.7054 3.7716 sec/batch\n",
      "Epoch 5/10  Iteration 760/1780 Training loss: 1.7047 3.7724 sec/batch\n",
      "Epoch 5/10  Iteration 761/1780 Training loss: 1.7042 3.7646 sec/batch\n",
      "Epoch 5/10  Iteration 762/1780 Training loss: 1.7045 3.7576 sec/batch\n",
      "Epoch 5/10  Iteration 763/1780 Training loss: 1.7033 3.7591 sec/batch\n",
      "Epoch 5/10  Iteration 764/1780 Training loss: 1.7041 3.7543 sec/batch\n",
      "Epoch 5/10  Iteration 765/1780 Training loss: 1.7036 3.7805 sec/batch\n",
      "Epoch 5/10  Iteration 766/1780 Training loss: 1.7034 3.7726 sec/batch\n",
      "Epoch 5/10  Iteration 767/1780 Training loss: 1.7027 3.7551 sec/batch\n",
      "Epoch 5/10  Iteration 768/1780 Training loss: 1.7024 3.7614 sec/batch\n",
      "Epoch 5/10  Iteration 769/1780 Training loss: 1.7024 3.7594 sec/batch\n",
      "Epoch 5/10  Iteration 770/1780 Training loss: 1.7017 3.7536 sec/batch\n",
      "Epoch 5/10  Iteration 771/1780 Training loss: 1.7010 3.7743 sec/batch\n",
      "Epoch 5/10  Iteration 772/1780 Training loss: 1.7013 3.7569 sec/batch\n",
      "Epoch 5/10  Iteration 773/1780 Training loss: 1.7009 3.7643 sec/batch\n",
      "Epoch 5/10  Iteration 774/1780 Training loss: 1.7013 3.7634 sec/batch\n",
      "Epoch 5/10  Iteration 775/1780 Training loss: 1.7014 3.7696 sec/batch\n",
      "Epoch 5/10  Iteration 776/1780 Training loss: 1.7014 3.7678 sec/batch\n",
      "Epoch 5/10  Iteration 777/1780 Training loss: 1.7009 3.7626 sec/batch\n",
      "Epoch 5/10  Iteration 778/1780 Training loss: 1.7009 3.7739 sec/batch\n",
      "Epoch 5/10  Iteration 779/1780 Training loss: 1.7009 3.7617 sec/batch\n",
      "Epoch 5/10  Iteration 780/1780 Training loss: 1.7002 3.7624 sec/batch\n",
      "Epoch 5/10  Iteration 781/1780 Training loss: 1.6999 3.7620 sec/batch\n",
      "Epoch 5/10  Iteration 782/1780 Training loss: 1.6996 3.7701 sec/batch\n",
      "Epoch 5/10  Iteration 783/1780 Training loss: 1.6998 3.7585 sec/batch\n",
      "Epoch 5/10  Iteration 784/1780 Training loss: 1.6998 3.7560 sec/batch\n",
      "Epoch 5/10  Iteration 785/1780 Training loss: 1.7000 3.7695 sec/batch\n",
      "Epoch 5/10  Iteration 786/1780 Training loss: 1.6994 3.8039 sec/batch\n",
      "Epoch 5/10  Iteration 787/1780 Training loss: 1.6992 4.0621 sec/batch\n",
      "Epoch 5/10  Iteration 788/1780 Training loss: 1.6992 3.8035 sec/batch\n",
      "Epoch 5/10  Iteration 789/1780 Training loss: 1.6988 3.7675 sec/batch\n",
      "Epoch 5/10  Iteration 790/1780 Training loss: 1.6986 3.7540 sec/batch\n",
      "Epoch 5/10  Iteration 791/1780 Training loss: 1.6977 3.7876 sec/batch\n",
      "Epoch 5/10  Iteration 792/1780 Training loss: 1.6973 3.7710 sec/batch\n",
      "Epoch 5/10  Iteration 793/1780 Training loss: 1.6967 3.7608 sec/batch\n",
      "Epoch 5/10  Iteration 794/1780 Training loss: 1.6966 3.7676 sec/batch\n",
      "Epoch 5/10  Iteration 795/1780 Training loss: 1.6958 3.7653 sec/batch\n",
      "Epoch 5/10  Iteration 796/1780 Training loss: 1.6956 3.7605 sec/batch\n",
      "Epoch 5/10  Iteration 797/1780 Training loss: 1.6949 3.7516 sec/batch\n",
      "Epoch 5/10  Iteration 798/1780 Training loss: 1.6945 3.7847 sec/batch\n",
      "Epoch 5/10  Iteration 799/1780 Training loss: 1.6940 3.7567 sec/batch\n",
      "Epoch 5/10  Iteration 800/1780 Training loss: 1.6935 3.7625 sec/batch\n",
      "Validation loss: 1.55103 Saving checkpoint!\n",
      "Epoch 5/10  Iteration 801/1780 Training loss: 1.6935 3.7861 sec/batch\n",
      "Epoch 5/10  Iteration 802/1780 Training loss: 1.6933 3.7731 sec/batch\n",
      "Epoch 5/10  Iteration 803/1780 Training loss: 1.6927 3.7812 sec/batch\n",
      "Epoch 5/10  Iteration 804/1780 Training loss: 1.6924 3.7581 sec/batch\n",
      "Epoch 5/10  Iteration 805/1780 Training loss: 1.6918 3.7617 sec/batch\n",
      "Epoch 5/10  Iteration 806/1780 Training loss: 1.6913 3.7585 sec/batch\n",
      "Epoch 5/10  Iteration 807/1780 Training loss: 1.6907 3.7817 sec/batch\n",
      "Epoch 5/10  Iteration 808/1780 Training loss: 1.6904 3.7561 sec/batch\n",
      "Epoch 5/10  Iteration 809/1780 Training loss: 1.6900 3.7694 sec/batch\n",
      "Epoch 5/10  Iteration 810/1780 Training loss: 1.6893 3.7803 sec/batch\n",
      "Epoch 5/10  Iteration 811/1780 Training loss: 1.6888 3.8933 sec/batch\n",
      "Epoch 5/10  Iteration 812/1780 Training loss: 1.6880 3.7773 sec/batch\n",
      "Epoch 5/10  Iteration 813/1780 Training loss: 1.6878 3.7656 sec/batch\n",
      "Epoch 5/10  Iteration 814/1780 Training loss: 1.6874 3.7613 sec/batch\n",
      "Epoch 5/10  Iteration 815/1780 Training loss: 1.6869 3.7583 sec/batch\n",
      "Epoch 5/10  Iteration 816/1780 Training loss: 1.6864 3.7642 sec/batch\n",
      "Epoch 5/10  Iteration 817/1780 Training loss: 1.6860 3.8212 sec/batch\n",
      "Epoch 5/10  Iteration 818/1780 Training loss: 1.6856 3.7665 sec/batch\n",
      "Epoch 5/10  Iteration 819/1780 Training loss: 1.6851 3.7538 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10  Iteration 820/1780 Training loss: 1.6848 3.8145 sec/batch\n",
      "Epoch 5/10  Iteration 821/1780 Training loss: 1.6845 3.7540 sec/batch\n",
      "Epoch 5/10  Iteration 822/1780 Training loss: 1.6843 3.7526 sec/batch\n",
      "Epoch 5/10  Iteration 823/1780 Training loss: 1.6839 3.7651 sec/batch\n",
      "Epoch 5/10  Iteration 824/1780 Training loss: 1.6835 3.7544 sec/batch\n",
      "Epoch 5/10  Iteration 825/1780 Training loss: 1.6831 3.7605 sec/batch\n",
      "Epoch 5/10  Iteration 826/1780 Training loss: 1.6827 3.7692 sec/batch\n",
      "Epoch 5/10  Iteration 827/1780 Training loss: 1.6820 3.7867 sec/batch\n",
      "Epoch 5/10  Iteration 828/1780 Training loss: 1.6814 3.7873 sec/batch\n",
      "Epoch 5/10  Iteration 829/1780 Training loss: 1.6811 3.9312 sec/batch\n",
      "Epoch 5/10  Iteration 830/1780 Training loss: 1.6807 3.7539 sec/batch\n",
      "Epoch 5/10  Iteration 831/1780 Training loss: 1.6803 3.7570 sec/batch\n",
      "Epoch 5/10  Iteration 832/1780 Training loss: 1.6799 3.7771 sec/batch\n",
      "Epoch 5/10  Iteration 833/1780 Training loss: 1.6797 3.7639 sec/batch\n",
      "Epoch 5/10  Iteration 834/1780 Training loss: 1.6792 3.7805 sec/batch\n",
      "Epoch 5/10  Iteration 835/1780 Training loss: 1.6786 3.7815 sec/batch\n",
      "Epoch 5/10  Iteration 836/1780 Training loss: 1.6785 3.7598 sec/batch\n",
      "Epoch 5/10  Iteration 837/1780 Training loss: 1.6782 3.7576 sec/batch\n",
      "Epoch 5/10  Iteration 838/1780 Training loss: 1.6775 3.7625 sec/batch\n",
      "Epoch 5/10  Iteration 839/1780 Training loss: 1.6774 3.7609 sec/batch\n",
      "Epoch 5/10  Iteration 840/1780 Training loss: 1.6773 3.7674 sec/batch\n",
      "Epoch 5/10  Iteration 841/1780 Training loss: 1.6769 3.7681 sec/batch\n",
      "Epoch 5/10  Iteration 842/1780 Training loss: 1.6765 3.8028 sec/batch\n",
      "Epoch 5/10  Iteration 843/1780 Training loss: 1.6758 3.8555 sec/batch\n",
      "Epoch 5/10  Iteration 844/1780 Training loss: 1.6752 3.7696 sec/batch\n",
      "Epoch 5/10  Iteration 845/1780 Training loss: 1.6751 3.7833 sec/batch\n",
      "Epoch 5/10  Iteration 846/1780 Training loss: 1.6749 3.7618 sec/batch\n",
      "Epoch 5/10  Iteration 847/1780 Training loss: 1.6746 3.7616 sec/batch\n",
      "Epoch 5/10  Iteration 848/1780 Training loss: 1.6745 3.7616 sec/batch\n",
      "Epoch 5/10  Iteration 849/1780 Training loss: 1.6744 3.7613 sec/batch\n",
      "Epoch 5/10  Iteration 850/1780 Training loss: 1.6741 3.7741 sec/batch\n",
      "Epoch 5/10  Iteration 851/1780 Training loss: 1.6740 3.7642 sec/batch\n",
      "Epoch 5/10  Iteration 852/1780 Training loss: 1.6736 3.7624 sec/batch\n",
      "Epoch 5/10  Iteration 853/1780 Training loss: 1.6737 3.7629 sec/batch\n",
      "Epoch 5/10  Iteration 854/1780 Training loss: 1.6734 3.7666 sec/batch\n",
      "Epoch 5/10  Iteration 855/1780 Training loss: 1.6731 3.7505 sec/batch\n",
      "Epoch 5/10  Iteration 856/1780 Training loss: 1.6730 3.7616 sec/batch\n",
      "Epoch 5/10  Iteration 857/1780 Training loss: 1.6727 3.9247 sec/batch\n",
      "Epoch 5/10  Iteration 858/1780 Training loss: 1.6725 3.7780 sec/batch\n",
      "Epoch 5/10  Iteration 859/1780 Training loss: 1.6723 3.8990 sec/batch\n",
      "Epoch 5/10  Iteration 860/1780 Training loss: 1.6723 3.7896 sec/batch\n",
      "Epoch 5/10  Iteration 861/1780 Training loss: 1.6721 3.7645 sec/batch\n",
      "Epoch 5/10  Iteration 862/1780 Training loss: 1.6718 3.8895 sec/batch\n",
      "Epoch 5/10  Iteration 863/1780 Training loss: 1.6714 3.7974 sec/batch\n",
      "Epoch 5/10  Iteration 864/1780 Training loss: 1.6712 3.7713 sec/batch\n",
      "Epoch 5/10  Iteration 865/1780 Training loss: 1.6711 3.7566 sec/batch\n",
      "Epoch 5/10  Iteration 866/1780 Training loss: 1.6709 3.7548 sec/batch\n",
      "Epoch 5/10  Iteration 867/1780 Training loss: 1.6707 4.4086 sec/batch\n",
      "Epoch 5/10  Iteration 868/1780 Training loss: 1.6704 4.8132 sec/batch\n",
      "Epoch 5/10  Iteration 869/1780 Training loss: 1.6702 4.2323 sec/batch\n",
      "Epoch 5/10  Iteration 870/1780 Training loss: 1.6700 4.2831 sec/batch\n",
      "Epoch 5/10  Iteration 871/1780 Training loss: 1.6695 4.1420 sec/batch\n",
      "Epoch 5/10  Iteration 872/1780 Training loss: 1.6694 4.0015 sec/batch\n",
      "Epoch 5/10  Iteration 873/1780 Training loss: 1.6694 4.1513 sec/batch\n",
      "Epoch 5/10  Iteration 874/1780 Training loss: 1.6691 4.1215 sec/batch\n",
      "Epoch 5/10  Iteration 875/1780 Training loss: 1.6689 4.2356 sec/batch\n",
      "Epoch 5/10  Iteration 876/1780 Training loss: 1.6687 4.1682 sec/batch\n",
      "Epoch 5/10  Iteration 877/1780 Training loss: 1.6685 4.1141 sec/batch\n",
      "Epoch 5/10  Iteration 878/1780 Training loss: 1.6682 3.9014 sec/batch\n",
      "Epoch 5/10  Iteration 879/1780 Training loss: 1.6681 4.0639 sec/batch\n",
      "Epoch 5/10  Iteration 880/1780 Training loss: 1.6683 4.2934 sec/batch\n",
      "Epoch 5/10  Iteration 881/1780 Training loss: 1.6680 4.3089 sec/batch\n",
      "Epoch 5/10  Iteration 882/1780 Training loss: 1.6677 4.8593 sec/batch\n",
      "Epoch 5/10  Iteration 883/1780 Training loss: 1.6673 5.4309 sec/batch\n",
      "Epoch 5/10  Iteration 884/1780 Training loss: 1.6670 5.1469 sec/batch\n",
      "Epoch 5/10  Iteration 885/1780 Training loss: 1.6669 4.1300 sec/batch\n",
      "Epoch 5/10  Iteration 886/1780 Training loss: 1.6667 3.9611 sec/batch\n",
      "Epoch 5/10  Iteration 887/1780 Training loss: 1.6665 4.3383 sec/batch\n",
      "Epoch 5/10  Iteration 888/1780 Training loss: 1.6662 4.5858 sec/batch\n",
      "Epoch 5/10  Iteration 889/1780 Training loss: 1.6658 4.0339 sec/batch\n",
      "Epoch 5/10  Iteration 890/1780 Training loss: 1.6656 4.2102 sec/batch\n",
      "Epoch 6/10  Iteration 891/1780 Training loss: 1.7167 3.9970 sec/batch\n",
      "Epoch 6/10  Iteration 892/1780 Training loss: 1.6668 4.0142 sec/batch\n",
      "Epoch 6/10  Iteration 893/1780 Training loss: 1.6500 3.8045 sec/batch\n",
      "Epoch 6/10  Iteration 894/1780 Training loss: 1.6434 3.8236 sec/batch\n",
      "Epoch 6/10  Iteration 895/1780 Training loss: 1.6362 4.7251 sec/batch\n",
      "Epoch 6/10  Iteration 896/1780 Training loss: 1.6260 4.6945 sec/batch\n",
      "Epoch 6/10  Iteration 897/1780 Training loss: 1.6262 4.6318 sec/batch\n",
      "Epoch 6/10  Iteration 898/1780 Training loss: 1.6228 4.2774 sec/batch\n",
      "Epoch 6/10  Iteration 899/1780 Training loss: 1.6233 3.8077 sec/batch\n",
      "Epoch 6/10  Iteration 900/1780 Training loss: 1.6236 3.7925 sec/batch\n",
      "Validation loss: 1.49582 Saving checkpoint!\n",
      "Epoch 6/10  Iteration 901/1780 Training loss: 1.6252 3.7147 sec/batch\n",
      "Epoch 6/10  Iteration 902/1780 Training loss: 1.6239 3.7785 sec/batch\n",
      "Epoch 6/10  Iteration 903/1780 Training loss: 1.6237 3.7778 sec/batch\n",
      "Epoch 6/10  Iteration 904/1780 Training loss: 1.6261 3.7670 sec/batch\n",
      "Epoch 6/10  Iteration 905/1780 Training loss: 1.6245 3.7777 sec/batch\n",
      "Epoch 6/10  Iteration 906/1780 Training loss: 1.6224 3.7681 sec/batch\n",
      "Epoch 6/10  Iteration 907/1780 Training loss: 1.6220 3.7756 sec/batch\n",
      "Epoch 6/10  Iteration 908/1780 Training loss: 1.6231 3.7787 sec/batch\n",
      "Epoch 6/10  Iteration 909/1780 Training loss: 1.6228 3.7674 sec/batch\n",
      "Epoch 6/10  Iteration 910/1780 Training loss: 1.6229 3.7809 sec/batch\n",
      "Epoch 6/10  Iteration 911/1780 Training loss: 1.6221 3.7934 sec/batch\n",
      "Epoch 6/10  Iteration 912/1780 Training loss: 1.6228 4.1136 sec/batch\n",
      "Epoch 6/10  Iteration 913/1780 Training loss: 1.6217 4.2366 sec/batch\n",
      "Epoch 6/10  Iteration 914/1780 Training loss: 1.6211 4.1883 sec/batch\n",
      "Epoch 6/10  Iteration 915/1780 Training loss: 1.6206 4.4698 sec/batch\n",
      "Epoch 6/10  Iteration 916/1780 Training loss: 1.6187 4.1267 sec/batch\n",
      "Epoch 6/10  Iteration 917/1780 Training loss: 1.6168 3.8866 sec/batch\n",
      "Epoch 6/10  Iteration 918/1780 Training loss: 1.6169 3.7870 sec/batch\n",
      "Epoch 6/10  Iteration 919/1780 Training loss: 1.6171 3.7877 sec/batch\n",
      "Epoch 6/10  Iteration 920/1780 Training loss: 1.6172 3.7627 sec/batch\n",
      "Epoch 6/10  Iteration 921/1780 Training loss: 1.6166 3.7814 sec/batch\n",
      "Epoch 6/10  Iteration 922/1780 Training loss: 1.6152 3.7606 sec/batch\n",
      "Epoch 6/10  Iteration 923/1780 Training loss: 1.6151 3.7815 sec/batch\n",
      "Epoch 6/10  Iteration 924/1780 Training loss: 1.6152 3.8120 sec/batch\n",
      "Epoch 6/10  Iteration 925/1780 Training loss: 1.6144 3.8048 sec/batch\n",
      "Epoch 6/10  Iteration 926/1780 Training loss: 1.6137 4.0169 sec/batch\n",
      "Epoch 6/10  Iteration 927/1780 Training loss: 1.6126 3.8346 sec/batch\n",
      "Epoch 6/10  Iteration 928/1780 Training loss: 1.6112 4.7093 sec/batch\n",
      "Epoch 6/10  Iteration 929/1780 Training loss: 1.6097 4.4937 sec/batch\n",
      "Epoch 6/10  Iteration 930/1780 Training loss: 1.6087 4.0753 sec/batch\n",
      "Epoch 6/10  Iteration 931/1780 Training loss: 1.6079 4.5975 sec/batch\n",
      "Epoch 6/10  Iteration 932/1780 Training loss: 1.6081 3.9024 sec/batch\n",
      "Epoch 6/10  Iteration 933/1780 Training loss: 1.6073 3.9414 sec/batch\n",
      "Epoch 6/10  Iteration 934/1780 Training loss: 1.6063 3.8005 sec/batch\n",
      "Epoch 6/10  Iteration 935/1780 Training loss: 1.6064 3.7717 sec/batch\n",
      "Epoch 6/10  Iteration 936/1780 Training loss: 1.6051 3.7762 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10  Iteration 937/1780 Training loss: 1.6044 3.7967 sec/batch\n",
      "Epoch 6/10  Iteration 938/1780 Training loss: 1.6037 3.7657 sec/batch\n",
      "Epoch 6/10  Iteration 939/1780 Training loss: 1.6032 3.7579 sec/batch\n",
      "Epoch 6/10  Iteration 940/1780 Training loss: 1.6035 3.7662 sec/batch\n",
      "Epoch 6/10  Iteration 941/1780 Training loss: 1.6029 3.7990 sec/batch\n",
      "Epoch 6/10  Iteration 942/1780 Training loss: 1.6036 3.7639 sec/batch\n",
      "Epoch 6/10  Iteration 943/1780 Training loss: 1.6032 3.7571 sec/batch\n",
      "Epoch 6/10  Iteration 944/1780 Training loss: 1.6030 3.7741 sec/batch\n",
      "Epoch 6/10  Iteration 945/1780 Training loss: 1.6025 3.8302 sec/batch\n",
      "Epoch 6/10  Iteration 946/1780 Training loss: 1.6026 3.9172 sec/batch\n",
      "Epoch 6/10  Iteration 947/1780 Training loss: 1.6027 3.9261 sec/batch\n",
      "Epoch 6/10  Iteration 948/1780 Training loss: 1.6021 4.5245 sec/batch\n",
      "Epoch 6/10  Iteration 949/1780 Training loss: 1.6013 4.2210 sec/batch\n",
      "Epoch 6/10  Iteration 950/1780 Training loss: 1.6016 3.7936 sec/batch\n",
      "Epoch 6/10  Iteration 951/1780 Training loss: 1.6013 3.9198 sec/batch\n",
      "Epoch 6/10  Iteration 952/1780 Training loss: 1.6019 3.8364 sec/batch\n",
      "Epoch 6/10  Iteration 953/1780 Training loss: 1.6022 4.0393 sec/batch\n",
      "Epoch 6/10  Iteration 954/1780 Training loss: 1.6023 4.0362 sec/batch\n",
      "Epoch 6/10  Iteration 955/1780 Training loss: 1.6020 4.0489 sec/batch\n",
      "Epoch 6/10  Iteration 956/1780 Training loss: 1.6020 4.7431 sec/batch\n",
      "Epoch 6/10  Iteration 957/1780 Training loss: 1.6018 3.9766 sec/batch\n",
      "Epoch 6/10  Iteration 958/1780 Training loss: 1.6011 4.1117 sec/batch\n",
      "Epoch 6/10  Iteration 959/1780 Training loss: 1.6010 4.0483 sec/batch\n",
      "Epoch 6/10  Iteration 960/1780 Training loss: 1.6008 3.8842 sec/batch\n",
      "Epoch 6/10  Iteration 961/1780 Training loss: 1.6011 3.8937 sec/batch\n",
      "Epoch 6/10  Iteration 962/1780 Training loss: 1.6012 3.8948 sec/batch\n",
      "Epoch 6/10  Iteration 963/1780 Training loss: 1.6014 3.8541 sec/batch\n",
      "Epoch 6/10  Iteration 964/1780 Training loss: 1.6010 3.9375 sec/batch\n",
      "Epoch 6/10  Iteration 965/1780 Training loss: 1.6006 3.8432 sec/batch\n",
      "Epoch 6/10  Iteration 966/1780 Training loss: 1.6007 3.8918 sec/batch\n",
      "Epoch 6/10  Iteration 967/1780 Training loss: 1.6004 3.9592 sec/batch\n",
      "Epoch 6/10  Iteration 968/1780 Training loss: 1.6002 4.2060 sec/batch\n",
      "Epoch 6/10  Iteration 969/1780 Training loss: 1.5994 3.8496 sec/batch\n",
      "Epoch 6/10  Iteration 970/1780 Training loss: 1.5991 4.2447 sec/batch\n",
      "Epoch 6/10  Iteration 971/1780 Training loss: 1.5984 3.8245 sec/batch\n",
      "Epoch 6/10  Iteration 972/1780 Training loss: 1.5982 3.8190 sec/batch\n",
      "Epoch 6/10  Iteration 973/1780 Training loss: 1.5975 3.7673 sec/batch\n",
      "Epoch 6/10  Iteration 974/1780 Training loss: 1.5973 3.7682 sec/batch\n",
      "Epoch 6/10  Iteration 975/1780 Training loss: 1.5969 3.7830 sec/batch\n",
      "Epoch 6/10  Iteration 976/1780 Training loss: 1.5965 3.7629 sec/batch\n",
      "Epoch 6/10  Iteration 977/1780 Training loss: 1.5961 3.7559 sec/batch\n",
      "Epoch 6/10  Iteration 978/1780 Training loss: 1.5957 3.9025 sec/batch\n",
      "Epoch 6/10  Iteration 979/1780 Training loss: 1.5950 3.8011 sec/batch\n",
      "Epoch 6/10  Iteration 980/1780 Training loss: 1.5950 3.7840 sec/batch\n",
      "Epoch 6/10  Iteration 981/1780 Training loss: 1.5945 3.7872 sec/batch\n",
      "Epoch 6/10  Iteration 982/1780 Training loss: 1.5941 3.7656 sec/batch\n",
      "Epoch 6/10  Iteration 983/1780 Training loss: 1.5935 3.7726 sec/batch\n",
      "Epoch 6/10  Iteration 984/1780 Training loss: 1.5930 3.7936 sec/batch\n",
      "Epoch 6/10  Iteration 985/1780 Training loss: 1.5925 3.7628 sec/batch\n",
      "Epoch 6/10  Iteration 986/1780 Training loss: 1.5923 3.8183 sec/batch\n",
      "Epoch 6/10  Iteration 987/1780 Training loss: 1.5920 3.8631 sec/batch\n",
      "Epoch 6/10  Iteration 988/1780 Training loss: 1.5914 3.7781 sec/batch\n",
      "Epoch 6/10  Iteration 989/1780 Training loss: 1.5910 3.7643 sec/batch\n",
      "Epoch 6/10  Iteration 990/1780 Training loss: 1.5903 3.7992 sec/batch\n",
      "Epoch 6/10  Iteration 991/1780 Training loss: 1.5901 3.7624 sec/batch\n",
      "Epoch 6/10  Iteration 992/1780 Training loss: 1.5899 3.8658 sec/batch\n",
      "Epoch 6/10  Iteration 993/1780 Training loss: 1.5895 3.7665 sec/batch\n",
      "Epoch 6/10  Iteration 994/1780 Training loss: 1.5892 3.7725 sec/batch\n",
      "Epoch 6/10  Iteration 995/1780 Training loss: 1.5888 3.7807 sec/batch\n",
      "Epoch 6/10  Iteration 996/1780 Training loss: 1.5884 3.7643 sec/batch\n",
      "Epoch 6/10  Iteration 997/1780 Training loss: 1.5881 3.7612 sec/batch\n",
      "Epoch 6/10  Iteration 998/1780 Training loss: 1.5878 3.7623 sec/batch\n",
      "Epoch 6/10  Iteration 999/1780 Training loss: 1.5876 3.7569 sec/batch\n",
      "Epoch 6/10  Iteration 1000/1780 Training loss: 1.5874 3.7512 sec/batch\n",
      "Validation loss: 1.44975 Saving checkpoint!\n",
      "Epoch 6/10  Iteration 1001/1780 Training loss: 1.5877 5.5907 sec/batch\n",
      "Epoch 6/10  Iteration 1002/1780 Training loss: 1.5875 5.2421 sec/batch\n",
      "Epoch 6/10  Iteration 1003/1780 Training loss: 1.5871 4.0931 sec/batch\n",
      "Epoch 6/10  Iteration 1004/1780 Training loss: 1.5868 3.8485 sec/batch\n",
      "Epoch 6/10  Iteration 1005/1780 Training loss: 1.5863 3.8394 sec/batch\n",
      "Epoch 6/10  Iteration 1006/1780 Training loss: 1.5857 3.9956 sec/batch\n",
      "Epoch 6/10  Iteration 1007/1780 Training loss: 1.5856 4.3203 sec/batch\n",
      "Epoch 6/10  Iteration 1008/1780 Training loss: 1.5854 5.3240 sec/batch\n",
      "Epoch 6/10  Iteration 1009/1780 Training loss: 1.5851 4.2283 sec/batch\n",
      "Epoch 6/10  Iteration 1010/1780 Training loss: 1.5848 5.0791 sec/batch\n",
      "Epoch 6/10  Iteration 1011/1780 Training loss: 1.5844 5.1581 sec/batch\n",
      "Epoch 6/10  Iteration 1012/1780 Training loss: 1.5839 4.2954 sec/batch\n",
      "Epoch 6/10  Iteration 1013/1780 Training loss: 1.5833 3.8634 sec/batch\n",
      "Epoch 6/10  Iteration 1014/1780 Training loss: 1.5832 3.8368 sec/batch\n",
      "Epoch 6/10  Iteration 1015/1780 Training loss: 1.5830 3.8494 sec/batch\n",
      "Epoch 6/10  Iteration 1016/1780 Training loss: 1.5824 3.9559 sec/batch\n",
      "Epoch 6/10  Iteration 1017/1780 Training loss: 1.5823 4.0230 sec/batch\n",
      "Epoch 6/10  Iteration 1018/1780 Training loss: 1.5822 3.8567 sec/batch\n",
      "Epoch 6/10  Iteration 1019/1780 Training loss: 1.5818 4.4801 sec/batch\n",
      "Epoch 6/10  Iteration 1020/1780 Training loss: 1.5814 3.9539 sec/batch\n",
      "Epoch 6/10  Iteration 1021/1780 Training loss: 1.5808 4.1571 sec/batch\n",
      "Epoch 6/10  Iteration 1022/1780 Training loss: 1.5803 3.8832 sec/batch\n",
      "Epoch 6/10  Iteration 1023/1780 Training loss: 1.5802 4.0349 sec/batch\n",
      "Epoch 6/10  Iteration 1024/1780 Training loss: 1.5801 4.8708 sec/batch\n",
      "Epoch 6/10  Iteration 1025/1780 Training loss: 1.5799 4.0891 sec/batch\n",
      "Epoch 6/10  Iteration 1026/1780 Training loss: 1.5797 4.5714 sec/batch\n",
      "Epoch 6/10  Iteration 1027/1780 Training loss: 1.5797 3.9302 sec/batch\n",
      "Epoch 6/10  Iteration 1028/1780 Training loss: 1.5797 4.0843 sec/batch\n",
      "Epoch 6/10  Iteration 1029/1780 Training loss: 1.5795 4.1518 sec/batch\n",
      "Epoch 6/10  Iteration 1030/1780 Training loss: 1.5793 3.8719 sec/batch\n",
      "Epoch 6/10  Iteration 1031/1780 Training loss: 1.5796 3.8494 sec/batch\n",
      "Epoch 6/10  Iteration 1032/1780 Training loss: 1.5794 3.8385 sec/batch\n",
      "Epoch 6/10  Iteration 1033/1780 Training loss: 1.5792 3.8311 sec/batch\n",
      "Epoch 6/10  Iteration 1034/1780 Training loss: 1.5792 3.8364 sec/batch\n",
      "Epoch 6/10  Iteration 1035/1780 Training loss: 1.5789 3.8244 sec/batch\n",
      "Epoch 6/10  Iteration 1036/1780 Training loss: 1.5789 3.8308 sec/batch\n",
      "Epoch 6/10  Iteration 1037/1780 Training loss: 1.5788 3.9434 sec/batch\n",
      "Epoch 6/10  Iteration 1038/1780 Training loss: 1.5789 3.9831 sec/batch\n",
      "Epoch 6/10  Iteration 1039/1780 Training loss: 1.5788 3.8913 sec/batch\n",
      "Epoch 6/10  Iteration 1040/1780 Training loss: 1.5785 3.8517 sec/batch\n",
      "Epoch 6/10  Iteration 1041/1780 Training loss: 1.5780 3.8593 sec/batch\n",
      "Epoch 6/10  Iteration 1042/1780 Training loss: 1.5778 3.8608 sec/batch\n",
      "Epoch 6/10  Iteration 1043/1780 Training loss: 1.5777 3.8459 sec/batch\n",
      "Epoch 6/10  Iteration 1044/1780 Training loss: 1.5775 3.8685 sec/batch\n",
      "Epoch 6/10  Iteration 1045/1780 Training loss: 1.5774 3.8692 sec/batch\n",
      "Epoch 6/10  Iteration 1046/1780 Training loss: 1.5772 3.8428 sec/batch\n",
      "Epoch 6/10  Iteration 1047/1780 Training loss: 1.5771 3.8495 sec/batch\n",
      "Epoch 6/10  Iteration 1048/1780 Training loss: 1.5769 3.8626 sec/batch\n",
      "Epoch 6/10  Iteration 1049/1780 Training loss: 1.5765 3.8521 sec/batch\n",
      "Epoch 6/10  Iteration 1050/1780 Training loss: 1.5765 3.8550 sec/batch\n",
      "Epoch 6/10  Iteration 1051/1780 Training loss: 1.5765 3.8568 sec/batch\n",
      "Epoch 6/10  Iteration 1052/1780 Training loss: 1.5764 3.8676 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10  Iteration 1053/1780 Training loss: 1.5763 3.8705 sec/batch\n",
      "Epoch 6/10  Iteration 1054/1780 Training loss: 1.5761 3.9028 sec/batch\n",
      "Epoch 6/10  Iteration 1055/1780 Training loss: 1.5759 3.8490 sec/batch\n",
      "Epoch 6/10  Iteration 1056/1780 Training loss: 1.5757 3.8659 sec/batch\n",
      "Epoch 6/10  Iteration 1057/1780 Training loss: 1.5756 3.8648 sec/batch\n",
      "Epoch 6/10  Iteration 1058/1780 Training loss: 1.5758 3.8501 sec/batch\n",
      "Epoch 6/10  Iteration 1059/1780 Training loss: 1.5756 3.8406 sec/batch\n",
      "Epoch 6/10  Iteration 1060/1780 Training loss: 1.5755 3.8435 sec/batch\n",
      "Epoch 6/10  Iteration 1061/1780 Training loss: 1.5751 3.8515 sec/batch\n",
      "Epoch 6/10  Iteration 1062/1780 Training loss: 1.5748 3.8451 sec/batch\n",
      "Epoch 6/10  Iteration 1063/1780 Training loss: 1.5747 3.8540 sec/batch\n",
      "Epoch 6/10  Iteration 1064/1780 Training loss: 1.5746 3.8976 sec/batch\n",
      "Epoch 6/10  Iteration 1065/1780 Training loss: 1.5745 3.8549 sec/batch\n",
      "Epoch 6/10  Iteration 1066/1780 Training loss: 1.5742 3.8659 sec/batch\n",
      "Epoch 6/10  Iteration 1067/1780 Training loss: 1.5739 3.8644 sec/batch\n",
      "Epoch 6/10  Iteration 1068/1780 Training loss: 1.5738 3.9121 sec/batch\n",
      "Epoch 7/10  Iteration 1069/1780 Training loss: 1.6480 4.2158 sec/batch\n",
      "Epoch 7/10  Iteration 1070/1780 Training loss: 1.5913 3.9185 sec/batch\n",
      "Epoch 7/10  Iteration 1071/1780 Training loss: 1.5707 3.8424 sec/batch\n",
      "Epoch 7/10  Iteration 1072/1780 Training loss: 1.5653 3.8778 sec/batch\n",
      "Epoch 7/10  Iteration 1073/1780 Training loss: 1.5580 3.8633 sec/batch\n",
      "Epoch 7/10  Iteration 1074/1780 Training loss: 1.5477 3.8631 sec/batch\n",
      "Epoch 7/10  Iteration 1075/1780 Training loss: 1.5474 3.8600 sec/batch\n",
      "Epoch 7/10  Iteration 1076/1780 Training loss: 1.5438 3.8776 sec/batch\n",
      "Epoch 7/10  Iteration 1077/1780 Training loss: 1.5447 3.8591 sec/batch\n",
      "Epoch 7/10  Iteration 1078/1780 Training loss: 1.5434 3.8615 sec/batch\n",
      "Epoch 7/10  Iteration 1079/1780 Training loss: 1.5405 3.8466 sec/batch\n",
      "Epoch 7/10  Iteration 1080/1780 Training loss: 1.5401 3.8602 sec/batch\n",
      "Epoch 7/10  Iteration 1081/1780 Training loss: 1.5392 3.8622 sec/batch\n",
      "Epoch 7/10  Iteration 1082/1780 Training loss: 1.5411 3.8454 sec/batch\n",
      "Epoch 7/10  Iteration 1083/1780 Training loss: 1.5397 3.8400 sec/batch\n",
      "Epoch 7/10  Iteration 1084/1780 Training loss: 1.5376 3.8915 sec/batch\n",
      "Epoch 7/10  Iteration 1085/1780 Training loss: 1.5375 3.8560 sec/batch\n",
      "Epoch 7/10  Iteration 1086/1780 Training loss: 1.5389 3.8511 sec/batch\n",
      "Epoch 7/10  Iteration 1087/1780 Training loss: 1.5384 3.8473 sec/batch\n",
      "Epoch 7/10  Iteration 1088/1780 Training loss: 1.5397 3.8520 sec/batch\n",
      "Epoch 7/10  Iteration 1089/1780 Training loss: 1.5392 3.8319 sec/batch\n",
      "Epoch 7/10  Iteration 1090/1780 Training loss: 1.5400 3.8531 sec/batch\n",
      "Epoch 7/10  Iteration 1091/1780 Training loss: 1.5389 3.8503 sec/batch\n",
      "Epoch 7/10  Iteration 1092/1780 Training loss: 1.5386 3.8224 sec/batch\n",
      "Epoch 7/10  Iteration 1093/1780 Training loss: 1.5382 3.8591 sec/batch\n",
      "Epoch 7/10  Iteration 1094/1780 Training loss: 1.5366 3.8586 sec/batch\n",
      "Epoch 7/10  Iteration 1095/1780 Training loss: 1.5352 3.8505 sec/batch\n",
      "Epoch 7/10  Iteration 1096/1780 Training loss: 1.5353 3.8564 sec/batch\n",
      "Epoch 7/10  Iteration 1097/1780 Training loss: 1.5358 3.8492 sec/batch\n",
      "Epoch 7/10  Iteration 1098/1780 Training loss: 1.5361 3.8513 sec/batch\n",
      "Epoch 7/10  Iteration 1099/1780 Training loss: 1.5359 3.8658 sec/batch\n",
      "Epoch 7/10  Iteration 1100/1780 Training loss: 1.5349 3.9535 sec/batch\n",
      "Validation loss: 1.41396 Saving checkpoint!\n",
      "Epoch 7/10  Iteration 1101/1780 Training loss: 1.5374 3.9836 sec/batch\n",
      "Epoch 7/10  Iteration 1102/1780 Training loss: 1.5375 5.1057 sec/batch\n",
      "Epoch 7/10  Iteration 1103/1780 Training loss: 1.5368 4.9613 sec/batch\n",
      "Epoch 7/10  Iteration 1104/1780 Training loss: 1.5365 3.9051 sec/batch\n",
      "Epoch 7/10  Iteration 1105/1780 Training loss: 1.5355 4.2791 sec/batch\n",
      "Epoch 7/10  Iteration 1106/1780 Training loss: 1.5341 3.8150 sec/batch\n",
      "Epoch 7/10  Iteration 1107/1780 Training loss: 1.5326 3.8148 sec/batch\n",
      "Epoch 7/10  Iteration 1108/1780 Training loss: 1.5319 3.8447 sec/batch\n",
      "Epoch 7/10  Iteration 1109/1780 Training loss: 1.5311 3.8147 sec/batch\n",
      "Epoch 7/10  Iteration 1110/1780 Training loss: 1.5315 3.8183 sec/batch\n",
      "Epoch 7/10  Iteration 1111/1780 Training loss: 1.5308 3.8083 sec/batch\n",
      "Epoch 7/10  Iteration 1112/1780 Training loss: 1.5297 3.8239 sec/batch\n",
      "Epoch 7/10  Iteration 1113/1780 Training loss: 1.5298 3.8068 sec/batch\n",
      "Epoch 7/10  Iteration 1114/1780 Training loss: 1.5287 3.8138 sec/batch\n",
      "Epoch 7/10  Iteration 1115/1780 Training loss: 1.5284 3.7935 sec/batch\n",
      "Epoch 7/10  Iteration 1116/1780 Training loss: 1.5279 3.8011 sec/batch\n",
      "Epoch 7/10  Iteration 1117/1780 Training loss: 1.5277 3.8087 sec/batch\n",
      "Epoch 7/10  Iteration 1118/1780 Training loss: 1.5281 3.7902 sec/batch\n",
      "Epoch 7/10  Iteration 1119/1780 Training loss: 1.5274 3.8084 sec/batch\n",
      "Epoch 7/10  Iteration 1120/1780 Training loss: 1.5282 3.8066 sec/batch\n",
      "Epoch 7/10  Iteration 1121/1780 Training loss: 1.5279 3.8018 sec/batch\n",
      "Epoch 7/10  Iteration 1122/1780 Training loss: 1.5281 3.8009 sec/batch\n",
      "Epoch 7/10  Iteration 1123/1780 Training loss: 1.5277 3.8712 sec/batch\n",
      "Epoch 7/10  Iteration 1124/1780 Training loss: 1.5276 3.8327 sec/batch\n",
      "Epoch 7/10  Iteration 1125/1780 Training loss: 1.5281 3.9775 sec/batch\n",
      "Epoch 7/10  Iteration 1126/1780 Training loss: 1.5277 3.7950 sec/batch\n",
      "Epoch 7/10  Iteration 1127/1780 Training loss: 1.5271 3.8051 sec/batch\n",
      "Epoch 7/10  Iteration 1128/1780 Training loss: 1.5279 3.7997 sec/batch\n",
      "Epoch 7/10  Iteration 1129/1780 Training loss: 1.5279 3.8102 sec/batch\n",
      "Epoch 7/10  Iteration 1130/1780 Training loss: 1.5286 3.7987 sec/batch\n",
      "Epoch 7/10  Iteration 1131/1780 Training loss: 1.5290 3.8035 sec/batch\n",
      "Epoch 7/10  Iteration 1132/1780 Training loss: 1.5291 3.8075 sec/batch\n",
      "Epoch 7/10  Iteration 1133/1780 Training loss: 1.5288 3.7990 sec/batch\n",
      "Epoch 7/10  Iteration 1134/1780 Training loss: 1.5288 3.7890 sec/batch\n",
      "Epoch 7/10  Iteration 1135/1780 Training loss: 1.5288 3.8023 sec/batch\n",
      "Epoch 7/10  Iteration 1136/1780 Training loss: 1.5283 3.7964 sec/batch\n",
      "Epoch 7/10  Iteration 1137/1780 Training loss: 1.5282 3.8073 sec/batch\n",
      "Epoch 7/10  Iteration 1138/1780 Training loss: 1.5280 3.7973 sec/batch\n",
      "Epoch 7/10  Iteration 1139/1780 Training loss: 1.5285 3.9085 sec/batch\n",
      "Epoch 7/10  Iteration 1140/1780 Training loss: 1.5285 3.8002 sec/batch\n",
      "Epoch 7/10  Iteration 1141/1780 Training loss: 1.5289 3.8001 sec/batch\n",
      "Epoch 7/10  Iteration 1142/1780 Training loss: 1.5283 3.7902 sec/batch\n",
      "Epoch 7/10  Iteration 1143/1780 Training loss: 1.5281 3.8081 sec/batch\n",
      "Epoch 7/10  Iteration 1144/1780 Training loss: 1.5280 3.8037 sec/batch\n",
      "Epoch 7/10  Iteration 1145/1780 Training loss: 1.5277 3.7785 sec/batch\n",
      "Epoch 7/10  Iteration 1146/1780 Training loss: 1.5276 3.8032 sec/batch\n",
      "Epoch 7/10  Iteration 1147/1780 Training loss: 1.5268 3.7962 sec/batch\n",
      "Epoch 7/10  Iteration 1148/1780 Training loss: 1.5266 3.8039 sec/batch\n",
      "Epoch 7/10  Iteration 1149/1780 Training loss: 1.5259 3.8306 sec/batch\n",
      "Epoch 7/10  Iteration 1150/1780 Training loss: 1.5258 3.7947 sec/batch\n",
      "Epoch 7/10  Iteration 1151/1780 Training loss: 1.5252 3.8013 sec/batch\n",
      "Epoch 7/10  Iteration 1152/1780 Training loss: 1.5249 3.8012 sec/batch\n",
      "Epoch 7/10  Iteration 1153/1780 Training loss: 1.5245 3.9444 sec/batch\n",
      "Epoch 7/10  Iteration 1154/1780 Training loss: 1.5241 3.8242 sec/batch\n",
      "Epoch 7/10  Iteration 1155/1780 Training loss: 1.5236 3.8683 sec/batch\n",
      "Epoch 7/10  Iteration 1156/1780 Training loss: 1.5232 3.8052 sec/batch\n",
      "Epoch 7/10  Iteration 1157/1780 Training loss: 1.5227 3.8118 sec/batch\n",
      "Epoch 7/10  Iteration 1158/1780 Training loss: 1.5227 3.8136 sec/batch\n",
      "Epoch 7/10  Iteration 1159/1780 Training loss: 1.5223 3.8084 sec/batch\n",
      "Epoch 7/10  Iteration 1160/1780 Training loss: 1.5221 3.8097 sec/batch\n",
      "Epoch 7/10  Iteration 1161/1780 Training loss: 1.5215 3.8096 sec/batch\n",
      "Epoch 7/10  Iteration 1162/1780 Training loss: 1.5211 3.8004 sec/batch\n",
      "Epoch 7/10  Iteration 1163/1780 Training loss: 1.5206 3.8140 sec/batch\n",
      "Epoch 7/10  Iteration 1164/1780 Training loss: 1.5206 3.7854 sec/batch\n",
      "Epoch 7/10  Iteration 1165/1780 Training loss: 1.5204 3.8121 sec/batch\n",
      "Epoch 7/10  Iteration 1166/1780 Training loss: 1.5198 3.8018 sec/batch\n",
      "Epoch 7/10  Iteration 1167/1780 Training loss: 1.5193 3.7934 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10  Iteration 1168/1780 Training loss: 1.5188 3.7992 sec/batch\n",
      "Epoch 7/10  Iteration 1169/1780 Training loss: 1.5187 3.8162 sec/batch\n",
      "Epoch 7/10  Iteration 1170/1780 Training loss: 1.5185 3.8029 sec/batch\n",
      "Epoch 7/10  Iteration 1171/1780 Training loss: 1.5182 3.8183 sec/batch\n",
      "Epoch 7/10  Iteration 1172/1780 Training loss: 1.5179 3.8087 sec/batch\n",
      "Epoch 7/10  Iteration 1173/1780 Training loss: 1.5177 3.8036 sec/batch\n",
      "Epoch 7/10  Iteration 1174/1780 Training loss: 1.5175 3.8020 sec/batch\n",
      "Epoch 7/10  Iteration 1175/1780 Training loss: 1.5173 3.8139 sec/batch\n",
      "Epoch 7/10  Iteration 1176/1780 Training loss: 1.5172 3.8137 sec/batch\n",
      "Epoch 7/10  Iteration 1177/1780 Training loss: 1.5169 3.7997 sec/batch\n",
      "Epoch 7/10  Iteration 1178/1780 Training loss: 1.5169 3.8053 sec/batch\n",
      "Epoch 7/10  Iteration 1179/1780 Training loss: 1.5165 3.7983 sec/batch\n",
      "Epoch 7/10  Iteration 1180/1780 Training loss: 1.5162 3.8003 sec/batch\n",
      "Epoch 7/10  Iteration 1181/1780 Training loss: 1.5159 3.8649 sec/batch\n",
      "Epoch 7/10  Iteration 1182/1780 Training loss: 1.5155 3.9014 sec/batch\n",
      "Epoch 7/10  Iteration 1183/1780 Training loss: 1.5150 3.7930 sec/batch\n",
      "Epoch 7/10  Iteration 1184/1780 Training loss: 1.5146 3.8050 sec/batch\n",
      "Epoch 7/10  Iteration 1185/1780 Training loss: 1.5144 3.7957 sec/batch\n",
      "Epoch 7/10  Iteration 1186/1780 Training loss: 1.5142 3.8923 sec/batch\n",
      "Epoch 7/10  Iteration 1187/1780 Training loss: 1.5140 3.8051 sec/batch\n",
      "Epoch 7/10  Iteration 1188/1780 Training loss: 1.5138 3.8118 sec/batch\n",
      "Epoch 7/10  Iteration 1189/1780 Training loss: 1.5135 3.8122 sec/batch\n",
      "Epoch 7/10  Iteration 1190/1780 Training loss: 1.5130 3.8067 sec/batch\n",
      "Epoch 7/10  Iteration 1191/1780 Training loss: 1.5125 3.7999 sec/batch\n",
      "Epoch 7/10  Iteration 1192/1780 Training loss: 1.5124 3.8199 sec/batch\n",
      "Epoch 7/10  Iteration 1193/1780 Training loss: 1.5122 3.8140 sec/batch\n",
      "Epoch 7/10  Iteration 1194/1780 Training loss: 1.5118 3.7922 sec/batch\n",
      "Epoch 7/10  Iteration 1195/1780 Training loss: 1.5117 3.8000 sec/batch\n",
      "Epoch 7/10  Iteration 1196/1780 Training loss: 1.5116 3.8043 sec/batch\n",
      "Epoch 7/10  Iteration 1197/1780 Training loss: 1.5114 3.7974 sec/batch\n",
      "Epoch 7/10  Iteration 1198/1780 Training loss: 1.5110 3.8017 sec/batch\n",
      "Epoch 7/10  Iteration 1199/1780 Training loss: 1.5105 3.8094 sec/batch\n",
      "Epoch 7/10  Iteration 1200/1780 Training loss: 1.5102 3.7981 sec/batch\n",
      "Validation loss: 1.37659 Saving checkpoint!\n",
      "Epoch 7/10  Iteration 1201/1780 Training loss: 1.5106 3.7413 sec/batch\n",
      "Epoch 7/10  Iteration 1202/1780 Training loss: 1.5106 3.8055 sec/batch\n",
      "Epoch 7/10  Iteration 1203/1780 Training loss: 1.5104 3.8338 sec/batch\n",
      "Epoch 7/10  Iteration 1204/1780 Training loss: 1.5103 3.7897 sec/batch\n",
      "Epoch 7/10  Iteration 1205/1780 Training loss: 1.5102 3.7982 sec/batch\n",
      "Epoch 7/10  Iteration 1206/1780 Training loss: 1.5102 3.8020 sec/batch\n",
      "Epoch 7/10  Iteration 1207/1780 Training loss: 1.5100 3.7955 sec/batch\n",
      "Epoch 7/10  Iteration 1208/1780 Training loss: 1.5098 3.8040 sec/batch\n",
      "Epoch 7/10  Iteration 1209/1780 Training loss: 1.5101 3.8056 sec/batch\n",
      "Epoch 7/10  Iteration 1210/1780 Training loss: 1.5100 3.8279 sec/batch\n",
      "Epoch 7/10  Iteration 1211/1780 Training loss: 1.5098 4.0302 sec/batch\n",
      "Epoch 7/10  Iteration 1212/1780 Training loss: 1.5099 3.8166 sec/batch\n",
      "Epoch 7/10  Iteration 1213/1780 Training loss: 1.5096 3.8068 sec/batch\n",
      "Epoch 7/10  Iteration 1214/1780 Training loss: 1.5096 3.8165 sec/batch\n",
      "Epoch 7/10  Iteration 1215/1780 Training loss: 1.5094 3.8020 sec/batch\n",
      "Epoch 7/10  Iteration 1216/1780 Training loss: 1.5095 3.7999 sec/batch\n",
      "Epoch 7/10  Iteration 1217/1780 Training loss: 1.5094 3.7990 sec/batch\n",
      "Epoch 7/10  Iteration 1218/1780 Training loss: 1.5092 3.7955 sec/batch\n",
      "Epoch 7/10  Iteration 1219/1780 Training loss: 1.5088 3.7962 sec/batch\n",
      "Epoch 7/10  Iteration 1220/1780 Training loss: 1.5085 3.8059 sec/batch\n",
      "Epoch 7/10  Iteration 1221/1780 Training loss: 1.5084 3.7954 sec/batch\n",
      "Epoch 7/10  Iteration 1222/1780 Training loss: 1.5083 3.8072 sec/batch\n",
      "Epoch 7/10  Iteration 1223/1780 Training loss: 1.5082 3.7894 sec/batch\n",
      "Epoch 7/10  Iteration 1224/1780 Training loss: 1.5080 3.7954 sec/batch\n",
      "Epoch 7/10  Iteration 1225/1780 Training loss: 1.5080 3.7777 sec/batch\n",
      "Epoch 7/10  Iteration 1226/1780 Training loss: 1.5078 3.7975 sec/batch\n",
      "Epoch 7/10  Iteration 1227/1780 Training loss: 1.5075 3.8122 sec/batch\n",
      "Epoch 7/10  Iteration 1228/1780 Training loss: 1.5075 3.7896 sec/batch\n",
      "Epoch 7/10  Iteration 1229/1780 Training loss: 1.5077 3.7904 sec/batch\n",
      "Epoch 7/10  Iteration 1230/1780 Training loss: 1.5075 3.7965 sec/batch\n",
      "Epoch 7/10  Iteration 1231/1780 Training loss: 1.5074 3.7977 sec/batch\n",
      "Epoch 7/10  Iteration 1232/1780 Training loss: 1.5073 3.7997 sec/batch\n",
      "Epoch 7/10  Iteration 1233/1780 Training loss: 1.5071 3.7889 sec/batch\n",
      "Epoch 7/10  Iteration 1234/1780 Training loss: 1.5070 3.7941 sec/batch\n",
      "Epoch 7/10  Iteration 1235/1780 Training loss: 1.5071 3.7927 sec/batch\n",
      "Epoch 7/10  Iteration 1236/1780 Training loss: 1.5073 3.7803 sec/batch\n",
      "Epoch 7/10  Iteration 1237/1780 Training loss: 1.5072 3.7930 sec/batch\n",
      "Epoch 7/10  Iteration 1238/1780 Training loss: 1.5070 3.7821 sec/batch\n",
      "Epoch 7/10  Iteration 1239/1780 Training loss: 1.5068 3.7913 sec/batch\n",
      "Epoch 7/10  Iteration 1240/1780 Training loss: 1.5066 3.7872 sec/batch\n",
      "Epoch 7/10  Iteration 1241/1780 Training loss: 1.5066 3.7851 sec/batch\n",
      "Epoch 7/10  Iteration 1242/1780 Training loss: 1.5065 3.8594 sec/batch\n",
      "Epoch 7/10  Iteration 1243/1780 Training loss: 1.5065 3.7992 sec/batch\n",
      "Epoch 7/10  Iteration 1244/1780 Training loss: 1.5062 3.8004 sec/batch\n",
      "Epoch 7/10  Iteration 1245/1780 Training loss: 1.5060 3.7821 sec/batch\n",
      "Epoch 7/10  Iteration 1246/1780 Training loss: 1.5060 3.7868 sec/batch\n",
      "Epoch 8/10  Iteration 1247/1780 Training loss: 1.5821 3.7996 sec/batch\n",
      "Epoch 8/10  Iteration 1248/1780 Training loss: 1.5349 3.7838 sec/batch\n",
      "Epoch 8/10  Iteration 1249/1780 Training loss: 1.5172 3.8002 sec/batch\n",
      "Epoch 8/10  Iteration 1250/1780 Training loss: 1.5112 3.7858 sec/batch\n",
      "Epoch 8/10  Iteration 1251/1780 Training loss: 1.5030 3.7923 sec/batch\n",
      "Epoch 8/10  Iteration 1252/1780 Training loss: 1.4920 3.7851 sec/batch\n",
      "Epoch 8/10  Iteration 1253/1780 Training loss: 1.4919 3.7816 sec/batch\n",
      "Epoch 8/10  Iteration 1254/1780 Training loss: 1.4884 3.8087 sec/batch\n",
      "Epoch 8/10  Iteration 1255/1780 Training loss: 1.4890 3.7953 sec/batch\n",
      "Epoch 8/10  Iteration 1256/1780 Training loss: 1.4875 3.7718 sec/batch\n",
      "Epoch 8/10  Iteration 1257/1780 Training loss: 1.4831 3.7947 sec/batch\n",
      "Epoch 8/10  Iteration 1258/1780 Training loss: 1.4823 3.8106 sec/batch\n",
      "Epoch 8/10  Iteration 1259/1780 Training loss: 1.4819 3.8045 sec/batch\n",
      "Epoch 8/10  Iteration 1260/1780 Training loss: 1.4839 3.7978 sec/batch\n",
      "Epoch 8/10  Iteration 1261/1780 Training loss: 1.4822 3.7885 sec/batch\n",
      "Epoch 8/10  Iteration 1262/1780 Training loss: 1.4796 3.8097 sec/batch\n",
      "Epoch 8/10  Iteration 1263/1780 Training loss: 1.4799 3.7942 sec/batch\n",
      "Epoch 8/10  Iteration 1264/1780 Training loss: 1.4810 3.7963 sec/batch\n",
      "Epoch 8/10  Iteration 1265/1780 Training loss: 1.4806 3.8060 sec/batch\n",
      "Epoch 8/10  Iteration 1266/1780 Training loss: 1.4811 3.7793 sec/batch\n",
      "Epoch 8/10  Iteration 1267/1780 Training loss: 1.4798 3.7975 sec/batch\n",
      "Epoch 8/10  Iteration 1268/1780 Training loss: 1.4804 3.7811 sec/batch\n",
      "Epoch 8/10  Iteration 1269/1780 Training loss: 1.4794 3.7908 sec/batch\n",
      "Epoch 8/10  Iteration 1270/1780 Training loss: 1.4792 3.7862 sec/batch\n",
      "Epoch 8/10  Iteration 1271/1780 Training loss: 1.4788 3.9252 sec/batch\n",
      "Epoch 8/10  Iteration 1272/1780 Training loss: 1.4771 3.8691 sec/batch\n",
      "Epoch 8/10  Iteration 1273/1780 Training loss: 1.4754 3.8239 sec/batch\n",
      "Epoch 8/10  Iteration 1274/1780 Training loss: 1.4758 3.8622 sec/batch\n",
      "Epoch 8/10  Iteration 1275/1780 Training loss: 1.4758 3.7973 sec/batch\n",
      "Epoch 8/10  Iteration 1276/1780 Training loss: 1.4759 3.8056 sec/batch\n",
      "Epoch 8/10  Iteration 1277/1780 Training loss: 1.4757 3.8093 sec/batch\n",
      "Epoch 8/10  Iteration 1278/1780 Training loss: 1.4745 3.7979 sec/batch\n",
      "Epoch 8/10  Iteration 1279/1780 Training loss: 1.4747 3.7876 sec/batch\n",
      "Epoch 8/10  Iteration 1280/1780 Training loss: 1.4748 3.8132 sec/batch\n",
      "Epoch 8/10  Iteration 1281/1780 Training loss: 1.4746 3.7847 sec/batch\n",
      "Epoch 8/10  Iteration 1282/1780 Training loss: 1.4743 3.8038 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10  Iteration 1283/1780 Training loss: 1.4732 3.7824 sec/batch\n",
      "Epoch 8/10  Iteration 1284/1780 Training loss: 1.4719 3.8012 sec/batch\n",
      "Epoch 8/10  Iteration 1285/1780 Training loss: 1.4705 3.7848 sec/batch\n",
      "Epoch 8/10  Iteration 1286/1780 Training loss: 1.4698 3.7853 sec/batch\n",
      "Epoch 8/10  Iteration 1287/1780 Training loss: 1.4693 3.7937 sec/batch\n",
      "Epoch 8/10  Iteration 1288/1780 Training loss: 1.4696 3.7758 sec/batch\n",
      "Epoch 8/10  Iteration 1289/1780 Training loss: 1.4689 3.7935 sec/batch\n",
      "Epoch 8/10  Iteration 1290/1780 Training loss: 1.4682 3.8745 sec/batch\n",
      "Epoch 8/10  Iteration 1291/1780 Training loss: 1.4684 3.7870 sec/batch\n",
      "Epoch 8/10  Iteration 1292/1780 Training loss: 1.4674 3.7943 sec/batch\n",
      "Epoch 8/10  Iteration 1293/1780 Training loss: 1.4670 3.7935 sec/batch\n",
      "Epoch 8/10  Iteration 1294/1780 Training loss: 1.4664 3.7869 sec/batch\n",
      "Epoch 8/10  Iteration 1295/1780 Training loss: 1.4662 3.7866 sec/batch\n",
      "Epoch 8/10  Iteration 1296/1780 Training loss: 1.4664 3.8035 sec/batch\n",
      "Epoch 8/10  Iteration 1297/1780 Training loss: 1.4657 3.7893 sec/batch\n",
      "Epoch 8/10  Iteration 1298/1780 Training loss: 1.4663 3.7828 sec/batch\n",
      "Epoch 8/10  Iteration 1299/1780 Training loss: 1.4661 3.8711 sec/batch\n",
      "Epoch 8/10  Iteration 1300/1780 Training loss: 1.4663 3.8678 sec/batch\n",
      "Validation loss: 1.34445 Saving checkpoint!\n",
      "Epoch 8/10  Iteration 1301/1780 Training loss: 1.4676 3.7071 sec/batch\n",
      "Epoch 8/10  Iteration 1302/1780 Training loss: 1.4676 3.7969 sec/batch\n",
      "Epoch 8/10  Iteration 1303/1780 Training loss: 1.4678 3.7862 sec/batch\n",
      "Epoch 8/10  Iteration 1304/1780 Training loss: 1.4674 3.7790 sec/batch\n",
      "Epoch 8/10  Iteration 1305/1780 Training loss: 1.4667 3.7963 sec/batch\n",
      "Epoch 8/10  Iteration 1306/1780 Training loss: 1.4673 3.8010 sec/batch\n",
      "Epoch 8/10  Iteration 1307/1780 Training loss: 1.4672 3.7777 sec/batch\n",
      "Epoch 8/10  Iteration 1308/1780 Training loss: 1.4680 3.7954 sec/batch\n",
      "Epoch 8/10  Iteration 1309/1780 Training loss: 1.4681 3.7871 sec/batch\n",
      "Epoch 8/10  Iteration 1310/1780 Training loss: 1.4682 3.7961 sec/batch\n",
      "Epoch 8/10  Iteration 1311/1780 Training loss: 1.4679 3.7796 sec/batch\n",
      "Epoch 8/10  Iteration 1312/1780 Training loss: 1.4679 3.7796 sec/batch\n",
      "Epoch 8/10  Iteration 1313/1780 Training loss: 1.4680 3.7874 sec/batch\n",
      "Epoch 8/10  Iteration 1314/1780 Training loss: 1.4675 3.8162 sec/batch\n",
      "Epoch 8/10  Iteration 1315/1780 Training loss: 1.4674 3.7871 sec/batch\n",
      "Epoch 8/10  Iteration 1316/1780 Training loss: 1.4672 3.8027 sec/batch\n",
      "Epoch 8/10  Iteration 1317/1780 Training loss: 1.4676 3.7929 sec/batch\n",
      "Epoch 8/10  Iteration 1318/1780 Training loss: 1.4678 3.7992 sec/batch\n",
      "Epoch 8/10  Iteration 1319/1780 Training loss: 1.4681 3.7987 sec/batch\n",
      "Epoch 8/10  Iteration 1320/1780 Training loss: 1.4677 3.7819 sec/batch\n",
      "Epoch 8/10  Iteration 1321/1780 Training loss: 1.4674 3.9672 sec/batch\n",
      "Epoch 8/10  Iteration 1322/1780 Training loss: 1.4675 3.7851 sec/batch\n",
      "Epoch 8/10  Iteration 1323/1780 Training loss: 1.4673 3.7991 sec/batch\n",
      "Epoch 8/10  Iteration 1324/1780 Training loss: 1.4671 3.8003 sec/batch\n",
      "Epoch 8/10  Iteration 1325/1780 Training loss: 1.4664 3.7937 sec/batch\n",
      "Epoch 8/10  Iteration 1326/1780 Training loss: 1.4661 3.7832 sec/batch\n",
      "Epoch 8/10  Iteration 1327/1780 Training loss: 1.4655 3.7907 sec/batch\n",
      "Epoch 8/10  Iteration 1328/1780 Training loss: 1.4654 3.7986 sec/batch\n",
      "Epoch 8/10  Iteration 1329/1780 Training loss: 1.4649 3.8176 sec/batch\n",
      "Epoch 8/10  Iteration 1330/1780 Training loss: 1.4647 3.8486 sec/batch\n",
      "Epoch 8/10  Iteration 1331/1780 Training loss: 1.4643 3.7878 sec/batch\n",
      "Epoch 8/10  Iteration 1332/1780 Training loss: 1.4641 4.1095 sec/batch\n",
      "Epoch 8/10  Iteration 1333/1780 Training loss: 1.4638 4.3274 sec/batch\n",
      "Epoch 8/10  Iteration 1334/1780 Training loss: 1.4635 4.1539 sec/batch\n",
      "Epoch 8/10  Iteration 1335/1780 Training loss: 1.4631 3.8540 sec/batch\n",
      "Epoch 8/10  Iteration 1336/1780 Training loss: 1.4632 3.8819 sec/batch\n",
      "Epoch 8/10  Iteration 1337/1780 Training loss: 1.4629 3.8010 sec/batch\n",
      "Epoch 8/10  Iteration 1338/1780 Training loss: 1.4626 3.7930 sec/batch\n",
      "Epoch 8/10  Iteration 1339/1780 Training loss: 1.4620 3.7806 sec/batch\n",
      "Epoch 8/10  Iteration 1340/1780 Training loss: 1.4616 3.7879 sec/batch\n",
      "Epoch 8/10  Iteration 1341/1780 Training loss: 1.4612 3.7823 sec/batch\n",
      "Epoch 8/10  Iteration 1342/1780 Training loss: 1.4612 3.8067 sec/batch\n",
      "Epoch 8/10  Iteration 1343/1780 Training loss: 1.4610 3.7876 sec/batch\n",
      "Epoch 8/10  Iteration 1344/1780 Training loss: 1.4606 3.7812 sec/batch\n",
      "Epoch 8/10  Iteration 1345/1780 Training loss: 1.4600 3.8034 sec/batch\n",
      "Epoch 8/10  Iteration 1346/1780 Training loss: 1.4595 3.8056 sec/batch\n",
      "Epoch 8/10  Iteration 1347/1780 Training loss: 1.4593 3.8033 sec/batch\n",
      "Epoch 8/10  Iteration 1348/1780 Training loss: 1.4590 3.8223 sec/batch\n",
      "Epoch 8/10  Iteration 1349/1780 Training loss: 1.4586 3.9151 sec/batch\n",
      "Epoch 8/10  Iteration 1350/1780 Training loss: 1.4584 3.8016 sec/batch\n",
      "Epoch 8/10  Iteration 1351/1780 Training loss: 1.4582 3.7894 sec/batch\n",
      "Epoch 8/10  Iteration 1352/1780 Training loss: 1.4580 3.8043 sec/batch\n",
      "Epoch 8/10  Iteration 1353/1780 Training loss: 1.4579 3.7995 sec/batch\n",
      "Epoch 8/10  Iteration 1354/1780 Training loss: 1.4578 3.7898 sec/batch\n",
      "Epoch 8/10  Iteration 1355/1780 Training loss: 1.4576 3.8076 sec/batch\n",
      "Epoch 8/10  Iteration 1356/1780 Training loss: 1.4577 3.7923 sec/batch\n",
      "Epoch 8/10  Iteration 1357/1780 Training loss: 1.4575 3.8043 sec/batch\n",
      "Epoch 8/10  Iteration 1358/1780 Training loss: 1.4572 3.7799 sec/batch\n",
      "Epoch 8/10  Iteration 1359/1780 Training loss: 1.4570 3.7946 sec/batch\n",
      "Epoch 8/10  Iteration 1360/1780 Training loss: 1.4569 3.7744 sec/batch\n",
      "Epoch 8/10  Iteration 1361/1780 Training loss: 1.4566 3.8915 sec/batch\n",
      "Epoch 8/10  Iteration 1362/1780 Training loss: 1.4562 3.9556 sec/batch\n",
      "Epoch 8/10  Iteration 1363/1780 Training loss: 1.4561 4.3450 sec/batch\n",
      "Epoch 8/10  Iteration 1364/1780 Training loss: 1.4561 4.1493 sec/batch\n",
      "Epoch 8/10  Iteration 1365/1780 Training loss: 1.4559 4.1730 sec/batch\n",
      "Epoch 8/10  Iteration 1366/1780 Training loss: 1.4557 3.7974 sec/batch\n",
      "Epoch 8/10  Iteration 1367/1780 Training loss: 1.4556 3.8243 sec/batch\n",
      "Epoch 8/10  Iteration 1368/1780 Training loss: 1.4552 3.8007 sec/batch\n",
      "Epoch 8/10  Iteration 1369/1780 Training loss: 1.4547 3.7945 sec/batch\n",
      "Epoch 8/10  Iteration 1370/1780 Training loss: 1.4547 3.7971 sec/batch\n",
      "Epoch 8/10  Iteration 1371/1780 Training loss: 1.4544 3.7954 sec/batch\n",
      "Epoch 8/10  Iteration 1372/1780 Training loss: 1.4540 3.7902 sec/batch\n",
      "Epoch 8/10  Iteration 1373/1780 Training loss: 1.4540 3.7909 sec/batch\n",
      "Epoch 8/10  Iteration 1374/1780 Training loss: 1.4539 3.7991 sec/batch\n",
      "Epoch 8/10  Iteration 1375/1780 Training loss: 1.4537 3.7956 sec/batch\n",
      "Epoch 8/10  Iteration 1376/1780 Training loss: 1.4533 3.8027 sec/batch\n",
      "Epoch 8/10  Iteration 1377/1780 Training loss: 1.4528 3.8066 sec/batch\n",
      "Epoch 8/10  Iteration 1378/1780 Training loss: 1.4524 3.8329 sec/batch\n",
      "Epoch 8/10  Iteration 1379/1780 Training loss: 1.4524 3.7969 sec/batch\n",
      "Epoch 8/10  Iteration 1380/1780 Training loss: 1.4524 3.8068 sec/batch\n",
      "Epoch 8/10  Iteration 1381/1780 Training loss: 1.4522 3.7953 sec/batch\n",
      "Epoch 8/10  Iteration 1382/1780 Training loss: 1.4521 3.7925 sec/batch\n",
      "Epoch 8/10  Iteration 1383/1780 Training loss: 1.4522 3.7872 sec/batch\n",
      "Epoch 8/10  Iteration 1384/1780 Training loss: 1.4522 3.7961 sec/batch\n",
      "Epoch 8/10  Iteration 1385/1780 Training loss: 1.4521 3.8058 sec/batch\n",
      "Epoch 8/10  Iteration 1386/1780 Training loss: 1.4520 3.8332 sec/batch\n",
      "Epoch 8/10  Iteration 1387/1780 Training loss: 1.4523 3.7960 sec/batch\n",
      "Epoch 8/10  Iteration 1388/1780 Training loss: 1.4521 3.8113 sec/batch\n",
      "Epoch 8/10  Iteration 1389/1780 Training loss: 1.4519 3.8032 sec/batch\n",
      "Epoch 8/10  Iteration 1390/1780 Training loss: 1.4521 3.7885 sec/batch\n",
      "Epoch 8/10  Iteration 1391/1780 Training loss: 1.4518 3.7968 sec/batch\n",
      "Epoch 8/10  Iteration 1392/1780 Training loss: 1.4518 3.8730 sec/batch\n",
      "Epoch 8/10  Iteration 1393/1780 Training loss: 1.4518 3.8211 sec/batch\n",
      "Epoch 8/10  Iteration 1394/1780 Training loss: 1.4520 3.8024 sec/batch\n",
      "Epoch 8/10  Iteration 1395/1780 Training loss: 1.4519 3.8039 sec/batch\n",
      "Epoch 8/10  Iteration 1396/1780 Training loss: 1.4517 3.7911 sec/batch\n",
      "Epoch 8/10  Iteration 1397/1780 Training loss: 1.4514 3.7870 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10  Iteration 1398/1780 Training loss: 1.4512 3.7922 sec/batch\n",
      "Epoch 8/10  Iteration 1399/1780 Training loss: 1.4511 3.7913 sec/batch\n",
      "Epoch 8/10  Iteration 1400/1780 Training loss: 1.4510 3.7915 sec/batch\n",
      "Validation loss: 1.3233 Saving checkpoint!\n",
      "Epoch 8/10  Iteration 1401/1780 Training loss: 1.4515 3.7291 sec/batch\n",
      "Epoch 8/10  Iteration 1402/1780 Training loss: 1.4514 3.7986 sec/batch\n",
      "Epoch 8/10  Iteration 1403/1780 Training loss: 1.4513 3.7958 sec/batch\n",
      "Epoch 8/10  Iteration 1404/1780 Training loss: 1.4512 3.7984 sec/batch\n",
      "Epoch 8/10  Iteration 1405/1780 Training loss: 1.4508 3.7949 sec/batch\n",
      "Epoch 8/10  Iteration 1406/1780 Training loss: 1.4508 3.7960 sec/batch\n",
      "Epoch 8/10  Iteration 1407/1780 Training loss: 1.4509 3.7969 sec/batch\n",
      "Epoch 8/10  Iteration 1408/1780 Training loss: 1.4508 3.7937 sec/batch\n",
      "Epoch 8/10  Iteration 1409/1780 Training loss: 1.4507 3.8064 sec/batch\n",
      "Epoch 8/10  Iteration 1410/1780 Training loss: 1.4506 3.7915 sec/batch\n",
      "Epoch 8/10  Iteration 1411/1780 Training loss: 1.4505 3.7879 sec/batch\n",
      "Epoch 8/10  Iteration 1412/1780 Training loss: 1.4504 3.7879 sec/batch\n",
      "Epoch 8/10  Iteration 1413/1780 Training loss: 1.4505 3.7958 sec/batch\n",
      "Epoch 8/10  Iteration 1414/1780 Training loss: 1.4507 3.7877 sec/batch\n",
      "Epoch 8/10  Iteration 1415/1780 Training loss: 1.4507 3.7866 sec/batch\n",
      "Epoch 8/10  Iteration 1416/1780 Training loss: 1.4506 3.8459 sec/batch\n",
      "Epoch 8/10  Iteration 1417/1780 Training loss: 1.4504 3.8539 sec/batch\n",
      "Epoch 8/10  Iteration 1418/1780 Training loss: 1.4501 3.7957 sec/batch\n",
      "Epoch 8/10  Iteration 1419/1780 Training loss: 1.4501 3.8864 sec/batch\n",
      "Epoch 8/10  Iteration 1420/1780 Training loss: 1.4501 3.8085 sec/batch\n",
      "Epoch 8/10  Iteration 1421/1780 Training loss: 1.4501 3.7899 sec/batch\n",
      "Epoch 8/10  Iteration 1422/1780 Training loss: 1.4499 3.7793 sec/batch\n",
      "Epoch 8/10  Iteration 1423/1780 Training loss: 1.4496 3.8028 sec/batch\n",
      "Epoch 8/10  Iteration 1424/1780 Training loss: 1.4497 3.7920 sec/batch\n",
      "Epoch 9/10  Iteration 1425/1780 Training loss: 1.5322 3.7885 sec/batch\n",
      "Epoch 9/10  Iteration 1426/1780 Training loss: 1.4801 3.9649 sec/batch\n",
      "Epoch 9/10  Iteration 1427/1780 Training loss: 1.4636 3.8020 sec/batch\n",
      "Epoch 9/10  Iteration 1428/1780 Training loss: 1.4582 3.7811 sec/batch\n",
      "Epoch 9/10  Iteration 1429/1780 Training loss: 1.4474 3.7920 sec/batch\n",
      "Epoch 9/10  Iteration 1430/1780 Training loss: 1.4362 3.7870 sec/batch\n",
      "Epoch 9/10  Iteration 1431/1780 Training loss: 1.4357 3.7875 sec/batch\n",
      "Epoch 9/10  Iteration 1432/1780 Training loss: 1.4326 3.7879 sec/batch\n",
      "Epoch 9/10  Iteration 1433/1780 Training loss: 1.4317 3.8668 sec/batch\n",
      "Epoch 9/10  Iteration 1434/1780 Training loss: 1.4302 3.8420 sec/batch\n",
      "Epoch 9/10  Iteration 1435/1780 Training loss: 1.4262 3.7801 sec/batch\n",
      "Epoch 9/10  Iteration 1436/1780 Training loss: 1.4262 3.8540 sec/batch\n",
      "Epoch 9/10  Iteration 1437/1780 Training loss: 1.4263 3.7904 sec/batch\n",
      "Epoch 9/10  Iteration 1438/1780 Training loss: 1.4284 3.7991 sec/batch\n",
      "Epoch 9/10  Iteration 1439/1780 Training loss: 1.4273 3.8067 sec/batch\n",
      "Epoch 9/10  Iteration 1440/1780 Training loss: 1.4257 3.7899 sec/batch\n",
      "Epoch 9/10  Iteration 1441/1780 Training loss: 1.4261 3.7997 sec/batch\n",
      "Epoch 9/10  Iteration 1442/1780 Training loss: 1.4277 3.7855 sec/batch\n",
      "Epoch 9/10  Iteration 1443/1780 Training loss: 1.4275 3.7858 sec/batch\n",
      "Epoch 9/10  Iteration 1444/1780 Training loss: 1.4279 3.7950 sec/batch\n",
      "Epoch 9/10  Iteration 1445/1780 Training loss: 1.4273 3.8035 sec/batch\n",
      "Epoch 9/10  Iteration 1446/1780 Training loss: 1.4278 3.7792 sec/batch\n",
      "Epoch 9/10  Iteration 1447/1780 Training loss: 1.4265 3.7912 sec/batch\n",
      "Epoch 9/10  Iteration 1448/1780 Training loss: 1.4261 3.8558 sec/batch\n",
      "Epoch 9/10  Iteration 1449/1780 Training loss: 1.4258 3.8287 sec/batch\n",
      "Epoch 9/10  Iteration 1450/1780 Training loss: 1.4237 3.7910 sec/batch\n",
      "Epoch 9/10  Iteration 1451/1780 Training loss: 1.4225 3.7954 sec/batch\n",
      "Epoch 9/10  Iteration 1452/1780 Training loss: 1.4230 3.7951 sec/batch\n",
      "Epoch 9/10  Iteration 1453/1780 Training loss: 1.4231 3.7911 sec/batch\n",
      "Epoch 9/10  Iteration 1454/1780 Training loss: 1.4233 3.9272 sec/batch\n",
      "Epoch 9/10  Iteration 1455/1780 Training loss: 1.4229 3.8005 sec/batch\n",
      "Epoch 9/10  Iteration 1456/1780 Training loss: 1.4216 3.7760 sec/batch\n",
      "Epoch 9/10  Iteration 1457/1780 Training loss: 1.4218 3.7841 sec/batch\n",
      "Epoch 9/10  Iteration 1458/1780 Training loss: 1.4219 3.7941 sec/batch\n",
      "Epoch 9/10  Iteration 1459/1780 Training loss: 1.4217 3.7911 sec/batch\n",
      "Epoch 9/10  Iteration 1460/1780 Training loss: 1.4215 3.7860 sec/batch\n",
      "Epoch 9/10  Iteration 1461/1780 Training loss: 1.4206 3.7982 sec/batch\n",
      "Epoch 9/10  Iteration 1462/1780 Training loss: 1.4193 3.7881 sec/batch\n",
      "Epoch 9/10  Iteration 1463/1780 Training loss: 1.4182 3.7949 sec/batch\n",
      "Epoch 9/10  Iteration 1464/1780 Training loss: 1.4176 3.8011 sec/batch\n",
      "Epoch 9/10  Iteration 1465/1780 Training loss: 1.4170 3.7997 sec/batch\n",
      "Epoch 9/10  Iteration 1466/1780 Training loss: 1.4173 3.7812 sec/batch\n",
      "Epoch 9/10  Iteration 1467/1780 Training loss: 1.4168 3.7920 sec/batch\n",
      "Epoch 9/10  Iteration 1468/1780 Training loss: 1.4161 3.7964 sec/batch\n",
      "Epoch 9/10  Iteration 1469/1780 Training loss: 1.4163 3.7958 sec/batch\n",
      "Epoch 9/10  Iteration 1470/1780 Training loss: 1.4153 3.7975 sec/batch\n",
      "Epoch 9/10  Iteration 1471/1780 Training loss: 1.4149 3.7849 sec/batch\n",
      "Epoch 9/10  Iteration 1472/1780 Training loss: 1.4144 3.7869 sec/batch\n",
      "Epoch 9/10  Iteration 1473/1780 Training loss: 1.4142 3.8184 sec/batch\n",
      "Epoch 9/10  Iteration 1474/1780 Training loss: 1.4145 1437.7914 sec/batch\n",
      "Epoch 9/10  Iteration 1475/1780 Training loss: 1.4138 3.8586 sec/batch\n",
      "Epoch 9/10  Iteration 1476/1780 Training loss: 1.4146 4.4787 sec/batch\n",
      "Epoch 9/10  Iteration 1477/1780 Training loss: 1.4145 4.1375 sec/batch\n",
      "Epoch 9/10  Iteration 1478/1780 Training loss: 1.4148 4.0617 sec/batch\n",
      "Epoch 9/10  Iteration 1479/1780 Training loss: 1.4146 4.9284 sec/batch\n",
      "Epoch 9/10  Iteration 1480/1780 Training loss: 1.4147 4.4028 sec/batch\n",
      "Epoch 9/10  Iteration 1481/1780 Training loss: 1.4149 4.2754 sec/batch\n",
      "Epoch 9/10  Iteration 1482/1780 Training loss: 1.4146 5.4019 sec/batch\n",
      "Epoch 9/10  Iteration 1483/1780 Training loss: 1.4140 5.6401 sec/batch\n",
      "Epoch 9/10  Iteration 1484/1780 Training loss: 1.4144 5.3447 sec/batch\n",
      "Epoch 9/10  Iteration 1485/1780 Training loss: 1.4143 5.4926 sec/batch\n",
      "Epoch 9/10  Iteration 1486/1780 Training loss: 1.4149 4.3943 sec/batch\n",
      "Epoch 9/10  Iteration 1487/1780 Training loss: 1.4149 3.8092 sec/batch\n",
      "Epoch 9/10  Iteration 1488/1780 Training loss: 1.4150 3.8282 sec/batch\n",
      "Epoch 9/10  Iteration 1489/1780 Training loss: 1.4147 3.8077 sec/batch\n",
      "Epoch 9/10  Iteration 1490/1780 Training loss: 1.4148 3.8588 sec/batch\n",
      "Epoch 9/10  Iteration 1491/1780 Training loss: 1.4148 3.7857 sec/batch\n",
      "Epoch 9/10  Iteration 1492/1780 Training loss: 1.4145 3.7829 sec/batch\n",
      "Epoch 9/10  Iteration 1493/1780 Training loss: 1.4146 3.7893 sec/batch\n",
      "Epoch 9/10  Iteration 1494/1780 Training loss: 1.4145 3.7910 sec/batch\n",
      "Epoch 9/10  Iteration 1495/1780 Training loss: 1.4149 3.7966 sec/batch\n",
      "Epoch 9/10  Iteration 1496/1780 Training loss: 1.4151 3.8070 sec/batch\n",
      "Epoch 9/10  Iteration 1497/1780 Training loss: 1.4155 3.7830 sec/batch\n",
      "Epoch 9/10  Iteration 1498/1780 Training loss: 1.4150 3.7758 sec/batch\n",
      "Epoch 9/10  Iteration 1499/1780 Training loss: 1.4148 3.8068 sec/batch\n",
      "Epoch 9/10  Iteration 1500/1780 Training loss: 1.4148 3.7877 sec/batch\n",
      "Validation loss: 1.29041 Saving checkpoint!\n",
      "Epoch 9/10  Iteration 1501/1780 Training loss: 1.4160 5.2038 sec/batch\n",
      "Epoch 9/10  Iteration 1502/1780 Training loss: 1.4159 4.9913 sec/batch\n",
      "Epoch 9/10  Iteration 1503/1780 Training loss: 1.4153 4.9348 sec/batch\n",
      "Epoch 9/10  Iteration 1504/1780 Training loss: 1.4150 5.0412 sec/batch\n",
      "Epoch 9/10  Iteration 1505/1780 Training loss: 1.4144 4.9437 sec/batch\n",
      "Epoch 9/10  Iteration 1506/1780 Training loss: 1.4142 4.9460 sec/batch\n",
      "Epoch 9/10  Iteration 1507/1780 Training loss: 1.4136 5.0881 sec/batch\n",
      "Epoch 9/10  Iteration 1508/1780 Training loss: 1.4134 5.0135 sec/batch\n",
      "Epoch 9/10  Iteration 1509/1780 Training loss: 1.4130 4.3375 sec/batch\n",
      "Epoch 9/10  Iteration 1510/1780 Training loss: 1.4128 5.3666 sec/batch\n",
      "Epoch 9/10  Iteration 1511/1780 Training loss: 1.4124 4.4997 sec/batch\n",
      "Epoch 9/10  Iteration 1512/1780 Training loss: 1.4122 4.3670 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10  Iteration 1513/1780 Training loss: 1.4116 4.6834 sec/batch\n",
      "Epoch 9/10  Iteration 1514/1780 Training loss: 1.4116 5.3768 sec/batch\n",
      "Epoch 9/10  Iteration 1515/1780 Training loss: 1.4113 5.2227 sec/batch\n",
      "Epoch 9/10  Iteration 1516/1780 Training loss: 1.4111 4.7540 sec/batch\n",
      "Epoch 9/10  Iteration 1517/1780 Training loss: 1.4106 4.7751 sec/batch\n",
      "Epoch 9/10  Iteration 1518/1780 Training loss: 1.4101 4.5470 sec/batch\n",
      "Epoch 9/10  Iteration 1519/1780 Training loss: 1.4096 4.0631 sec/batch\n",
      "Epoch 9/10  Iteration 1520/1780 Training loss: 1.4095 3.9526 sec/batch\n",
      "Epoch 9/10  Iteration 1521/1780 Training loss: 1.4095 3.8920 sec/batch\n",
      "Epoch 9/10  Iteration 1522/1780 Training loss: 1.4090 3.9649 sec/batch\n",
      "Epoch 9/10  Iteration 1523/1780 Training loss: 1.4086 4.2309 sec/batch\n",
      "Epoch 9/10  Iteration 1524/1780 Training loss: 1.4080 3.8640 sec/batch\n",
      "Epoch 9/10  Iteration 1525/1780 Training loss: 1.4080 3.9292 sec/batch\n",
      "Epoch 9/10  Iteration 1526/1780 Training loss: 1.4077 3.7797 sec/batch\n",
      "Epoch 9/10  Iteration 1527/1780 Training loss: 1.4075 3.8736 sec/batch\n",
      "Epoch 9/10  Iteration 1528/1780 Training loss: 1.4073 3.7776 sec/batch\n",
      "Epoch 9/10  Iteration 1529/1780 Training loss: 1.4071 3.7984 sec/batch\n",
      "Epoch 9/10  Iteration 1530/1780 Training loss: 1.4069 3.7701 sec/batch\n",
      "Epoch 9/10  Iteration 1531/1780 Training loss: 1.4068 3.7817 sec/batch\n",
      "Epoch 9/10  Iteration 1532/1780 Training loss: 1.4066 3.7850 sec/batch\n",
      "Epoch 9/10  Iteration 1533/1780 Training loss: 1.4064 4.2569 sec/batch\n",
      "Epoch 9/10  Iteration 1534/1780 Training loss: 1.4064 4.5427 sec/batch\n",
      "Epoch 9/10  Iteration 1535/1780 Training loss: 1.4061 3.9740 sec/batch\n",
      "Epoch 9/10  Iteration 1536/1780 Training loss: 1.4058 4.0199 sec/batch\n",
      "Epoch 9/10  Iteration 1537/1780 Training loss: 1.4057 4.2439 sec/batch\n",
      "Epoch 9/10  Iteration 1538/1780 Training loss: 1.4053 3.8187 sec/batch\n",
      "Epoch 9/10  Iteration 1539/1780 Training loss: 1.4050 4.1668 sec/batch\n",
      "Epoch 9/10  Iteration 1540/1780 Training loss: 1.4046 4.9192 sec/batch\n",
      "Epoch 9/10  Iteration 1541/1780 Training loss: 1.4045 4.5124 sec/batch\n",
      "Epoch 9/10  Iteration 1542/1780 Training loss: 1.4044 4.5364 sec/batch\n",
      "Epoch 9/10  Iteration 1543/1780 Training loss: 1.4043 4.6895 sec/batch\n",
      "Epoch 9/10  Iteration 1544/1780 Training loss: 1.4040 4.7657 sec/batch\n",
      "Epoch 9/10  Iteration 1545/1780 Training loss: 1.4038 4.8321 sec/batch\n",
      "Epoch 9/10  Iteration 1546/1780 Training loss: 1.4034 4.0158 sec/batch\n",
      "Epoch 9/10  Iteration 1547/1780 Training loss: 1.4029 5.3549 sec/batch\n",
      "Epoch 9/10  Iteration 1548/1780 Training loss: 1.4028 5.2576 sec/batch\n",
      "Epoch 9/10  Iteration 1549/1780 Training loss: 1.4026 4.6645 sec/batch\n",
      "Epoch 9/10  Iteration 1550/1780 Training loss: 1.4022 4.6090 sec/batch\n",
      "Epoch 9/10  Iteration 1551/1780 Training loss: 1.4022 4.0151 sec/batch\n",
      "Epoch 9/10  Iteration 1552/1780 Training loss: 1.4021 4.8610 sec/batch\n",
      "Epoch 9/10  Iteration 1553/1780 Training loss: 1.4018 4.7738 sec/batch\n",
      "Epoch 9/10  Iteration 1554/1780 Training loss: 1.4014 4.5180 sec/batch\n",
      "Epoch 9/10  Iteration 1555/1780 Training loss: 1.4009 3.8896 sec/batch\n",
      "Epoch 9/10  Iteration 1556/1780 Training loss: 1.4005 3.8418 sec/batch\n",
      "Epoch 9/10  Iteration 1557/1780 Training loss: 1.4005 3.8283 sec/batch\n",
      "Epoch 9/10  Iteration 1558/1780 Training loss: 1.4004 3.8063 sec/batch\n",
      "Epoch 9/10  Iteration 1559/1780 Training loss: 1.4002 3.7995 sec/batch\n",
      "Epoch 9/10  Iteration 1560/1780 Training loss: 1.4002 3.8089 sec/batch\n",
      "Epoch 9/10  Iteration 1561/1780 Training loss: 1.4002 3.9249 sec/batch\n",
      "Epoch 9/10  Iteration 1562/1780 Training loss: 1.4002 3.8120 sec/batch\n",
      "Epoch 9/10  Iteration 1563/1780 Training loss: 1.4001 3.7935 sec/batch\n",
      "Epoch 9/10  Iteration 1564/1780 Training loss: 1.3999 3.8097 sec/batch\n",
      "Epoch 9/10  Iteration 1565/1780 Training loss: 1.4001 3.7977 sec/batch\n",
      "Epoch 9/10  Iteration 1566/1780 Training loss: 1.4000 3.8031 sec/batch\n",
      "Epoch 9/10  Iteration 1567/1780 Training loss: 1.3998 3.8030 sec/batch\n",
      "Epoch 9/10  Iteration 1568/1780 Training loss: 1.3999 3.7993 sec/batch\n",
      "Epoch 9/10  Iteration 1569/1780 Training loss: 1.3997 3.9108 sec/batch\n",
      "Epoch 9/10  Iteration 1570/1780 Training loss: 1.3997 4.2481 sec/batch\n",
      "Epoch 9/10  Iteration 1571/1780 Training loss: 1.3997 3.9641 sec/batch\n",
      "Epoch 9/10  Iteration 1572/1780 Training loss: 1.3999 3.9154 sec/batch\n",
      "Epoch 9/10  Iteration 1573/1780 Training loss: 1.3999 4.3523 sec/batch\n",
      "Epoch 9/10  Iteration 1574/1780 Training loss: 1.3996 5.7572 sec/batch\n",
      "Epoch 9/10  Iteration 1575/1780 Training loss: 1.3992 4.7212 sec/batch\n",
      "Epoch 9/10  Iteration 1576/1780 Training loss: 1.3990 3.8931 sec/batch\n",
      "Epoch 9/10  Iteration 1577/1780 Training loss: 1.3989 3.9912 sec/batch\n",
      "Epoch 9/10  Iteration 1578/1780 Training loss: 1.3988 4.1204 sec/batch\n",
      "Epoch 9/10  Iteration 1579/1780 Training loss: 1.3987 4.4447 sec/batch\n",
      "Epoch 9/10  Iteration 1580/1780 Training loss: 1.3986 4.0345 sec/batch\n",
      "Epoch 9/10  Iteration 1581/1780 Training loss: 1.3985 4.9522 sec/batch\n",
      "Epoch 9/10  Iteration 1582/1780 Training loss: 1.3984 4.8273 sec/batch\n",
      "Epoch 9/10  Iteration 1583/1780 Training loss: 1.3980 4.1127 sec/batch\n",
      "Epoch 9/10  Iteration 1584/1780 Training loss: 1.3980 4.2069 sec/batch\n",
      "Epoch 9/10  Iteration 1585/1780 Training loss: 1.3981 3.9817 sec/batch\n",
      "Epoch 9/10  Iteration 1586/1780 Training loss: 1.3980 4.6130 sec/batch\n",
      "Epoch 9/10  Iteration 1587/1780 Training loss: 1.3980 4.0760 sec/batch\n",
      "Epoch 9/10  Iteration 1588/1780 Training loss: 1.3980 4.0064 sec/batch\n",
      "Epoch 9/10  Iteration 1589/1780 Training loss: 1.3979 4.7321 sec/batch\n",
      "Epoch 9/10  Iteration 1590/1780 Training loss: 1.3979 5.5284 sec/batch\n",
      "Epoch 9/10  Iteration 1591/1780 Training loss: 1.3979 4.0255 sec/batch\n",
      "Epoch 9/10  Iteration 1592/1780 Training loss: 1.3982 4.0424 sec/batch\n",
      "Epoch 9/10  Iteration 1593/1780 Training loss: 1.3982 4.2015 sec/batch\n",
      "Epoch 9/10  Iteration 1594/1780 Training loss: 1.3981 3.9134 sec/batch\n",
      "Epoch 9/10  Iteration 1595/1780 Training loss: 1.3979 4.0098 sec/batch\n",
      "Epoch 9/10  Iteration 1596/1780 Training loss: 1.3976 3.9073 sec/batch\n",
      "Epoch 9/10  Iteration 1597/1780 Training loss: 1.3976 4.3386 sec/batch\n",
      "Epoch 9/10  Iteration 1598/1780 Training loss: 1.3976 4.2891 sec/batch\n",
      "Epoch 9/10  Iteration 1599/1780 Training loss: 1.3976 4.2054 sec/batch\n",
      "Epoch 9/10  Iteration 1600/1780 Training loss: 1.3973 4.1163 sec/batch\n",
      "Validation loss: 1.26669 Saving checkpoint!\n",
      "Epoch 9/10  Iteration 1601/1780 Training loss: 1.3978 4.2258 sec/batch\n",
      "Epoch 9/10  Iteration 1602/1780 Training loss: 1.3979 4.9457 sec/batch\n",
      "Epoch 10/10  Iteration 1603/1780 Training loss: 1.4860 4.1637 sec/batch\n",
      "Epoch 10/10  Iteration 1604/1780 Training loss: 1.4441 4.0808 sec/batch\n",
      "Epoch 10/10  Iteration 1605/1780 Training loss: 1.4230 4.8080 sec/batch\n",
      "Epoch 10/10  Iteration 1606/1780 Training loss: 1.4151 5.2693 sec/batch\n",
      "Epoch 10/10  Iteration 1607/1780 Training loss: 1.4043 3.9497 sec/batch\n",
      "Epoch 10/10  Iteration 1608/1780 Training loss: 1.3933 4.3005 sec/batch\n",
      "Epoch 10/10  Iteration 1609/1780 Training loss: 1.3935 5.3766 sec/batch\n",
      "Epoch 10/10  Iteration 1610/1780 Training loss: 1.3898 4.7680 sec/batch\n",
      "Epoch 10/10  Iteration 1611/1780 Training loss: 1.3888 4.8459 sec/batch\n",
      "Epoch 10/10  Iteration 1612/1780 Training loss: 1.3876 3.8314 sec/batch\n",
      "Epoch 10/10  Iteration 1613/1780 Training loss: 1.3837 3.8709 sec/batch\n",
      "Epoch 10/10  Iteration 1614/1780 Training loss: 1.3830 3.7800 sec/batch\n",
      "Epoch 10/10  Iteration 1615/1780 Training loss: 1.3831 3.7781 sec/batch\n",
      "Epoch 10/10  Iteration 1616/1780 Training loss: 1.3839 3.7705 sec/batch\n",
      "Epoch 10/10  Iteration 1617/1780 Training loss: 1.3821 3.9155 sec/batch\n",
      "Epoch 10/10  Iteration 1618/1780 Training loss: 1.3801 3.7851 sec/batch\n",
      "Epoch 10/10  Iteration 1619/1780 Training loss: 1.3798 3.8104 sec/batch\n",
      "Epoch 10/10  Iteration 1620/1780 Training loss: 1.3804 3.7598 sec/batch\n",
      "Epoch 10/10  Iteration 1621/1780 Training loss: 1.3801 3.7774 sec/batch\n",
      "Epoch 10/10  Iteration 1622/1780 Training loss: 1.3809 3.7750 sec/batch\n",
      "Epoch 10/10  Iteration 1623/1780 Training loss: 1.3802 3.7727 sec/batch\n",
      "Epoch 10/10  Iteration 1624/1780 Training loss: 1.3805 3.7756 sec/batch\n",
      "Epoch 10/10  Iteration 1625/1780 Training loss: 1.3793 3.7742 sec/batch\n",
      "Epoch 10/10  Iteration 1626/1780 Training loss: 1.3791 3.7629 sec/batch\n",
      "Epoch 10/10  Iteration 1627/1780 Training loss: 1.3791 3.7722 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10  Iteration 1628/1780 Training loss: 1.3773 3.7602 sec/batch\n",
      "Epoch 10/10  Iteration 1629/1780 Training loss: 1.3756 3.7750 sec/batch\n",
      "Epoch 10/10  Iteration 1630/1780 Training loss: 1.3760 3.7696 sec/batch\n",
      "Epoch 10/10  Iteration 1631/1780 Training loss: 1.3760 3.7653 sec/batch\n",
      "Epoch 10/10  Iteration 1632/1780 Training loss: 1.3760 3.7680 sec/batch\n",
      "Epoch 10/10  Iteration 1633/1780 Training loss: 1.3756 3.7748 sec/batch\n",
      "Epoch 10/10  Iteration 1634/1780 Training loss: 1.3746 3.7686 sec/batch\n",
      "Epoch 10/10  Iteration 1635/1780 Training loss: 1.3748 3.8759 sec/batch\n",
      "Epoch 10/10  Iteration 1636/1780 Training loss: 1.3749 3.7865 sec/batch\n",
      "Epoch 10/10  Iteration 1637/1780 Training loss: 1.3747 3.7704 sec/batch\n",
      "Epoch 10/10  Iteration 1638/1780 Training loss: 1.3744 3.7622 sec/batch\n",
      "Epoch 10/10  Iteration 1639/1780 Training loss: 1.3735 3.7780 sec/batch\n",
      "Epoch 10/10  Iteration 1640/1780 Training loss: 1.3723 3.7800 sec/batch\n",
      "Epoch 10/10  Iteration 1641/1780 Training loss: 1.3709 3.7652 sec/batch\n",
      "Epoch 10/10  Iteration 1642/1780 Training loss: 1.3703 3.7507 sec/batch\n",
      "Epoch 10/10  Iteration 1643/1780 Training loss: 1.3696 3.7543 sec/batch\n",
      "Epoch 10/10  Iteration 1644/1780 Training loss: 1.3702 3.7836 sec/batch\n",
      "Epoch 10/10  Iteration 1645/1780 Training loss: 1.3697 3.8945 sec/batch\n",
      "Epoch 10/10  Iteration 1646/1780 Training loss: 1.3690 3.8184 sec/batch\n",
      "Epoch 10/10  Iteration 1647/1780 Training loss: 1.3694 3.7659 sec/batch\n",
      "Epoch 10/10  Iteration 1648/1780 Training loss: 1.3684 3.7662 sec/batch\n",
      "Epoch 10/10  Iteration 1649/1780 Training loss: 1.3682 3.7586 sec/batch\n",
      "Epoch 10/10  Iteration 1650/1780 Training loss: 1.3679 3.7692 sec/batch\n",
      "Epoch 10/10  Iteration 1651/1780 Training loss: 1.3677 3.8124 sec/batch\n",
      "Epoch 10/10  Iteration 1652/1780 Training loss: 1.3683 3.7599 sec/batch\n",
      "Epoch 10/10  Iteration 1653/1780 Training loss: 1.3678 3.8859 sec/batch\n",
      "Epoch 10/10  Iteration 1654/1780 Training loss: 1.3684 3.7913 sec/batch\n",
      "Epoch 10/10  Iteration 1655/1780 Training loss: 1.3684 3.7620 sec/batch\n",
      "Epoch 10/10  Iteration 1656/1780 Training loss: 1.3686 3.7897 sec/batch\n",
      "Epoch 10/10  Iteration 1657/1780 Training loss: 1.3684 3.7660 sec/batch\n",
      "Epoch 10/10  Iteration 1658/1780 Training loss: 1.3684 3.7602 sec/batch\n",
      "Epoch 10/10  Iteration 1659/1780 Training loss: 1.3686 3.7850 sec/batch\n",
      "Epoch 10/10  Iteration 1660/1780 Training loss: 1.3681 3.7701 sec/batch\n",
      "Epoch 10/10  Iteration 1661/1780 Training loss: 1.3676 3.7669 sec/batch\n",
      "Epoch 10/10  Iteration 1662/1780 Training loss: 1.3681 3.7840 sec/batch\n",
      "Epoch 10/10  Iteration 1663/1780 Training loss: 1.3681 3.7753 sec/batch\n",
      "Epoch 10/10  Iteration 1664/1780 Training loss: 1.3688 3.7555 sec/batch\n",
      "Epoch 10/10  Iteration 1665/1780 Training loss: 1.3690 3.7701 sec/batch\n",
      "Epoch 10/10  Iteration 1666/1780 Training loss: 1.3692 3.7773 sec/batch\n",
      "Epoch 10/10  Iteration 1667/1780 Training loss: 1.3690 3.8509 sec/batch\n",
      "Epoch 10/10  Iteration 1668/1780 Training loss: 1.3690 3.7774 sec/batch\n",
      "Epoch 10/10  Iteration 1669/1780 Training loss: 1.3693 3.7742 sec/batch\n",
      "Epoch 10/10  Iteration 1670/1780 Training loss: 1.3689 3.7711 sec/batch\n",
      "Epoch 10/10  Iteration 1671/1780 Training loss: 1.3689 3.7597 sec/batch\n",
      "Epoch 10/10  Iteration 1672/1780 Training loss: 1.3686 3.7844 sec/batch\n",
      "Epoch 10/10  Iteration 1673/1780 Training loss: 1.3690 3.7779 sec/batch\n",
      "Epoch 10/10  Iteration 1674/1780 Training loss: 1.3693 3.7754 sec/batch\n",
      "Epoch 10/10  Iteration 1675/1780 Training loss: 1.3698 3.7558 sec/batch\n",
      "Epoch 10/10  Iteration 1676/1780 Training loss: 1.3692 3.7742 sec/batch\n",
      "Epoch 10/10  Iteration 1677/1780 Training loss: 1.3690 3.7656 sec/batch\n",
      "Epoch 10/10  Iteration 1678/1780 Training loss: 1.3692 3.7685 sec/batch\n",
      "Epoch 10/10  Iteration 1679/1780 Training loss: 1.3689 3.7606 sec/batch\n",
      "Epoch 10/10  Iteration 1680/1780 Training loss: 1.3689 3.7747 sec/batch\n",
      "Epoch 10/10  Iteration 1681/1780 Training loss: 1.3681 3.7747 sec/batch\n",
      "Epoch 10/10  Iteration 1682/1780 Training loss: 1.3678 3.7700 sec/batch\n",
      "Epoch 10/10  Iteration 1683/1780 Training loss: 1.3672 3.8410 sec/batch\n",
      "Epoch 10/10  Iteration 1684/1780 Training loss: 1.3672 4.0522 sec/batch\n",
      "Epoch 10/10  Iteration 1685/1780 Training loss: 1.3666 3.9648 sec/batch\n",
      "Epoch 10/10  Iteration 1686/1780 Training loss: 1.3665 3.9191 sec/batch\n",
      "Epoch 10/10  Iteration 1687/1780 Training loss: 1.3662 4.2191 sec/batch\n",
      "Epoch 10/10  Iteration 1688/1780 Training loss: 1.3660 3.8194 sec/batch\n",
      "Epoch 10/10  Iteration 1689/1780 Training loss: 1.3657 3.7866 sec/batch\n",
      "Epoch 10/10  Iteration 1690/1780 Training loss: 1.3653 3.7842 sec/batch\n",
      "Epoch 10/10  Iteration 1691/1780 Training loss: 1.3648 3.7702 sec/batch\n",
      "Epoch 10/10  Iteration 1692/1780 Training loss: 1.3648 3.8979 sec/batch\n",
      "Epoch 10/10  Iteration 1693/1780 Training loss: 1.3645 3.7952 sec/batch\n",
      "Epoch 10/10  Iteration 1694/1780 Training loss: 1.3643 3.7631 sec/batch\n",
      "Epoch 10/10  Iteration 1695/1780 Training loss: 1.3638 3.7868 sec/batch\n",
      "Epoch 10/10  Iteration 1696/1780 Training loss: 1.3633 3.7734 sec/batch\n",
      "Epoch 10/10  Iteration 1697/1780 Training loss: 1.3630 3.7850 sec/batch\n",
      "Epoch 10/10  Iteration 1698/1780 Training loss: 1.3629 4.0687 sec/batch\n",
      "Epoch 10/10  Iteration 1699/1780 Training loss: 1.3629 3.7885 sec/batch\n",
      "Epoch 10/10  Iteration 1700/1780 Training loss: 1.3624 3.7939 sec/batch\n",
      "Validation loss: 1.24762 Saving checkpoint!\n",
      "Epoch 10/10  Iteration 1701/1780 Training loss: 1.3635 3.7617 sec/batch\n",
      "Epoch 10/10  Iteration 1702/1780 Training loss: 1.3632 3.8089 sec/batch\n",
      "Epoch 10/10  Iteration 1703/1780 Training loss: 1.3632 3.8015 sec/batch\n",
      "Epoch 10/10  Iteration 1704/1780 Training loss: 1.3629 3.8171 sec/batch\n",
      "Epoch 10/10  Iteration 1705/1780 Training loss: 1.3626 3.8043 sec/batch\n",
      "Epoch 10/10  Iteration 1706/1780 Training loss: 1.3625 3.7974 sec/batch\n",
      "Epoch 10/10  Iteration 1707/1780 Training loss: 1.3623 3.8488 sec/batch\n",
      "Epoch 10/10  Iteration 1708/1780 Training loss: 1.3621 3.8342 sec/batch\n",
      "Epoch 10/10  Iteration 1709/1780 Training loss: 1.3619 3.8132 sec/batch\n",
      "Epoch 10/10  Iteration 1710/1780 Training loss: 1.3619 3.7968 sec/batch\n",
      "Epoch 10/10  Iteration 1711/1780 Training loss: 1.3617 3.8136 sec/batch\n",
      "Epoch 10/10  Iteration 1712/1780 Training loss: 1.3616 4.0651 sec/batch\n",
      "Epoch 10/10  Iteration 1713/1780 Training loss: 1.3613 3.8570 sec/batch\n",
      "Epoch 10/10  Iteration 1714/1780 Training loss: 1.3612 4.0826 sec/batch\n",
      "Epoch 10/10  Iteration 1715/1780 Training loss: 1.3610 3.9116 sec/batch\n",
      "Epoch 10/10  Iteration 1716/1780 Training loss: 1.3607 4.3255 sec/batch\n",
      "Epoch 10/10  Iteration 1717/1780 Training loss: 1.3604 3.8674 sec/batch\n",
      "Epoch 10/10  Iteration 1718/1780 Training loss: 1.3600 3.9497 sec/batch\n",
      "Epoch 10/10  Iteration 1719/1780 Training loss: 1.3599 3.8391 sec/batch\n",
      "Epoch 10/10  Iteration 1720/1780 Training loss: 1.3598 3.8008 sec/batch\n",
      "Epoch 10/10  Iteration 1721/1780 Training loss: 1.3596 3.8173 sec/batch\n",
      "Epoch 10/10  Iteration 1722/1780 Training loss: 1.3595 3.8721 sec/batch\n",
      "Epoch 10/10  Iteration 1723/1780 Training loss: 1.3593 3.8020 sec/batch\n",
      "Epoch 10/10  Iteration 1724/1780 Training loss: 1.3589 4.1261 sec/batch\n",
      "Epoch 10/10  Iteration 1725/1780 Training loss: 1.3584 4.6189 sec/batch\n",
      "Epoch 10/10  Iteration 1726/1780 Training loss: 1.3584 4.1922 sec/batch\n",
      "Epoch 10/10  Iteration 1727/1780 Training loss: 1.3582 4.0564 sec/batch\n",
      "Epoch 10/10  Iteration 1728/1780 Training loss: 1.3578 3.8542 sec/batch\n",
      "Epoch 10/10  Iteration 1729/1780 Training loss: 1.3577 4.1570 sec/batch\n",
      "Epoch 10/10  Iteration 1730/1780 Training loss: 1.3577 4.1502 sec/batch\n",
      "Epoch 10/10  Iteration 1731/1780 Training loss: 1.3575 3.8340 sec/batch\n",
      "Epoch 10/10  Iteration 1732/1780 Training loss: 1.3572 3.8347 sec/batch\n",
      "Epoch 10/10  Iteration 1733/1780 Training loss: 1.3568 4.2014 sec/batch\n",
      "Epoch 10/10  Iteration 1734/1780 Training loss: 1.3566 4.3063 sec/batch\n",
      "Epoch 10/10  Iteration 1735/1780 Training loss: 1.3567 3.9910 sec/batch\n",
      "Epoch 10/10  Iteration 1736/1780 Training loss: 1.3567 3.9361 sec/batch\n",
      "Epoch 10/10  Iteration 1737/1780 Training loss: 1.3565 3.8757 sec/batch\n",
      "Epoch 10/10  Iteration 1738/1780 Training loss: 1.3564 3.8094 sec/batch\n",
      "Epoch 10/10  Iteration 1739/1780 Training loss: 1.3566 3.8206 sec/batch\n",
      "Epoch 10/10  Iteration 1740/1780 Training loss: 1.3566 3.8229 sec/batch\n",
      "Epoch 10/10  Iteration 1741/1780 Training loss: 1.3566 3.7987 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10  Iteration 1742/1780 Training loss: 1.3565 3.8140 sec/batch\n",
      "Epoch 10/10  Iteration 1743/1780 Training loss: 1.3568 3.8028 sec/batch\n",
      "Epoch 10/10  Iteration 1744/1780 Training loss: 1.3568 3.8136 sec/batch\n",
      "Epoch 10/10  Iteration 1745/1780 Training loss: 1.3567 3.8644 sec/batch\n",
      "Epoch 10/10  Iteration 1746/1780 Training loss: 1.3569 3.9156 sec/batch\n",
      "Epoch 10/10  Iteration 1747/1780 Training loss: 1.3567 3.7955 sec/batch\n",
      "Epoch 10/10  Iteration 1748/1780 Training loss: 1.3568 3.8007 sec/batch\n",
      "Epoch 10/10  Iteration 1749/1780 Training loss: 1.3568 3.8150 sec/batch\n",
      "Epoch 10/10  Iteration 1750/1780 Training loss: 1.3570 3.7830 sec/batch\n",
      "Epoch 10/10  Iteration 1751/1780 Training loss: 1.3571 3.7994 sec/batch\n",
      "Epoch 10/10  Iteration 1752/1780 Training loss: 1.3568 3.8511 sec/batch\n",
      "Epoch 10/10  Iteration 1753/1780 Training loss: 1.3566 3.8527 sec/batch\n",
      "Epoch 10/10  Iteration 1754/1780 Training loss: 1.3564 3.8005 sec/batch\n",
      "Epoch 10/10  Iteration 1755/1780 Training loss: 1.3564 3.8157 sec/batch\n",
      "Epoch 10/10  Iteration 1756/1780 Training loss: 1.3563 3.8092 sec/batch\n",
      "Epoch 10/10  Iteration 1757/1780 Training loss: 1.3563 3.7911 sec/batch\n",
      "Epoch 10/10  Iteration 1758/1780 Training loss: 1.3562 3.7892 sec/batch\n",
      "Epoch 10/10  Iteration 1759/1780 Training loss: 1.3562 3.7996 sec/batch\n",
      "Epoch 10/10  Iteration 1760/1780 Training loss: 1.3562 3.7872 sec/batch\n",
      "Epoch 10/10  Iteration 1761/1780 Training loss: 1.3559 3.7949 sec/batch\n",
      "Epoch 10/10  Iteration 1762/1780 Training loss: 1.3559 3.8113 sec/batch\n",
      "Epoch 10/10  Iteration 1763/1780 Training loss: 1.3561 3.7962 sec/batch\n",
      "Epoch 10/10  Iteration 1764/1780 Training loss: 1.3560 3.8139 sec/batch\n",
      "Epoch 10/10  Iteration 1765/1780 Training loss: 1.3560 3.8008 sec/batch\n",
      "Epoch 10/10  Iteration 1766/1780 Training loss: 1.3559 3.7925 sec/batch\n",
      "Epoch 10/10  Iteration 1767/1780 Training loss: 1.3560 3.8003 sec/batch\n",
      "Epoch 10/10  Iteration 1768/1780 Training loss: 1.3559 3.8609 sec/batch\n",
      "Epoch 10/10  Iteration 1769/1780 Training loss: 1.3560 3.7928 sec/batch\n",
      "Epoch 10/10  Iteration 1770/1780 Training loss: 1.3563 3.8166 sec/batch\n",
      "Epoch 10/10  Iteration 1771/1780 Training loss: 1.3563 3.8071 sec/batch\n",
      "Epoch 10/10  Iteration 1772/1780 Training loss: 1.3563 3.8170 sec/batch\n",
      "Epoch 10/10  Iteration 1773/1780 Training loss: 1.3562 3.8061 sec/batch\n",
      "Epoch 10/10  Iteration 1774/1780 Training loss: 1.3559 3.9834 sec/batch\n",
      "Epoch 10/10  Iteration 1775/1780 Training loss: 1.3560 3.7885 sec/batch\n",
      "Epoch 10/10  Iteration 1776/1780 Training loss: 1.3560 3.8148 sec/batch\n",
      "Epoch 10/10  Iteration 1777/1780 Training loss: 1.3560 3.8008 sec/batch\n",
      "Epoch 10/10  Iteration 1778/1780 Training loss: 1.3558 3.7983 sec/batch\n",
      "Epoch 10/10  Iteration 1779/1780 Training loss: 1.3556 3.7987 sec/batch\n",
      "Epoch 10/10  Iteration 1780/1780 Training loss: 1.3557 3.8053 sec/batch\n",
      "Validation loss: 1.24187 Saving checkpoint!\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "save_every_n = 100\n",
    "train_x, train_y, val_x, val_y = split_data(chars, batch_size, num_steps)\n",
    "\n",
    "model = build_rnn(len(vocab), \n",
    "                  batch_size=batch_size,\n",
    "                  num_steps=num_steps,\n",
    "                  learning_rate=learning_rate,\n",
    "                  lstm_size=lstm_size,\n",
    "                  num_layers=num_layers)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    train_writer = tf.summary.FileWriter('./logs/2/train', sess.graph)\n",
    "    test_writer = tf.summary.FileWriter('./logs/2/test')\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/anna20.ckpt')\n",
    "    \n",
    "    n_batches = int(train_x.shape[1]/num_steps)\n",
    "    iterations = n_batches * epochs\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for b, (x, y) in enumerate(get_batch([train_x, train_y], num_steps), 1):\n",
    "            iteration = e*n_batches + b\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: 0.5,\n",
    "                    model.initial_state: new_state}\n",
    "            summary, batch_loss, new_state, _ = sess.run([model.merged,model.cost, \n",
    "                                                          model.final_state, model.optimizer], feed_dict=feed)\n",
    "\n",
    "            loss += batch_loss\n",
    "            end = time.time()\n",
    "            print('Epoch {}/{} '.format(e+1, epochs),\n",
    "                  'Iteration {}/{}'.format(iteration, iterations),\n",
    "                  'Training loss: {:.4f}'.format(loss/b),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            train_writer.add_summary(summary, iteration)\n",
    "        \n",
    "            if (iteration%save_every_n == 0) or (iteration == iterations):\n",
    "                # Check performance, notice dropout has been set to 1\n",
    "                val_loss = []\n",
    "                new_state = sess.run(model.initial_state)\n",
    "                for x, y in get_batch([val_x, val_y], num_steps ):\n",
    "                    feed = {model.inputs: x,\n",
    "                            model.targets: y,\n",
    "                            model.keep_prob: 1.,\n",
    "                            model.initial_state: new_state}\n",
    "                    summary, batch_loss, new_state = sess.run([model.merged, model.cost, \n",
    "                                                               model.final_state], feed_dict=feed)\n",
    "                    val_loss.append(batch_loss)\n",
    "                test_writer.add_summary(summary, iteration)\n",
    "                \n",
    "                print('Validation loss:', np.mean(val_loss),\n",
    "                      'Saving checkpoint!')\n",
    "                saver.save(sess, \"checkpoints/anna/i{}_l{}_{:.3f}.ckpt\".format(iteration, lstm_size, np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/anna/i1780_l512_1.242.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i100_l512_3.033.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i200_l512_2.381.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i300_l512_2.117.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i400_l512_1.933.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i500_l512_1.811.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i600_l512_1.715.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i700_l512_1.620.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i800_l512_1.551.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i900_l512_1.496.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i1000_l512_1.450.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i1100_l512_1.414.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i1200_l512_1.377.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i1300_l512_1.344.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i1400_l512_1.323.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i1500_l512_1.290.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i1600_l512_1.267.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i1700_l512_1.248.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/anna/i1780_l512_1.242.ckpt\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints/anna')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    prime = \"Far\"\n",
    "    samples = [c for c in prime]\n",
    "    model = build_rnn(vocab_size, lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/anna/i3560_l512_1.122.ckpt\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to find any matching files for checkpoints/anna/i3560_l512_1.122.ckpt\n\t [[Node: save/RestoreV2_2 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save/Const_0_0, save/RestoreV2_2/tensor_names, save/RestoreV2_2/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2_2', defined at:\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-28-91ad82b46673>\", line 2, in <module>\n    samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n  File \"<ipython-input-27-a7ae04af7e97>\", line 5, in sample\n    saver = tf.train.Saver()\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1139, in __init__\n    self.build()\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1170, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 691, in build\n    restore_sequentially, reshape)\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 407, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 247, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 640, in restore_v2\n    dtypes=dtypes, name=name)\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nNotFoundError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to find any matching files for checkpoints/anna/i3560_l512_1.122.ckpt\n\t [[Node: save/RestoreV2_2 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save/Const_0_0, save/RestoreV2_2/tensor_names, save/RestoreV2_2/shape_and_slices)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/maheshubuntu/anaconda3/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for checkpoints/anna/i3560_l512_1.122.ckpt\n\t [[Node: save/RestoreV2_2 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save/Const_0_0, save/RestoreV2_2/tensor_names, save/RestoreV2_2/shape_and_slices)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-91ad82b46673>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"checkpoints/anna/i3560_l512_1.122.ckpt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Far\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-a7ae04af7e97>\u001b[0m in \u001b[0;36msample\u001b[0;34m(checkpoint, n_samples, lstm_size, vocab_size, prime)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprime\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1546\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Restoring parameters from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1547\u001b[0m     sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1548\u001b[0;31m              {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for checkpoints/anna/i3560_l512_1.122.ckpt\n\t [[Node: save/RestoreV2_2 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save/Const_0_0, save/RestoreV2_2/tensor_names, save/RestoreV2_2/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2_2', defined at:\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-28-91ad82b46673>\", line 2, in <module>\n    samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n  File \"<ipython-input-27-a7ae04af7e97>\", line 5, in sample\n    saver = tf.train.Saver()\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1139, in __init__\n    self.build()\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1170, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 691, in build\n    restore_sequentially, reshape)\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 407, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 247, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 640, in restore_v2\n    dtypes=dtypes, name=name)\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/maheshubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nNotFoundError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to find any matching files for checkpoints/anna/i3560_l512_1.122.ckpt\n\t [[Node: save/RestoreV2_2 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save/Const_0_0, save/RestoreV2_2/tensor_names, save/RestoreV2_2/shape_and_slices)]]\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/anna/i3560_l512_1.122.ckpt\"\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint = \"checkpoints/anna/i200_l512_2.432.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/anna/i600_l512_1.750.ckpt\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to find any matching files for checkpoints/anna/i600_l512_1.750.ckpt\n\t [[Node: save/RestoreV2_19 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_19/tensor_names, save/RestoreV2_19/shape_and_slices)]]\n\t [[Node: save/RestoreV2_17/_31 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_180_save/RestoreV2_17\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\nCaused by op 'save/RestoreV2_19', defined at:\n  File \"/usr/local/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/local/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2683, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2787, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2847, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-24-6b72ec853273>\", line 2, in <module>\n    samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n  File \"<ipython-input-22-a7ae04af7e97>\", line 5, in sample\n    saver = tf.train.Saver()\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1056, in __init__\n    self.build()\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1086, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 691, in build\n    restore_sequentially, reshape)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 407, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 247, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 669, in restore_v2\n    dtypes=dtypes, name=name)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nNotFoundError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to find any matching files for checkpoints/anna/i600_l512_1.750.ckpt\n\t [[Node: save/RestoreV2_19 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_19/tensor_names, save/RestoreV2_19/shape_and_slices)]]\n\t [[Node: save/RestoreV2_17/_31 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_180_save/RestoreV2_17\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for checkpoints/anna/i600_l512_1.750.ckpt\n\t [[Node: save/RestoreV2_19 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_19/tensor_names, save/RestoreV2_19/shape_and_slices)]]\n\t [[Node: save/RestoreV2_17/_31 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_180_save/RestoreV2_17\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-6b72ec853273>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"checkpoints/anna/i600_l512_1.750.ckpt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Far\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-a7ae04af7e97>\u001b[0m in \u001b[0;36msample\u001b[0;34m(checkpoint, n_samples, lstm_size, vocab_size, prime)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprime\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Restoring parameters from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m     sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1457\u001b[0;31m              {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for checkpoints/anna/i600_l512_1.750.ckpt\n\t [[Node: save/RestoreV2_19 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_19/tensor_names, save/RestoreV2_19/shape_and_slices)]]\n\t [[Node: save/RestoreV2_17/_31 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_180_save/RestoreV2_17\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\nCaused by op 'save/RestoreV2_19', defined at:\n  File \"/usr/local/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/local/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2683, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2787, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2847, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-24-6b72ec853273>\", line 2, in <module>\n    samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n  File \"<ipython-input-22-a7ae04af7e97>\", line 5, in sample\n    saver = tf.train.Saver()\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1056, in __init__\n    self.build()\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1086, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 691, in build\n    restore_sequentially, reshape)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 407, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 247, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 669, in restore_v2\n    dtypes=dtypes, name=name)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nNotFoundError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to find any matching files for checkpoints/anna/i600_l512_1.750.ckpt\n\t [[Node: save/RestoreV2_19 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_19/tensor_names, save/RestoreV2_19/shape_and_slices)]]\n\t [[Node: save/RestoreV2_17/_31 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_180_save/RestoreV2_17\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/anna/i600_l512_1.750.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Farrat, his felt has at it.\n",
      "\n",
      "\"When the pose ther hor exceed\n",
      "to his sheant was,\" weat a sime of his sounsed. The coment and the facily that which had began terede a marilicaly whice whether the pose of his hand, at she was alligated herself the same on she had to\n",
      "taiking to his forthing and streath how to hand\n",
      "began in a lang at some at it, this he cholded not set all her. \"Wo love that is setthing. Him anstering as seen that.\"\n",
      "\n",
      "\"Yes in the man that say the mare a crances is it?\" said Sergazy Ivancatching. \"You doon think were somether is ifficult of a mone of\n",
      "though the most at the countes that the\n",
      "mean on the come to say the most, to\n",
      "his feesing of\n",
      "a man she, whilo he\n",
      "sained and well, that he would still at to said. He wind at his for the sore in the most\n",
      "of hoss and almoved to see him. They have betine the sumper into at he his stire, and what he was that at the so steate of the\n",
      "sound, and shin should have a geest of shall feet on the conderation to she had been at that imporsing the dre\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/anna/i1000_l512_1.484.ckpt\"\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
